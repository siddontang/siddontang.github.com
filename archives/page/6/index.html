<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第 6 页 | 归档 | Siddon&#39;s Blog</title>
  <meta name="author" content="SiddonTang">
  
  <meta name="description" content="My thought for program">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Siddon&#39;s Blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Siddon&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-27956076-1']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Siddon&#39;s Blog</a></h1>
  <h2><a href="/">My thought for program</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/Presentations">Presentations</a></li>
    
      <li><a href="/About">About</a></li>
    
      <li><a href="/atom.xml">RSS</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
<h2 class="archive-title">归档</h2>


  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-16T07:33:02.000Z"><a href="/2014/01/16/lua-function-register/">01-16-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/16/lua-function-register/">Lua与C接口层开发</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="lua与c的交互">lua与c的交互</h1>
<p>关于lua和c的交互，主要有两个方面，一是lua调用c的函数，而另一个则是c调用lua函数。而这些都是通过lua stack来进行的。</p>
<h2 id="c调用lua">c调用lua</h2>
<p>在c里面使用lua，主要是通过lua_call这类函数，下面来自lua manual的例子：</p>
<pre><code>lua_getglobal(L, &quot;f&quot;);                  /* function to be called */
lua_pushstring(L, &quot;how&quot;);                        /* 1st argument */
lua_getglobal(L, &quot;t&quot;);                    /* table to be indexed */
lua_getfield(L, -1, &quot;x&quot;);        /* push result of t.x (2nd arg) */
lua_remove(L, -2);                  /* remove &#39;t&#39; from the stack */
lua_pushinteger(L, 14);                          /* 3rd argument */
lua_call(L, 3, 1);     /* call &#39;f&#39; with 3 arguments and 1 result */
lua_setglobal(L, &quot;a&quot;);                         /* set global &#39;a&#39; */
</code></pre><p>该例子等同于直接在lua里面调用 a = f(“how”, t.x, 14)。<br>通过上面的例子可以看到，在c中使用lua是一件很容易的事情，首先获取需要调用的lua函数，然后将其需要的参数依次压入stack，然后通过lua_call调用，该函数调用的返回值也压入stack，供c去获取。</p>
<h2 id="lua调用c">lua调用c</h2>
<p>对于lua调用c，我们首先需要将c函数注册给lua，而注册给lua的函数，需要满足 int (*lua_CFunction)(lua_State* pState) 这种类型。如下例子:</p>
<pre><code>int multi(lua_State* pState)
{
    int data1 = int(lua_tonumber(pState, 1));
    int data2 = int(lua_tonumber(pState, 2));
    lua_pushnumber(pState, data1 * data2);
    return 1;
}

lua_register(pState, multi, &quot;multi&quot;);

#for use in lua
#a = multi(10, 20)
</code></pre><p>我们通过lua_register将mutli函数注册给lua，该函数接受两个参数，并且有一个返回值。当lua调用multi的时候，会将参数压入stack，所以我们可以通过lua_tonumber(pState, 1)和lua_tonumber(pState, 2)来获取，其中1为第一个参数10，2为第二个参数20。当运行完成之后，multi函数通过lua_pushnumber将结果压入lua堆栈，并通过return 1告知lua有一个返回值，为200。</p>
<p>可以看到，在lua中使用c也是一件很简单的事情。</p>
<h1 id="lua_mainly_or_c_mainly">lua mainly or c mainly</h1>
<p>通过上面的例子可以看出，lua与c是很方便的交互的，但是在实际的游戏项目中，我们首先必须确定的一个问题就是，代码逻辑是以lua为主还是以c为主。</p>
<ul>
<li>lua为主的游戏就是逻辑主要由lua负责，核心的对性能要求较高的逻辑则由c负责，游戏中的数据大多由lua负责。</li>
<li>c为主的游戏则是逻辑主要由c负责，lua只是负责简单的配置。</li>
</ul>
<p>这两种方式都有优劣，对于实际游戏项目来说，个人认为，应该采用lua为主，c作为高性能核心的方式。之所以这样选择，是因为游戏的逻辑变动很大，我们需要快速的进行代码迭代，这个对于lua来说非常方便。而对于核心引擎，因为变化不大，同时对性能要求较高，所以采用c是一个很好的选择。</p>
<h1 id="reg_helper">reg helper</h1>
<p>在实际的游戏项目中，我们会遇到这样一个问题，假设c提供的函数为 int func(int a, int b)，如果这个函数要提供给lua使用，我们需要写一个对应的注册函数，如下:</p>
<pre><code>int func_wrapper(lua_State* pState)
{
    int a = int(lua_tonumber(pState, 1));
    int b = int(lua_tonumber(pState, 2));
    int ret = func(a, b);
    lua_pushnumber(pState, ret);
    return 1;
}

lua_register(pState, func_wrapper, &quot;func&quot;);
</code></pre><p>对于任意的c函数，我们需要写一个对应的wrapper用来注册给lua。如果项目中只有几个c函数，那么无所谓，但是如果需要注册给lua的c函数很多，那么对于每一个c函数写一个wrapper，是一件很不现实的事情。并且如果c函数的参数或者返回值有变化，我们同时需要修改对应的wrapper函数。基于上述原因，我们需要一套自动机制，能够将任意的c函数注册给lua使用。实际来说，我们需要提供一个函数，对于任意的c函数func，我们只需要调用register(func, “func”)，那么就能直接注册给lua使用。</p>
<h2 id="traits">traits</h2>
<p>首先，我们必须面对的问题就是，不同的c函数，参数和返回值是不一样的，譬如对于int类型的参数，我们需要通过lua_tonumber获取数据，而对于const char* 类型的参数，我们需要通过lua_tostring来获取，同理对于返回值也一样。所以我们需要一套机制，根据c函数不同的参数和返回值类型来调用lua对应的stack操纵函数。我们可以通过c++ traits来实现。</p>
<p>我们提供如下一套函数:</p>
<pre><code>template&lt;typename T&gt;
struct TypeHelper{};
bool getValue(TypeHelper&lt;bool&gt;, lua_State* pState, int index)
{
    return lua_toboolean(pState, index) == 1;
}
char getValue(TypeHelper&lt;char&gt;, lua_State* pState, int index)
{
    return static_cast&lt;char&gt;(lua_tonumber(pState, index));
}
int getValue(TypeHelper&lt;int&gt;, lua_State* pState, int index)
{
    return static_cast&lt;int&gt;(lua_tonumber(pState, index));
}
void pushValue(lua_State* pState, bool value)
{
    lua_pushboolean(pState, int(value));
}

void pushValue(lua_State* pState, char value)
{
    lua_pushnumber(pState, value);
}

void pushValue(lua_State* pState, int value)
{
    lua_pushnumber(pState, value);
}
</code></pre><p>通过traits技术，对于不同的参数类型，我们可以调用对应的getValue函数，来从lua获取实际的数据，而对于返回值，通过c++自动的参数匹配，就能调用对应的pushValue函数。</p>
<h2 id="CCallHelper">CCallHelper</h2>
<p>上面解决了类型匹配的问题，下面就需要提供wrapper函数，用以封装c函数。这里我们提供call helper类来实现。</p>
<pre><code>template&lt;typename Ret&gt;
class CCallHelper
{
public:
    static int call(Ret (*func)(), lua_State* pState)
    {
        Ret ret = (*func)();
        pushValue(pState, ret);
        return 1;
    }

    template&lt;typename P1&gt;
    static int call(Ret (*func)(P1), lua_State* pState)
    {
        P1 p1 = getValue(TypeHelper&lt;P1&gt;(), pState, 1);
        Ret ret = (*func)(p1);
        pushValue(pState, ret);
        return 1;
    }
};
</code></pre><p>CCallHelper提供了静态的call函数，第一个参数就是实际c函数，通过函数模板可以进行任意c函数的匹配，这里只提供了匹配无参数和一个参数类型的c函数模板，我们可以扩展到支持任意参数个数，但也别太多了。因为对于任意c函数来说，可能有一个返回值，也可能没有返回值，那么我们如何匹配没有返回值的c函数呢？这里就是为什么我们需要CCallHelper的原因。在c++中，是不支持函数级别的模板特化的，但是类却可以，所以我们通过特化CCallHelper来匹配无返回值的c函数。如下：</p>
<pre><code>template&lt;&gt;
class CCallHelper&lt;void&gt;
{
public:
    static int call(void (*func)(), lua_State* pState)
    {
        (*func)();
        return 0;
    } 

    template&lt;typename P1&gt;
    static int call(void (*func)(P1), lua_State* pState)
    {
        P1 p1 = getValue(TypeHelper&lt;P1&gt;(), pState, 1);
        (*func)(p1);
        return 0;
    }
};
</code></pre><h2 id="CCallDispatcher">CCallDispatcher</h2>
<p>通过CCallHelper::call(func, pState)，我们就可以与lua进行交互，那么又如何调用到相应的CCallHelper呢？这里我们通过CCallDispatcher来进行，如下：</p>
<pre><code>template&lt;typename Func&gt;
class CCallDispatcher
{
public:
    template&lt;typename Ret&gt;
    static int dispatch(Ret (*func)(), lua_State* pState)
    {
        return CCallHelper&lt;Ret&gt;::call(func, pState);
    }

    template&lt;typename Ret, typename P1&gt;
    static int dispatch(Ret (*func)(P1), lua_State* pState)
    {
        return CCallHelper&lt;Ret&gt;::call(func, pState);
    }
};
</code></pre><p>通过CCallDispatcher，我们就可以将不同的c函数dispatch到不同的CCallHelper上面。</p>
<h2 id="CCallRegister_and_regFunction">CCallRegister and regFunction</h2>
<p>解决了c函数派发调用的问题，最后我们就需要处理如何将任意的c函数注册给lua，代码如下：</p>
<pre><code>template&lt;typename Func&gt;
class CCallRegister
{
public:
    static int call(lua_State* pState)
    {
        Func* func = static_cast&lt;Func*&gt;(lua_touserdata(pState, lua_upvalueindex(1));
        return CCallDispatcher&lt;Func&gt;::dispatch(*func, pState);
    }
};

template&lt;typename Func&gt;
void regFunction(lua_State* pState, Func func, const char* funcName)
{
    int funcSize = sizeof(Func);
    void* data = lua_newuserdata(pState, funcSize);
    memcpy(data, &amp;func, funcSize);

    lua_pushcclosure(pState, CCallRegister&lt;Func&gt;::call, 1);
    lua_setglobal(pState, funcName);
}
</code></pre><p>首先，我们提供CCallRegister类，里面提供了一个static的call函数，该函数满足lua注册格式，所以实际我们是将该函数注册给lua，在call函数里面，我们通过lua_touserdata(pState, lua_upvalueindex(1))来获取实际的func，然后传递给CCallDispatcher进行派发。而将call注册则是通过regFunction，该函数将实际的c函数func存储在一个userdata中，然后将该userdata绑定到对应的CCallRgister call上面，作为一个upvalue，这样当在lua里面调用call函数的时候，通过lua_upvalueindex获取对应的upvalue，则可以取到实际的c函数。</p>
<h1 id="一些设计上面的考虑">一些设计上面的考虑</h1>
<p>上述reghelper的实现，我已经放到github <a href="https://github.com/siddontang/luahelper" target="_blank" rel="external">luahelper</a>上面，并且在max os，gcc 4.2，lua5.2环境下面测试通过。</p>
<p>这里谈一些设计上面的问题，首先，我说的任意c函数，参数和返回值只能是基本数值类型，如bool，char，short，int，long，float，double以及字符串类型char*等，这里我并没有提供复杂类型譬如class，struct的支持。之所以这样考虑，是因为我想保证lua与<br>c交互的简单，云风曾经说过<a href="http://blog.codingnow.com/2008/08/lua_is_not_c_plus_plus.html" target="_blank" rel="external">lua不是c++</a>，我本人当年也曾经用了2年的时间做了同样的事情。但是实现了这套东西，功能是狠强大了，以至于可以把lua当成c++来用了，但是这真的是使用lua正确的方式吗？我现在觉得，引入复杂的结合层，反而在某些时候会带来更大的复杂性，导致语言侧重点的混淆。所以有时候，我反而觉得，对于这种语言的交互，可能使用其他方式，譬如json，反而来的更容易。</p>
<h1 id="写在后面的话">写在后面的话</h1>
<p>对于lua和游戏开发的一些东西，其实一直想写，但是以前因为很多方面的原因而中途放弃。现在重新开始，有几个方面的原因，一个在于仍然对于游戏开发的热爱，做了4年的游戏开发，虽然现在从事云存储方面的研究，但是对游戏热情依旧。另一个方面在于lua5.2的发布，觉得是应该对以前游戏人生做一个总结了。</p>
<p>如果需要更强大的lua与c的交互，我觉得<a href="http://www.swig.org/" target="_blank" rel="external">swig</a>可能更合适。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T06:04:17.000Z"><a href="/2014/01/01/libtnet-comettest/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/libtnet-comettest/">Libtnet 百万连接测试</a></h1>
  

    </header>
    <div class="entry">
      
        <p>最近在用go语言做一个挂载大量长连接的推送服务器，虽然已经完成，但是内存占用情况让我不怎么满意，于是考虑使用libtnet来重新实现一个。后续我会使用comet来表明推送服务器。</p>
<p>对于comet来说，单机能支撑大量的并发连接，是最优先考虑的事项。虽然现在业界已经有了很多数据，说单机支撑200w，300w，但我还是先把目标定在100w上面，主要的原因在于实际运行中，comet还会有少量逻辑功能，我得保证在单机挂载100w的基础上，完全能无压力的处理这些逻辑。</p>
<h1 id="CometServer_Test">CometServer Test</h1>
<p>首先我用libtnet简单写了一个comet server。它接受http请求，并将其挂起，过一段随机时间之后在返回200。</p>
<pre><code>void onTimeout(const TimingWheelPtr_t&amp; wheel, const WeakHttpConnectionPtr_t&amp; conn)
{
    HttpConnectionPtr_t c = conn.lock();
    if(c)
    {
        c-&gt;send(200);
    } 
}

void onHandler(const HttpConnectionPtr_t&amp; conn, const HttpRequest&amp; request)
{
    int timeout = random() % 60 + 30;
    comet.wheel-&gt;add(std::bind(&amp;onTimeout, _1, WeakHttpConnectionPtr_t(conn)), timeout * 1000);
}

int main()
{
    TcpServer s;        
    s.setRunCallback(std::bind(&amp;onServerRun, _1));
    HttpServer httpd(&amp;s);
    httpd.setHttpCallback(&quot;/&quot;, std::bind(&amp;onHandler, _1, _2));
    httpd.listen(Address(11181));
    s.start(8);
    return 0; 
}
</code></pre><p>可以看到comet server只是负责了挂载长连接的事情，而没有消息的推送。在实际项目中，我已经将挂载连接和推送消息分开到两个服务去完成。所以这里comet仅仅是挂载连接测试。</p>
<h1 id="测试机器准备">测试机器准备</h1>
<p>因为linux系统上面一个网卡tcp连接端口数量是有限制的，我们调整ip_local_port_range使其能支撑60000个tcp连接：</p>
<pre><code>net.ipv4.ip_local_port_range = 1024 65535
</code></pre><p>对于100w连接来说，我们至少需要16台机器，但实际我只有可怜的3台4G内存的虚拟机。所以就要运维的童鞋在每台机器上面装了6块网卡。这样我就能建立100w的连接了。</p>
<p>测试客户端也非常简单，每秒向服务器请求1000个连接，但是需要注意的是，因为一台机器上面有多块网卡，所以在创建socket之后，我们需要将socket绑定到某一块网卡上面。</p>
<p>实际测试中，因为内存问题，每台机器顶多能支撑34w左右的tcp连接，对我来说已经足够，所以也懒得去调优了。</p>
<h1 id="CometServer_Linux调优">CometServer Linux调优</h1>
<p>首先我们需要调整最大打开文件数，在我的机器上面，nr_open最大的值为1048576，对我来说已经足够，所以我将最大文件描述符数量调整为1040000。</p>
<pre><code>fs.file-max = 1040000
</code></pre><p>然后就是对tcp一些系统参数的调优：</p>
<pre><code>net.core.somaxconn = 60000
net.core.rmem_default = 4096
net.core.wmem_default = 4096
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 4096 16777216
net.ipv4.tcp_wmem = 4096 4096 16777216
net.ipv4.tcp_mem = 786432 2097152 3145728
net.core.netdev_max_backlog = 60000
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_max_syn_backlog = 60000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_max_orphans = 131072
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_max_tw_buckets = 60000
net.netfilter.nf_conntrack_max = 1000000
net.netfilter.nf_conntrack_tcp_timeout_established = 1200
</code></pre><p>对于如何调节这些值，网上都是各有各的说法，建议直接man 7 tcp。我在实践中也会通过查看dmesg输出的tcp错误来动态调节。这里单独需要说明的是tcp buffer的设置，我最小和默认都是4k，这主要是考虑到推送服务器不需要太多太频繁的数据交互，所以需要尽可能的减少tcp的内存消耗。</p>
<h1 id="测试结果">测试结果</h1>
<p>实际的测试比较让我满意。</p>
<p>cometserver test8个进程cpu消耗都比较低，因为有轮训timing wheel然后再发送200的逻辑，所以铁定有cpu消耗，如果只是挂载，cpu应该会更低。</p>
<pre><code>PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                       
 4685 root      20   0  187m 171m  652 S 19.9  1.1   2:19.20 cometserver_test                                                                                                
 4691 root      20   0  191m 175m  656 S 16.6  1.1   2:17.80 cometserver_test                                                                                                
 4686 root      20   0  170m 155m  652 S 16.3  1.0   2:09.54 cometserver_test                                                                                                
 4690 root      20   0  183m 167m  652 S 16.3  1.1   2:11.44 cometserver_test                                                                                                
 4692 root      20   0  167m 152m  652 S 16.3  1.0   2:11.29 cometserver_test                                                                                                
 4689 root      20   0  167m 152m  652 S 15.3  1.0   2:03.08 cometserver_test                                                                                                
 4687 root      20   0  173m 158m  652 S 14.3  1.0   2:07.34 cometserver_test                                                                                                
 4688 root      20   0  129m 114m  652 S 12.3  0.7   1:35.77 cometserver_test
</code></pre><p>socket的统计情况：</p>
<pre><code>[root@localhost ~]# cat /proc/net/sockstat
sockets: used 1017305
TCP: inuse 1017147 orphan 0 tw 0 alloc 1017167 mem 404824
UDP: inuse 0 mem 0
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0
</code></pre><p>可以看到，总共有1017147个tcp链接，同时占用了将近4G（1017167是页数，需要乘以4096）的内存。</p>
<p>系统内存的情况：</p>
<pre><code>[root@localhost ~]# free
             total       used       free     shared    buffers     cached
Mem:      16334412   11210224    5124188          0     179424    1609300
-/+ buffers/cache:    9421500    6912912
Swap:      4194296          0    4194296
</code></pre><p>系统有16G内存，还有5G可用，所以不出意外单机应该还能承载更多的tcp连接。</p>
<h1 id="总结">总结</h1>
<p>使用libtnet开发的一个简单的comet server支撑了百万级的连接，加深了我对其应用的信心。</p>
<p>libnet地址<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">https://github.com/siddontang/libtnet</a>，欢迎围观。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T05:03:54.000Z"><a href="/2014/01/01/my-2013/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/my-2013/">我的2013</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="前言">前言</h1>
<p>每年到这个时候，总需要回顾过去，展望未来。2013这一年学到了很多东西，收货了很多，也成长了很多。主要在技术和生活上面，让自己有了记录一下的冲动。</p>
<h1 id="技术">技术</h1>
<p>在技术上面，这一年接触了很多新的东西，让自己眼界开阔不少，同时也开始自我提升，疯狂的在github上面玩<a href="https://github.com/siddontang" target="_blank" rel="external">开源</a>，只是很多都惨不忍睹。</p>
<h2 id="Openresty">Openresty</h2>
<p>今年最开始接触的新东西就是<a href="http://openresty.org/" target="_blank" rel="external">openresty</a>，一个集成nginx的web应用开发框架。</p>
<p>最开始，我们项目的架构采用的是前端nginx做proxy，然后将请求反向代理到后端python tornado的方式实现的。但是随着压力的增加以及一些新功能的上线，这套架构开始显现其局限性，首当其冲的在于慢，虽然可以通过增加tornado进程的方式来进行负载均衡处理，但我总觉得不是长久之计。</p>
<p>同时，随着业务逻辑的复杂，一些操作需要多个service协同完成，然后在返回给用户，而在什么地方组织多个service的数据以及逻辑也成了我们的一个难题。</p>
<p>鉴于上述几个原因，我开始研究openresty，因为之前有3年多lua开发的经验，所以非常容易上手。同时，通过研读openresty的一整套源码，真正的了解了nginx以及之上的openresty的运行机制。可以这么说，这段时间我从一个连nginx配置都不会写的小白程序员一跃成为了名义上精通nginx开发的屌丝程序员。</p>
<p>自然，我开始在项目中推广openresty，这也得到了大家的支持，现在虽然我们很多代码仍然是使用python在编写，但是对于很多高性能模块我们已经逐步转向了openresty。</p>
<p>在使用openresty的时候，还提交了几个bug，这点颇为自豪，同时也写了一些东西，譬如 <a href="http://siddontang.github.io/introduction-to-nginx/" target="_blank" rel="external">Introduction To Nginx</a></p>
<h2 id="Go">Go</h2>
<p>接触go纯属偶然，在上半年终于完结了一个持续时间特长的项目之后，整个组的童鞋都陷于一个无事可干的真空期，也就是在这段时间，第一次学习了go，立刻就被它的简单强大所吸引，尤其是在服务器并发编程方面，那可是非常的强悍。</p>
<p>于是，我带着两个完全不懂go的童鞋开始了我们推送服务器的编程之旅。最开始的时候，因为两位童鞋只会python，为了尽快的出功能，一些后台的服务采用tornado搭建，而我用go写了挂载大量长连接的comet服务。</p>
<p>这里不得不说go开发服务的快捷，在goroutine以及channel的机制下面，没有了层层的callback，没有了死锁，我只用了3天就弄出了comet，而且能持续稳定运行。</p>
<p>鉴于用go成功开发了comet，我让另外两个童鞋也开始用go重构先前写的python逻辑，进展也很顺利。</p>
<p>不过对于我来说，go现在最大的一个问题在于内存占用，go现在默认的stack大小为8k，对于需要挂载百万连接的comet来说，内存开销实在太大，虽然现在机器的配置完全不需要我担心，但总觉得有点不爽。不过如果优化，也是后续的事情了。</p>
<p>今年，对于我来说，竟然吃了两次螃蟹，第一个就是openresty，而第二个就是go，而且很幸运的是都能在项目中实施。</p>
<h2 id="Libtnet">Libtnet</h2>
<p>今年，我真正的开始了一个算是比较大的开源项目：<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>，它是一个参考tornado的c++高性能网络库。之所以写libtnet，主要是为了后续能用到comet上面，同时也让我自己对多年的网络编程做一个总结。</p>
<p>以前总说自己精通网络编程啥啥的，其实心里面也知道是用来忽悠的，毕竟精通这个词没多少年的沉淀是不可能的。但是通过写libtnet，不说精通，至少让我又对很多网络编程的东西了解了。</p>
<p>不过libtnet的问题在于使用c++进行开发，同时大量采用function + bind的开发模式，对于组内的童鞋来说理解上面还比较困难，如果在项目中实施很有可能面临只有我一个人维护的窘境。</p>
<h2 id="移动开发">移动开发</h2>
<p>今年没事的时候也涉猎了一些移动开发的方面，包括android以及ios。在android上面开发了一些小应用，只是都是自娱自乐。在ios上面使用cocos2d-x开发了一个小游戏demon，也当是消遣了。</p>
<p>不过在明年准备好好的尝试一下该领域的开发。尤其是ios上面，毕竟老婆都有了土豪金了，为了展示老公的程序员风采，再怎么也得弄一个出来。</p>
<h1 id="工作">工作</h1>
<p>今年在公司，我开始尝试站着上班，不得不说这对我工作效率的提升有很大的帮助，站着上班，不光减肥，还能让我专心工作，因为任何的聊天浏览网页都是一件很耗费力气的事情。这里也不得不佩服自己的毅力，每天竟然都能坚持站7，8个小时。</p>
<h1 id="生活">生活</h1>
<p>生活上面今年最主要就是几件事情：</p>
<ul>
<li>举办了婚礼</li>
<li>老婆怀了孩子</li>
<li>拿到驾照</li>
<li>买了小车</li>
</ul>
<p>可能对于我来说，明年在生活上面最大的事情就是要照顾孩子了。</p>
<h1 id="总结">总结</h1>
<p>总之，2013过的很快，但也过的很充实，希望自己在2014里面越来越好，能有更大的突破。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T01:32:56.000Z"><a href="/2014/01/01/learn-tornado-base/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/learn-tornado-base/">学习Tornado：基本</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="前言">前言</h1>
<p>在python里面，有许多<a href="http://wiki.python.org/moin/WebFrameworks" target="_blank" rel="external">web framework</a>。对于我来说，因为很长一段时间都在使用tornado，所以有了一些心得体会。</p>
<p>在这里，要说明一下，tornado采用的是<strong>2.4</strong>版本。</p>
<h1 id="架构">架构</h1>
<p>tornado是一个典型的prefork + io event loop的web server架构，<img src="https://raw.github.com/siddontang/blog/master/asserts/tornado-architecture.png" alt="Alt text" title="architecture"></p>
<p>从图上可以看出，tornado的架构是很简单清晰的。</p>
<ul>
<li>ioloop是tornado的核心，它就是一个io event loop，底层封装了select，epoll和kqueue，并根据不同的平台选择不同的实现。</li>
<li>iostream封装了non-blocking socket，用它来进行实际socket的数据读写。</li>
<li>TCPServer则是通过封装ioloop实现了一个简易的server，同时我们也在这里进行prefork的处理</li>
<li>HTTPServer则是继承TCPServer实现了一个能够处理http协议的server。</li>
<li>Application则是实际处理http请求的模块，HTTPServer收到http请求并解析之后会通过Application进行处理。</li>
<li>RequestHandler和WebSocketHandler则是注册给Application用来处理对应url的。</li>
<li>WSGIApplication则是tornado用于支持WSGI标准的接口，通过WSGIContainer包装共HTTPServer使用。</li>
</ul>
<h1 id="例子">例子</h1>
<p>通过上面的分析，直到tornado的架构是很简单明了的，所以自然我们也能够通过简短的一些代码就能搭建起自己的http server。以一个hello world开始：</p>
<pre><code>import tornado.web 
import tornado.httpserver 
import tornado.ioloop 

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write(&#39;Hello World&#39;)

application = tornado.web.Application([
    (r&quot;/&quot;, MainHandler),
])
http_server = tornado.httpserver.HTTPServer(application)
http_server.listen(8080)
tornado.ioloop.IOLoop.instance().start()
</code></pre><p>流程很简单，如下：</p>
<ul>
<li>定义了一个MainHandler，该handler用来处理对应url</li>
<li>生成一个Application实例，并设置url dispatch规则，(r”/“, MainHandler)就是一个规则，第一个pattern用来表明需要处理的url，内部会使用正则匹配，第二个就是对应url处理的handler</li>
<li>生成一个HTTPServer实例，使用Application进行构造，这样HTTPServer处理的http请求就会转给application处理。</li>
<li>HTTPServer监听一个端口8080，该listen socket会加入ioloop中，用于监听连接的建立。</li>
<li>ioloop启动，程序进入io event loop模式。</li>
</ul>
<p>当ioloop start之后，服务器就启动了，后续就是一个http server最基本的流程处理了。</p>
<h1 id="ReuqestHandler">ReuqestHandler</h1>
<h2 id="pattern_and_handler">pattern and handler</h2>
<p>从上面例子可以看出，搭建一个http server很简单，所以我们重点只需要考虑的是如何处理不同的url http请求，这也就是RequestHandler需要做的事情。</p>
<p>我们在创建Application的时候，会指定不同的url pattern需要处理的handler。如下：</p>
<pre><code>import tornado.web 
import tornado.httpserver 
import tornado.ioloop 

class Index1Handler(tornado.web.RequestHandler):
    def get(self):
        self.write(&#39;Index1&#39;)

class Index2Handler(tornado.web.RequestHandler):
    def get(self, data):
        self.write(&#39;Index2&#39;)
        self.write(data)

application = tornado.web.Application([
    (r&quot;/index1&quot;, Index1Handler),
    (r&quot;/index2/(\w+)&quot;, Index2Handler),
])

http_server = tornado.httpserver.HTTPServer(application)
http_server.listen(8080)
tornado.ioloop.IOLoop.instance().start()
</code></pre><p>在上面的例子中，我们有两个handler，分别处理url path为index1和index2的情况，对于index2来说，我们看到，它后面还需要匹配一个单词。我们通过curl访问如下：</p>
<pre><code>$ curl http://127.0.0.1:8080/index1
index1

$ curl http://127.0.0.1:8080/index2/abc
index2abc
</code></pre><h2 id="http_method">http method</h2>
<p>RequestHandler支持任何http mthod，包括get，post，head和delete，也就是说，tornado天生支持restful编程模型。</p>
<pre><code>class MainHandler(tornado.web.RequestHandler):
    def get(self):
        pass

    def post(self):
        pass

    def head(self):
        pass

    def delete(self):
        pass
</code></pre><p>从上面可以看到，我们只需要在handler里面实现自己的get，post，head和delete函数就可以了，这点再次说明tornado的简洁与强大。</p>
<h1 id="后续next">后续next</h1>
<p>这里，只是简单了介绍了一下tornado，后续将会从template，asynchronous，security等分别介绍一下。希望通过这个能让自己对tornado的理解更加深刻，同时也为后续使用其他python web framework做参考。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-29T08:27:12.000Z"><a href="/2013/12/29/libtnet-http/">12-29-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/29/libtnet-http/">Libtnet HTTP实现</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="HTTP">HTTP</h1>
<p>libtnet提供了简单的http支持，使用也很简单。</p>
<p>一个简单的http server：</p>
<pre><code>void onHandler(const HttpConnectionPtr_t&amp; conn, const HttpRequest&amp; request)
{
    HttpResponse resp;
    resp.statusCode = 200;
    resp.setContentType(&quot;text/html&quot;);
    resp.body.append(&quot;Hello World&quot;);    
    conn-&gt;send(resp);
}

TcpServer s;
HttpServer httpd(&amp;s);
httpd.setHttpCallback(&quot;/test&quot;, std::bind(&amp;onHandler, _1, _2));
httpd.listen(Address(80));
s.start(4);
</code></pre><p>我们对http server的<strong>“/test”</strong>注册了一个handler，当用户访问该url的时候，就会显示”Hello World”。</p>
<p>同样，http client的使用也很简单：</p>
<pre><code>void onResponse(IOLoop* loop, const HttpResponse&amp; resp)
{
    cout &lt;&lt; resp.body &lt;&lt; endl;
    loop-&gt;stop();
}

IOLoop loop;
HttpClientPtr_t client = std::make_shared&lt;HttpClient&gt;(&amp;loop);
client-&gt;request(&quot;http://127.0.0.1:80/test&quot;, std::bind(&amp;onResponse, &amp;loop, _1));
loop.start(); 
</code></pre><p>这里，我们使用了一个http client，向server请求”/test”的内容，当服务器有响应之后，会调用响应的回调函数。</p>
<h1 id="HTTP_Parser">HTTP Parser</h1>
<p>对于http的解析，我采用的是<a href="https://github.com/joyent/http-parser" target="_blank" rel="external">http parser</a>，因为它采用的是流式解析，同时非常容易集成进libtnet。</p>
<p>使用http parser只需要设置相应的回调函数即可。http parser有如下几种回调：</p>
<ul>
<li>message begin，解析开始的时候调用</li>
<li>url，解析url的时候调用</li>
<li>status complete，http response解析status的时候调用</li>
<li>header field，解析http header的field调用</li>
<li>header value，解析http header的value调用</li>
<li>headers complete，解析完成http header调用</li>
<li>body，解析http body调用</li>
<li>message complete，解析完成调用</li>
</ul>
<p>这里特别需要注意的是http header的解析，因为http parser将其拆分成了两种回调，所以我们在处理的时候需要记录上一次header callback是field的还是value的。在解析field的时候，如果上一次是value callback，那我们就需要将上一次解析的field和value保存下来，而该次的解析则是一个新的field了。</p>
<p>另外，http parser还提供了upgrade的支持，所以我们很方便的就能区分该次请求是否为websocket。</p>
<h1 id="Websocket">Websocket</h1>
<p>libtnet也提供了websocket的支持，现阶段，只支持<a href="http://tools.ietf.org/html/rfc6455" target="_blank" rel="external">RFC6455</a>。</p>
<p>当libtnet通过http parser发现该次请求为websocket的时候，就进入了websocket的流程。websocket的使用也很简单，当握手成功之后，后续的所有通讯就是纯粹的tcp通信了。</p>
<p>一个简单的websocket server：</p>
<pre><code>void onWsCallback(const WsConnectionPtr_t&amp; conn, WsEvent event, const void* context)
{
    switch(event)
    {
        case Ws_CloseEvent:
            break;
        case Ws_MessageEvent:
            {
                const string&amp; str = *(const string*)context;
                conn-&gt;send(str);
            }
            break;
        case Ws_PongEvent:
            break;
        default:
            break;
    }
}

TcpServer s;
HttpServer httpd(&amp;s);
httpd.setWsCallback(&quot;/push/ws&quot;, std::bind(&amp;onWsCallback, _1, _2, _3));    
httpd.listen(Address(80));
s.start();
</code></pre><p>可以看到，websocket的callback机制也类似于libtnet connection的callback机制，用户需通过event + context的方式来处理该次回调的数据。</p>
<p>libtnet对websocket的frame的处理参照的是tornado的websocket模块。也能够组合多frame的数据，外部只需要关注Ws_MessageEvent即可。</p>
<p>websocket client的实现也很简单：</p>
<pre><code>void onWsConnEvent(const WsConnectionPtr_t&amp; conn, WsEvent event, const void* context)
{
    switch(event)
    {
        case Ws_OpenEvent:
            conn-&gt;send(&quot;Hello world&quot;);
            break;    
        case Ws_MessageEvent:
            {
                const string&amp; msg = *(const string*)context;

                LOG_INFO(&quot;message %s&quot;, msg.c_str());
                conn-&gt;close();                        
            }
            break;
        default:
            break;
    }    
}

IOLoop loop;
WsClientPtr_t client = std::make_shared&lt;WsClient&gt;(&amp;loop);    
client-&gt;connect(&quot;ws://127.0.0.1:80/push/ws&quot;, std::bind(&amp;onWsConnEvent, _1, _2, _3));
loop.start();
</code></pre>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T10:28:49.000Z"><a href="/2013/12/28/libtnet-connection/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/libtnet-connection/">Libtnet Connection实现</a></h1>
  

    </header>
    <div class="entry">
      
        <p>libtnet只支持IPv4 TCP Connection，之所以这么做都是为了使得实现尽可能的简单。我们主要在Connection类中封装了对tcp连接的操作。</p>
<p>Connection继承自std::enable_shared_from_this，也就意味着外部我们会操作其shared_ptr<connection>，libtnet几乎所有的对象都采用智能指针的方式来进行内存管理。</connection></p>
<p>当Connection创建成功之后，会通过IOLoop的addHandler接口，将其绑定到ioloop上面：</p>
<pre><code>ConnectionPtr_t conn = shared_from_this();
m_loop-&gt;addHandler(m_fd, TNET_READ, std::bind(&amp;Connection::onHandler, conn, _1, _2));
</code></pre><p>因为我们直接在std::bind里面使用了shared_ptr，所以ioloop自然引用了该Connection，外部不需要在存储Connection，以防内存泄露。</p>
<p>对于一个connection而言，它只可能有几种状态，</p>
<ul>
<li>Connecting，表明正在尝试连接，发生在connect返回EINPROGRESS。</li>
<li>Connected，连接已经建立成功，发生在connect成功或者accept成功。</li>
<li>Disconnecting，表明连接正在断开，发生在用户主动调用shutDown之后。</li>
<li>Disconnected，连接已经断开，这时候对应的socket也会被close掉。</li>
</ul>
<h1 id="Event_Callback">Event Callback</h1>
<p>在Connection中，我们使用一个event callback来绑定相应事件的回调。主要有如下connection event：</p>
<ul>
<li>Conn_EstablishedEvent, 当server accept成功，创建了Connection对象之后，触发。</li>
<li>Conn_ConnectEvent，当client connect成功，触发。</li>
<li>Conn_ConnectingEvent，当client connect返回EINPROGRESS，触发。</li>
<li>Conn_ReadEvent，当连接可读，触发。</li>
<li>Conn_WriteCompleteEvent，当发送的数据都发送完毕，触发。</li>
<li>Conn_ErrorEvent，当连接有错误发生，触发。</li>
<li>Conn_CloseEvent，当连接主动或者被动关闭，触发。</li>
</ul>
<p>event callback原型如下：</p>
<pre><code>typedef shared_ptr&lt;Connection&gt; ConnectionPtr_t;
typedef std::function&lt;void (const ConnectionPtr_t&amp;, ConnEvent, const void* context)&gt; ConnEventCallback_t;
</code></pre><p>对应不同的事件，触发的时候context的内容不同。现阶段，只有ReadEvent的时候context为StackBuffer，原型如下：</p>
<pre><code>class StackBuffer
{
public:
    StackBuffer(const char* buf, size_t c) : buffer(buf), count(c) {}

    const char* buffer;
    size_t count;    
};
</code></pre><p>当连接可读的时候，Connection会将数据读取到栈上面，并用StackBuffer来指代，这样当外部处理ReadEvent的时候就能通过将context转换成StackBuffer获取到读取的数据。</p>
<p>下面简单说明一下一些设计上面的取舍：</p>
<ul>
<li><p>为什么只提供一个event callback，而不提供read callback，write complete callback，close callback多个回调接口？</p>
<p>  libtnet的所有callback都采用的是std::function实现，而该对象占用32字节，如果每个event都提供一个对应的callback，那么内存的开销会有点大，同时大部分时候很多callback我们是不感兴趣的。</p>
<p>  还有一个重要的原因在于只提供一个event callback，外部的一些对象就可以通过该callback跟Connection绑定，也就是将其自身的生命周期与Connection绑定在了一起，当Connection删除的时候该对象也自行删除。libtnet中，HttpConnection，WsConnection都是采用该方法，因为对于一个Http连接来说，如果底层的Tcp连接都断开无效了，基于Tcp的Http连接自然就无效了。</p>
</li>
<li><p>Connection为什么不缓存读取的数据，而是交由外部callback去处理？</p>
<p>  Connection作为一个底层的类，对于读取的数据，并不知道具体需要如何处理，所以还不如将数据直接发到外层，供上层实际的应用逻辑处理。但是如果后续Connection考虑支持ssl，那么就需要进行缓存数据了。</p>
</li>
</ul>
<h1 id="Write">Write</h1>
<p>Connection建立之后，默认只会在ioloop中设置TNET_READ事件，因为epoll采用的水平触发模式，如果直接设置TNET_WRITE事件，那么epoll会一直通知socket可写，但实际上并没有可以发送的数据。</p>
<p>所以，libtnet采用如下的方式进行数据发送：</p>
<ul>
<li>直接调用writev函数进行数据发送</li>
<li>如果数据未发送完毕，则向ioloop注册TNET_WRITE事件，下次触发可写的时候继续发送，直至发送成功，清除TNET_WRITE事件。</li>
</ul>
<p>另外，在发送的时候，我们还需要考虑signal pipe的情况，所以需要忽略该singal。使用如下方式：</p>
<pre><code>class IgnoreSigPipe
{
public:
    IgnoreSigPipe()
    {
        signal(SIGPIPE, SIG_IGN);    
    }    
};

static IgnoreSigPipe initObj;
</code></pre><p>当libtnet启动的时候，就忽略了signal pipe信号。虽然这样做稍微有一点副作用，但大部分时候我们并不需要关注SIGPIPE信号。</p>
<h1 id="Kick_Off_Connection">Kick Off Connection</h1>
<p>通常，为了处理不活跃连接，程序都会将每个connection设置一个timer，如果timer到了该连接仍然没有交互，则会删除该连接，否则则继续更新timer。另一种做法就是提供一个time wheel，将connection放置在该wheel中，如果有交互，则在wheel中移动。</p>
<p>libtnet采用了一种更简单，但是精度比较差的做法。</p>
<p>当server成功创建一个connection之后，将会添加到一个ConnChecker中，checker保存的是该connection的weak_ptr。每隔一段时间，checker检查一批connection：</p>
<ul>
<li>如果connection weak_ptr无法lock提升至shared_ptr，证明该连接已经删除，checker直接移除。</li>
<li>如果connection处于connecting状态，并且超过了设置的最大连接超时时间，shutDown该connection。</li>
<li>如果connection处于connected状态，并且在一段时间内没有任何交互，shutDown。</li>
</ul>
<p>ConnChecker的检查间隔以及每次检查步数都可以通过外部设置。使用ConnChecker虽然简单，但是在连接数过大的情况下面，一些过期的connection不能立刻被清理掉。对于这个问题，我觉得可以接受，一个连接一秒之后被关闭还是两秒之后被关闭，差别真的不大。如果我们真的需要对一些connection做精确的时间控制，那直接可以对其使用timer。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T10:28:23.000Z"><a href="/2013/12/28/libtnet-ioloop/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/libtnet-ioloop/">Libtnet事件循环</a></h1>
  

    </header>
    <div class="entry">
      
        <p>libtnet采用的是prefork + event loop的架构方式，prefork就是server在启动的时候预先fork多个子进程同时工作，而event loop则是基于epoll的事件处理机制。</p>
<p>在最新的linux系统中，提供了timerfd，eventfd，signalfd，加上原先的socket，大部分功能都可以抽象成io事件来处理了。而在libtnet中，这一切的基础就是IOLoop。</p>
<p>类似于tornado，libtnet的IOLoop也提供了相似的接口。其中最核心的就是以下三个：</p>
<pre><code>typedef std::function&lt;void (IOLoop*, int)&gt; IOHandler_t;

int addHandler(int fd, int events, const IOHandler_t&amp; handler);
int updateHandler(int fd, int events);
int removeHandler(int fd);  
</code></pre><p>对于任意的IO，我们可以注册感兴趣的事件（TNET_READ和TNET_WRITE），并绑定一个对应的callback回调。</p>
<p>callback的回调采用的是std::function的方式，这也就意味着，你可以在外部通过std::bind绑定任意不同的实现，再加上shared_ptr技术模拟闭包。</p>
<p>假设现在我们需要创建了一个socket对象，并将其添加到IOLoop中，我们可以这么做：</p>
<pre><code>std::shared_ptr&lt;Connection&gt; conn = std::make_shared&lt;Connection&gt;(socketfd);

ioloop-&gt;addHandler(socketfd, TNET_READ, std::bind(&amp;Connection::onHandler, conn, _1, _2));
</code></pre><p>这样，当该socket有读事件发生的时候，对应的onHandler就会被调用。在这里，我是用了shared_ptr技术，主要是为了方便进行对象生命周期的管理。</p>
<p>在上面的例子中，因为std::bind的时候引用了conn，只要不将socketfd进行removeHandler，conn对象就会一直存在。所以libtnet在IOLoop内部，自行维护了conn对象的生命周期。外面不需要在将其保存到另一个地方（如果真保存了该shared_ptr的conn，反而会引起内存泄露）。在libtnet的基础模块中，我都使用的是weak_ptr来保存相关对象，每次使用都通过lock来判定是否该对象存活。</p>
<p>在IOLoop内部，我使用一个vector来存放注册的handler，vector的索引就是io的fd。这样，我们通过io的fd就可以非常快速的查找到对应的handler了。为什么可以这样设计，是因为在linux系统中，进程中新建文件的file descriptor都是系统当前最小的可用整数。譬如，我创建了一个socket，fd为10，然后我关闭了该socket，再次新建一个socket，这时候新的socket的fd仍然为最小可用的整数，也就是10。</p>
<h1 id="EPoll">EPoll</h1>
<p>提到linux下面的高性能网络编程，epoll是一个铁定绕不开的话题，关于epoll的使用，网上有太多的讲解，这里就不展开了。</p>
<p>libtnet在Poller中集成了epoll，参考了libev的实现。epoll有两种工作模式，水平触发和边沿触发，各有利弊。libtnet使用的是水平触发方式，主要原因在于水平触发方式在有消息但是没处理的时候会一直通知你处理，实现起来不容易出错，也比较简单。</p>
<h2 id="fork_and_epoll_create">fork and epoll_create</h2>
<p>这里顺便记录一下我在实现prefork模型的时候遇到的一个坑。这个问题就是epoll fd应该在fork之前还是之后创建？</p>
<p>大家都知道，linux fork的时候采用COW（copy on write）方式复制父进程的内容，然后我想当然的以为各个子进程会拥有独立的epoll内核空间，于是在fork之前创建了epoll fd。但是后面我却惊奇的发现一个子进程对epoll的操作竟然会影响另一个子进程。也就是说，各个子进程共享了父进程的epoll内核空间。</p>
<p>所以，epoll fd的创建应该在fork之后，各个子进程独立创建。</p>
<h1 id="Example">Example</h1>
<h2 id="Timer">Timer</h2>
<p>IOLoop提供了一个简单的runAfter函数，用以实现定时器功能，使用非常简单：</p>
<pre><code>void func(IOLoop* loop)
{
    cout &lt;&lt; &quot;hello world&quot; &lt;&lt; endl;
    loop-&gt;stop();
}

IOLoop loop;
loop.runAfter(10 * 1000， std::bind(&amp;func, &amp;loop));
loop.start();
</code></pre><p>loop启动十秒之后，会打印hello world，然后整个loop退出。更多定制化的timer使用，可以使用libtnet提供的Timer class。</p>
<h2 id="Callback">Callback</h2>
<p>libtnet是一个单线程单ioloop的模型，但是不排除仍然会有其他线程希望与IOLoop进行通信，所以IOLoop提供了addCallback功能，这是libtnet唯一一个线程安全的函数。因为加入callback是一个很快速的操作，IOLoop使用了spinlock。在IOLoop每次循环的末尾，会将全部的callback取出，依次执行。</p>
<pre><code>void callback(IOLoop* loop)
{
    cout &lt;&lt; &quot;tell to exit&quot; &lt;&lt; endl;
    loop-&gt;stop();
}

IOLoop loop;
loop.addCallback(std::bind(&amp;func, &amp;loop));
loop.start();
</code></pre>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T10:25:49.000Z"><a href="/2013/12/28/libtnet-introduction/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/libtnet-introduction/">高性能网络库Libtnet介绍</a></h1>
  

    </header>
    <div class="entry">
      
        <p>libtnet是一个用c++编写的高性能网络库，它在设计上面主要参考tornado，为服务端网络编程提供简洁而高效的接口，非常易于使用。</p>
<h1 id="Echo_Server">Echo Server</h1>
<pre><code>void onConnEvent(const ConnectionPtr_t&amp; conn, ConnEvent event, const void* context)
{
    switch(event)
    {
        case Conn_ReadEvent:
            {
                const StackBuffer* buffer = static_cast&lt;const StackBuffer*&gt;(context);
                conn-&gt;send(string(buffer-&gt;buffer, buffer-&gt;count));
            }
            break;
        default:
            break;
    }    
}

int main()
{
    TcpServer s;
    s.listen(Address(11181), std::bind(&amp;onConnEvent, _1, _2, _3));

    s.start();

    return 0;
}
</code></pre><p>当程序启动，服务监听本地11181端口，我们使用telnet测试：</p>
<pre><code>root@tnet:~# telnet 127.0.0.1 11181
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is &#39;^]&#39;.
hello world
hello world
</code></pre><p>可以看到，libtnet在使用上面非常简单，在listen的时候，指定一个回调函数，当有新的连接到来的时候，该回调函数就会与该connection进行绑定，这样该connection的任何事件都能通过回调进行处理。</p>
<p>在上面那个例子中，我们只关心了connection的ReadEvent，也就是读事件，然后将读取到的所有数据原封不动的转发回去。</p>
<h1 id="Http_Server">Http Server</h1>
<pre><code>void onHandler(const HttpConnectionPtr_t&amp; conn, const HttpRequest&amp; request)
{
    HttpResponse resp;
    resp.statusCode = 200;
    resp.body.append(&quot;Hello World&quot;);

    conn-&gt;send(resp);
}

int main()
{
    TcpServer s;
    HttpServer httpd(&amp;s);   
    httpd.setHttpCallback(&quot;/abc&quot;, std::bind(&amp;onHandler, _1, _2));

    httpd.listen(Address(11181));    
    s.start(4);

    return 0;
} 
</code></pre><p>当server启动，程序使用本机11181端口提供http服务。我们使用curl测试。</p>
<pre><code>curl http://127.0.0.1:11181/abc

return: hello world
</code></pre><p>可以看到，使用http server也非常简单，我们只需要对相应的路径绑定一个callback回调，当有请求发生的时候，对应的callback执行。</p>
<p>使用benchmark测试，发现性能也不错RPS能到16000+，在512MB，单核CPU下面进行ab压测，具体可以参考<a href="https://github.com/siddontang/libtnet/wiki/Benchmark" target="_blank" rel="external">benchmark</a>。</p>
<h1 id="Webscoket_Server">Webscoket Server</h1>
<pre><code>void onWsCallback(const WsConnectionPtr_t&amp; conn, WsEvent event, const void* context)
{
    switch(event)
    {
        case Ws_MessageEvent:
            {
                const string&amp; str = *(const string*)context;
                conn-&gt;send(&quot;hello &quot; + str);
            }
            break;
        default:
            break;
    }
}

int main()
{
    TcpServer s;

    HttpServer httpd(&amp;s);

    httpd.setWsCallback(&quot;/push/ws&quot;, std::bind(&amp;onWsCallback, _1, _2, _3));    

    httpd.listen(Address(11181));

    s.start();

    return 0; 
}
</code></pre><p>libtnet同样提供了websocket <a href="http://tools.ietf.org/html/rfc6455" target="_blank" rel="external">RFC6455</a>的支持，使用方法同http server，只需要对相应的path注册特定的回调，就可以很方便的进行websocket交互。</p>
<h1 id="Client">Client</h1>
<p>libtnet不光提供了server层面的相关功能，同时也集成了<strong>http client</strong>，<strong>websocket client</strong>以及<strong>redis client</strong>。使得libtnet也能方便的进行客户端网络功能的开发。对于具体的使用，可以参考<a href="https://github.com/siddontang/libtnet/tree/master/test" target="_blank" rel="external">example</a>。</p>
<h1 id="设计上面的考量">设计上面的考量</h1>
<p>libtnet只支持linux版本，虽然做一个跨平台的通用库是一件吸引力非常大的事情，但是综合考虑之后，我决定只做linux版本的，主要有以下几个原因：</p>
<ul>
<li>Linux下面使用prefork + epoll是一种非常高效的网络编程模型，性能强悍，实现简单。虽然unix下面有kqueue，windows下面有IOCP，但是没必要为了适配所有得操作系统将代码写的复杂。</li>
<li>Linux在系统层面上面就提供了很多高性能的函数，譬如timerfd，eventfd等，不光性能提升，同时也简化了很多代码实现。</li>
<li>Linux在服务器编程领域的使用率很高，专门做精一个平台就够了。</li>
</ul>
<p>因为高性能的网络编程通常都是使用异步的编程方式，所以经常可以看到代码被异步拆的特别分散，不利于编写。所以我在libtnet里面大量的使用了c++ bind以及shared_ptr技术，用来模拟函数闭包，以及解决对象生命周期管理问题，简化代码的编写。并且我也使用了c++ 0x相关技术，gcc的版本至少要在4.4以上。</p>
<p>对于如何设计以及使用libtnet，后续我会有更加详细的说明。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T10:24:58.000Z"><a href="/2013/12/28/mysql-index-point/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/mysql-index-point/">MySQL索引研究</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="介绍">介绍</h1>
<p>这段时间在重构server，尤其是重新设计表结构以满足功能需求以及后续的性能需求。因为我们使用的是mysql，所以为了支持更多的并发访问，对大数据量的表设计一些index是必不可少的，而现在看我当时设计的index，又发现了很多不足的地方。同时又因为深入重新在学习了解了一些index知识，觉得有必要记录一下，以免自己再犯同样的错误。</p>
<p>假设有如下的表结构，后续所有的例子都通过该表来说明：</p>
<pre><code>table tbl,
fileid int,
groupid int,
opver int,
name varchar(1024),
entid int,
primary key (fileid),
engine = innodb
</code></pre><h1 id="聚集索引">聚集索引</h1>
<p>因为使用的mysql engine是innodb，所以对于主键使用的是聚簇索引，对于聚簇索引我们知道它是吧数据存放到index里面的，也就是通过主键查询就能直接定位到数据，非常方便。</p>
<p>以前我受到的教育就是，select *是邪恶的，因为会取出所有列的数据。诚然，对于数据的获取，我们是按需索取，这样就能极大的减少网络传输的消耗。正因为以前有这样的认识，我自然认为innodb在server端io数据读取的时候也是按需索取。但其实不是这样。</p>
<p>在innodb里面，数据是按照页来存放的，通常一页为16k，每一页至少存放2条数据，也就是说，每一条数据在最大为8k。这里有童鞋就可能疑惑了，那如果我表结构里面有blob，text这种类型的肿么办？对于这种大数据类型的，innodb会将其存放到一个overflow page里面去，然后数据页只存放对应的指针。同样，innodb也是按照页来读取数据，也就是说一次读取16k。</p>
<p>对于上面我们的表结构，是完全能够存放到同一个page里面的，也就是说，我们即使select的时候没有选择对应的列，在innodb层面也仍然会将该数据读出，也就说无论怎样，io消耗是一样的。具体可以参考<a href="http://www.zhaokunyao.com/archives/1558" target="_blank" rel="external">这篇</a>。</p>
<p>这里在谈谈主键id类型选择的问题，因为innodb采用的是b+tree来进行数据的存储，网上对于是否采用自增id或者guid这种争论满天飞，现在我的想法如下：</p>
<ul>
<li>guid会导致主键过于分散，以至于在数据进行插入，删除，更新的时候会进行b+tree的频繁分裂，同时读取数据的时候因为过于分散会导致io随即读取，性能不高。</li>
<li>自增id虽然能让io进行顺序读取，但是如果服务器数据量太大造成分表了，对于每张表的auto increment的初始值维护也是一件很烦的事情。</li>
<li>鉴于上面那种情况，我们现在考虑的做法就是提供一个id生成器，用来保证主键的顺序递增，同时又保证全局的完全唯一。这个id生成器很容易，使用redis的incr就可以搞定，而且还可以通过incrby进行批量申请，性能妥妥的。</li>
</ul>
<h1 id="联合索引">联合索引</h1>
<p>一般为了查询方便，我们有可能在多列上面建立联合索引，譬如在(groupid, opver)上面，那么我就可以很方便的使用如下索引：</p>
<pre><code>select * from tbl where groupid in (1,2,3)
select * from tbl where groupid = 1 and opver &gt; 10 and opver &lt; 100
select * from tbl where groupid = 1 and opver &gt; 10 order by opver desc limit 10
</code></pre><p>在上面那些查询语句立马，index都能很好的工作，然后我就自认为下面这种的也行：</p>
<pre><code>select * from tbl where groupid in (1,2,3) and opver &gt; 10 order by opver desc limit 10
</code></pre><p>对于这个查询，我自然想到的就是首先mysql会通过索引选出groupid为1，2，3的，然后再在这个结果集里面选出opver满足的进行排序。可真的是这样吗。使用explain之后，才发现，在extra那一栏生生的出现了using filesort。也就是说mysql并没有对opver使用索引进行order by，而是使用了filesort，这里我们的索引在groupid之后就无效了。</p>
<p>其实这种情况可以用通用的情况来归纳，在一个组合索引里面，如果出现了多个范围查询，那么mysql不可能为每一个范围都使用索引，一般碰到第一个范围之后就会停止索引工作了。</p>
<p>这里给了我一个深刻的教训，就是设计好表结构以及index之后，不要自认为就能按照我想的方式工作了，最好需要explain一下，看看mysql到底是怎么工作的，不然出了性能瓶颈都不知道。</p>
<h1 id="覆盖索引">覆盖索引</h1>
<p>当我们需要查询的数据通过索引就能查到的时候，这种索引就叫做覆盖索引。譬如下面这些:</p>
<pre><code>select groupid, opver from tbl where groupid = 1 and opver &gt; 10;

select fileid, groupid, opver from tbl where groupid = 1 and opver &gt; 10 order by opver limit 10
</code></pre><p>因为我们在(groupid, opver)上面建立了索引，那么当我们查询的所有列包含在该索引里面的时候，我们就可以通过覆盖索引直接找到。通过explain可以看到extra里面有using index，表明使用了覆盖索引。</p>
<p>对于第二个例子，为什么fileid也可以使用覆盖索引呢，因为对于innodb来说，非聚簇索引保存的是主键id，也就是fileid，所以通过索引也能够直接找到fileid。</p>
<p>使用覆盖索引的好处是很明显的，数据只需要通过索引就能获取，而不需要通过主键在进行随机的io读取。一个很简单的例子。</p>
<pre><code>select entid from t1 order by opver limit 100000, 10;

select entid from t1 inner join (select fileid from t1 order by opver limit 100000, 10) as ac using(fileid)
</code></pre><p>因为我们需要查询entid，而没有任何一个索引能覆盖entid，这里我们给opver单独建立一个索引。对于第一个查询，mysql因为需要获取entid，所以会通过fileid查到实际的数据并取出，在进行排序，同时因为limit的跨度很大，mysql会丢弃很多数据，所以导致该查询会很慢。</p>
<p>而对于第二个查询，在join立马，我们只是通过覆盖索引找到了fileid，而不需要通过fileid去随机io读取实际数据，取出所有的fileid之后，在取出了entid。虽然该查询有一个join操作，但是因为join的性能很高，所以比第一个查询快很多。</p>
<p>关于索引覆盖，网上有很多关于这个的介绍，譬如<a href="http://hi.baidu.com/shinegun/item/1f36f5517bd8b4dfd48bac2d" target="_blank" rel="external">这个</a>，总之用好了覆盖索引，是能极大提升效率的，但是也不可能为了覆盖索引去建立覆盖索引，如果索引泛滥了，也是一件很头疼的事情。</p>
<h1 id="filesort">filesort</h1>
<p>对于先前联合索引遇到的问题，我发现如果不重新设计整个的查询机制，按照现有做法是完全不能避免filesort的。因为我们的业务逻辑就是需要查询一批groupid里面，opver大于某个值的所有数据。既然避免不了，那我们就可以看看到底性能高不高。</p>
<p>mysql对于filesort，有两种处理方式，一种是单路排序，一种是双路排序。单路排序直接就是取出选择的字段，然后在sort buffer中排序。而双路排序则是取出排序字段以及主键，排序完成之后在通过主键获取实际的数据。可以看出，双路排序会有两次io操作，自然性能会差一点。</p>
<p>早期mysql采用的是双路排序，现在采用的是单路，但是如果我们需要排序的数据超过了配置的sort buffer空间，或者需要排序的单行数据长度大于max_length_for_sort_data，mysql仍然会使用单路排序。mysql默认max_length_for_sort_data为1024，这个需要根据实际数据长度进行配置。</p>
<p>不过如果能把mysql的硬盘换成ssd，或者把内存加到100g以上，我觉得这个filesort也还真不成问题了。</p>
<h1 id="后续">后续</h1>
<p>这几天对于mysql索引的研究就先记录一下，感觉对mysql的理解还需要加深，后续如果还有什么新的感想也会在这里记录。同时bs一下自己，号称用了mysql这么多年，到现在了还有很多东西没有领会，是时候开始好好深入研究了。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  
    <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T08:46:53.000Z"><a href="/2013/12/28/learn-tornado-asynchronous/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/learn-tornado-asynchronous/">学习Tornado：异步</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="why_asynchronous">why asynchronous</h1>
<p>tornado是一个异步web framework，说是异步，是因为tornado server与client的网络交互是异步的，底层基于io event loop。但是如果client请求server处理的handler里面有一个阻塞的耗时操作，那么整体的server性能就会下降。</p>
<pre><code>def MainHandler(tornado.web.RequestHandler):
    def get(self):
        client = tornado.httpclient.HttpClient()
        response = client.fetch(&quot;http://www.google.com/&quot;)
        self.write(&#39;Hello World&#39;)
</code></pre><p>在上面的例子中，tornado server的整体性能依赖于访问google的时间，如果访问google的时间比较长，就会导致整体server的阻塞。所以，为了提升整体的server性能，我们需要一套机制，使得handler处理都能够通过异步的方式实现。</p>
<p>幸运的是，tornado提供了一套异步机制，方便我们实现自己的异步操作。当handler处理需要进行其余的网络操作的时候，tornado提供了一个async http client用来支持异步。</p>
<pre><code>def MainHandler(tornado.web.RequestHandler):
    @tornado.web.asynchronous
    def get(self):
        client = tornado.httpclient.AsyncHTTPClient()
        def callback(response):
            self.write(&quot;Hello World&quot;)
            self.finish()

        client.fetch(&quot;http://www.google.com/&quot;, callback)
</code></pre><p>上面的例子，主要有几个变化：</p>
<ul>
<li>使用asynchronous decorator，它主要设置_auto_finish为false，这样handler的get函数返回的时候tornado就不会关闭与client的连接。</li>
<li>使用AsyncHttpClient，fetch的时候提供callback函数，这样当fetch http请求完成的时候才会去调用callback，而不会阻塞。</li>
<li>callback调用完成之后通过finish结束与client的连接。</li>
</ul>
<h1 id="asynchronous_flaw">asynchronous flaw</h1>
<p>异步操作是一个很强大的操作，但是它也有一些缺陷。最主要的问题就是在于callback导致了代码逻辑的拆分。对于程序员来说，同步顺序的想法是一个很自然的习惯，但是异步打破了这种顺序性，导致代码编写的困难。这点，对于写nodejs的童鞋来说，可能深有体会，如果所有的操作都是异步，那么最终我们的代码可能写成这样:</p>
<pre><code>def MainHandler(tornado.web.RequestHandler):
    @tornado.web.asynchronous
    def get(self):
        client = tornado.httpclient.AsyncHTTPClient()
        def callback1(response):

            def callback2(response):
                self.write(&quot;Hello World&quot;)
                self.finish()
            client.fetch(&quot;http://www.google.com&quot;, callback2)

        client.fetch(&quot;http://www.google.com/&quot;, callback1)
</code></pre><p>也就是说，我们可能会写出callback嵌套callback的情况，这个极大的会影响代码的阅读与流程的实现。</p>
<h1 id="synchronous">synchronous</h1>
<p>我个人认为，异步拆散了代码流程这个问题不大，毕竟如果一个逻辑需要过多的嵌套callback来实现的话，那么我们就需要考虑这个逻辑是否合理了，所以异步一般也不会有过多的嵌套层次。</p>
<p>虽然我认为异步的callback问题不大，但是如果仍然能够有一套机制，使得异步能够顺序化，那么对于代码逻辑的编写来说，会方便很多。tornado有一些机制来实现。</p>
<h2 id="yield">yield</h2>
<p>在python里面如果一个函数内部实现了yield，那么这个函数就不是函数了，而是一个生成器，它的整个运行机制也跟普通函数不一样，举一个例子:</p>
<pre><code>def test_yield():
    print &#39;yield 1&#39;
    a = yield &#39;yielded&#39;
    print &#39;over&#39;, a

t = test_yield()
print &#39;main&#39;, type(t)
ret = t.send(None)
print ret
try:
    t.send(&#39;hello yield&#39;)
except StopIteration:
    print &#39;yield over&#39;
</code></pre><p>输出结果如下：</p>
<pre><code>main &lt;type &#39;generator&#39;&gt;
yield 1
yielded
over hello yield
yield over
</code></pre><p>从上面可以看到，test_yield是一个生成器，当它第一次调用的时候，只是生成了一个Generator，不会执行。当第一次调用send的时候，生成器被resume，开始执行，然后碰到yield，就挂起，等待下一次被send唤醒。当生成器执行完毕，会抛出StopIteration异常，供外部send的地方知晓。</p>
<p>因为yield很方便的提供了一套函数挂起，运行的机制，所以我们能够通过yield来将原本是异步的流程变成同步的。</p>
<h2 id="gen">gen</h2>
<p>tornado有一个gen模块，提供了Task和Callback/Wait机制用来支持同步模型，以task为例：</p>
<pre><code>def MainHandler(tornado.web.RequestHandler):
    @tornado.web.asynchronous
    @tornado.gen.engine
    def get(self):
        client = tornado.httpclient.AsyncHTTPClient()
        response = yield tornado.gen.Task(client.fetch, &quot;http://www.google.com/&quot;)
        self.write(&quot;Hello World&quot;)
        self.finish()
</code></pre><p>可以看到，tornado的gen模块就是通过yield来进行同步化的。主要有如下需要注意的地方：</p>
<ul>
<li>使用gen.engine的decorator，该函数主要就是用来管理generator的流程控制。</li>
<li>使用了gen.Task，在gen.Task内部，会生成一个callback函数，传给async fetch，并执行fetch，因为fetch是一个异步操作，所以会很快返回。</li>
<li>在gen.Task返回之后使用yield，挂起</li>
<li>当fetch的callback执行之后，唤醒挂起的流程继续执行。</li>
</ul>
<p>可以看到，使用gen和yield之后，原先的异步逻辑变成了同步流程，在代码的阅读性上面就有不错的提升，不过对于不熟悉yield的童鞋来说，开始反而会很迷惑，不过只要理解了yield，那就很容易了。</p>
<h2 id="greenlet">greenlet</h2>
<p>虽然yield很强大，但是它只能挂起当前函数，而无法挂起整个堆栈，这个怎么说呢，譬如我想实现下面的功能:</p>
<pre><code>def a():
    yield 1

def b():
    a()

t = b()
t.send(None)
</code></pre><p>这个通过yield是无法实现的，也就是说，a里面使用yield，它是一个生成器，但是a的挂起无法将b也同时挂起。也就是说，我们需要一套机制，使得堆栈在任何地方都能够被挂起和恢复，能方便的进行栈切换，而这套机制就是coroutine。</p>
<p>最开始使用coroutine是在lua里面，它原生提供了coroutine的支持。然后在使用luajit的时候，发现内部是基于fiber(win)和context(unix)，也就是说，不光lua，其实c/c++我们也能实现coroutine。现在研究了go，也是内置coroutine，并且这里极力推荐一篇<a href="http://concur.rspace.googlecode.com/hg/talk/concur.html#table-of-contents" target="_blank" rel="external">slide</a>。</p>
<p>python没有原生提供coroutine，不知道以后会不会有。但有一个greenlet，能帮我们实现coroutine机制。而且还有人专门写好了tornado与greenlet结合的模块，叫做<a href="https://github.com/mopub/greenlet-tornado" target="_blank" rel="external">greenlet_tornado</a>，使用也很简单</p>
<pre><code>class MainHandler(tornado.web.RequestHandler):
    @greenlet_asynchronous
    def get(self):
        response = greenlet_fetch(&#39;http://www.google.com&#39;)
        self.write(&quot;Hello World&quot;)
        self.finish()
</code></pre><p>可以看到，使用greenlet，能更方便的实现代码逻辑，这点比使用gen更方便，因为这些连写代码的童鞋都不用去纠结yield问题了。</p>
<h1 id="总结">总结</h1>
<p>这里只是简单的介绍了tornado的一些异步处理流程，以及将异步同步化的一些方法。另外，这里举得例子都是网络http请求方面的，但是server处理请求的时候，可能还需要进行数据库，本地文件的操作，而这些也是同步阻塞耗时操作，同样可以通过异步来解决的，这里就不详细说明了。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



  

  <nav id="pagination">
  
    <a href="/archives/page/5/" class="alignleft prev">上一页</a>
  
  
    <a href="/archives/page/7/" class="alignright next">下一页</a>
  
  <div class="clearfix"></div>
</nav>
</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:siddontang.com">
  </form>
</div>

  <div class="widget tag">
  <h3 class="title">Github</h3>
  <ul class="entry">
  <li><a href="https://github.com/siddontang/ledisdb" target="_blank">LedisDB</a><small>Fast NoSQL</small></li>
  <li><a href="https://github.com/siddontang/mixer" target="_blank">Mixer</a><small>MySQL Proxy with Go</small></li>
  <li><a href="https://github.com/siddontang/libtnet" target="_blank">Libtnet</a><small>High Performance EventLoop</small></li>
  <li><a href="https://github.com/siddontang/moonmq" target="_blank">MoonMQ</a><small>Fast Message Queue</small></li>
  <li><a href="https://github.com/siddontang" target="_blank">More...</a></li>
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/c++/">c++</a><small>7</small></li>
  
    <li><a href="/categories/elasticsearch/">elasticsearch</a><small>1</small></li>
  
    <li><a href="/categories/go/">go</a><small>30</small></li>
  
    <li><a href="/categories/leetcode/">leetcode</a><small>1</small></li>
  
    <li><a href="/categories/lua/">lua</a><small>2</small></li>
  
    <li><a href="/categories/mysql/">mysql</a><small>5</small></li>
  
    <li><a href="/categories/nginx/">nginx</a><small>2</small></li>
  
    <li><a href="/categories/program/">program</a><small>16</small></li>
  
    <li><a href="/categories/python/">python</a><small>6</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">标签云</h3>
  <div class="entry">
    <a href="/tags/algorithm/" style="font-size: 10.00px;">algorithm</a><a href="/tags/c++/" style="font-size: 18.75px;">c++</a><a href="/tags/celery/" style="font-size: 10.00px;">celery</a><a href="/tags/docker/" style="font-size: 10.00px;">docker</a><a href="/tags/go/" style="font-size: 20.00px;">go</a><a href="/tags/gopkg/" style="font-size: 10.00px;">gopkg</a><a href="/tags/goroutine/" style="font-size: 10.00px;">goroutine</a><a href="/tags/json/" style="font-size: 10.00px;">json</a><a href="/tags/ledisdb/" style="font-size: 17.50px;">ledisdb</a><a href="/tags/leetcode/" style="font-size: 10.00px;">leetcode</a><a href="/tags/leveldb/" style="font-size: 10.00px;">leveldb</a><a href="/tags/libtnet/" style="font-size: 16.25px;">libtnet</a><a href="/tags/log/" style="font-size: 10.00px;">log</a><a href="/tags/lua/" style="font-size: 13.75px;">lua</a><a href="/tags/message-queue/" style="font-size: 10.00px;">message queue</a><a href="/tags/mixer/" style="font-size: 12.50px;">mixer</a><a href="/tags/moonmq/" style="font-size: 11.25px;">moonmq</a><a href="/tags/mysql/" style="font-size: 18.75px;">mysql</a><a href="/tags/nginx/" style="font-size: 11.25px;">nginx</a><a href="/tags/nosql/" style="font-size: 15.00px;">nosql</a><a href="/tags/polaris/" style="font-size: 11.25px;">polaris</a><a href="/tags/pprof/" style="font-size: 10.00px;">pprof</a><a href="/tags/proxy/" style="font-size: 10.00px;">proxy</a><a href="/tags/python/" style="font-size: 17.50px;">python</a><a href="/tags/reflect/" style="font-size: 10.00px;">reflect</a><a href="/tags/restful/" style="font-size: 10.00px;">restful</a><a href="/tags/rpc/" style="font-size: 10.00px;">rpc</a><a href="/tags/timer/" style="font-size: 11.25px;">timer</a><a href="/tags/timingwheel/" style="font-size: 10.00px;">timingwheel</a><a href="/tags/tornado/" style="font-size: 12.50px;">tornado</a><a href="/tags/xcodis/" style="font-size: 11.25px;">xcodis</a><a href="/tags/zookeeper/" style="font-size: 10.00px;">zookeeper</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 SiddonTang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'siddontang';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>