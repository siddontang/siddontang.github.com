<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Siddon's Blog]]></title>
  <subtitle><![CDATA[My thought for program]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://siddontang.com/"/>
  <updated>2015-01-25T13:07:16.917Z</updated>
  <id>http://siddontang.com/</id>
  
  <author>
    <name><![CDATA[SiddonTang]]></name>
    <email><![CDATA[siddontang@gmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[MySQL问题两则]]></title>
    <link href="http://siddontang.com/2015/01/25/two-mysql-problems/"/>
    <id>http://siddontang.com/2015/01/25/two-mysql-problems/</id>
    <published>2015-01-25T13:06:26.000Z</published>
    <updated>2015-01-25T13:07:08.000Z</updated>
    <content type="html"><![CDATA[<p>这段时间处理了两个比较有意思的MySQL问题，一个死锁的，一个优化的，陡然发现其实自己对MySQL的理解还不深入，很多运行机制也是知其然但不知其所以然，后续还需要好好恶补一下底层知识。</p>
<h2 id="一次不可思议的死锁">一次不可思议的死锁</h2>
<p>假设有如下表结构：</p>
<pre><code>mysql&gt; show create table tt \G;
*************************** 1. row ***************************
       Table: tt
Create Table: CREATE TABLE `tt` (
  `id` int(11) NOT NULL DEFAULT &#39;0&#39;,
  `fileid` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `fileid` (`fileid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
1 row in set (0.00 sec)
</code></pre><p>启动三个shell，连接MySQL，然后<code>begin</code>开启一个事务，各个shell分别执行对应的更新语句，</p>
<p>shell 1：</p>
<pre><code>shell 1&gt; update tt set id = 2 where fileid = 1;
</code></pre><p>shell 2：</p>
<pre><code>shell 2&gt; update tt set id = 3 where fileid = 1;
</code></pre><p>shell 3：</p>
<pre><code>shell 3&gt; update tt set id = 4 where fileid = 1;
</code></pre><p>假设shell 1先执行，这时候2和3会block，然后shell 1 commit提交，我们发现shell 2执行成功，但是3出现死锁错误，通过<code>show engine innodb status</code>我们得到如下死锁信息:</p>
<pre><code>------------------------
LATEST DETECTED DEADLOCK
------------------------
2015-01-23 14:24:16 10ceed000
*** (1) TRANSACTION:
TRANSACTION 24897, ACTIVE 3 sec starting index read
mysql tables in use 1, locked 1
LOCK WAIT 2 lock struct(s), heap size 360, 1 row lock(s)
MySQL thread id 8, OS thread handle 0x10cea5000, query id 138 127.0.0.1 root updating
update tt set id = 4 where fileid = 1
*** (1) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24897 lock_mode X locks rec but not gap waiting
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** (2) TRANSACTION:
TRANSACTION 24896, ACTIVE 8 sec updating or deleting
mysql tables in use 1, locked 1
4 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 2
MySQL thread id 7, OS thread handle 0x10ceed000, query id 136 127.0.0.1 root updating
update tt set id = 3 where fileid = 1
*** (2) HOLDS THE LOCK(S):
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24896 lock_mode X locks rec but not gap
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** (2) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24896 lock mode S waiting
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** WE ROLL BACK TRANSACTION (1)
------------
</code></pre><p>刚开始碰到这个死锁问题，真心觉得很奇怪，每个事务一条语句，通过一个唯一索引去更新同一条记录，正常来说完全不可能发生死锁，但确确实实发生了。笔者百思不得其解，幸好有google，然后搜到了这篇，<a href="http://hedengcheng.com/?p=844" target="_blank" rel="external">一个最不可思议的MySQL死锁分析</a>，虽然触发情况不一样，但是死锁原理都应该类似的，后续如果有精力，笔者将好好深入研究一下。</p>
<p>顺带再说一下，<a href="http://hedengcheng.com/?p=771" target="_blank" rel="external">MySQL 加锁处理分析</a>这篇文章也是干活满满，这两篇加起来深入理解了，对MySQL的deadlock就会有一个很全面的认识了。</p>
<h2 id="一次坑爹的优化">一次坑爹的优化</h2>
<p>我们需要在一张表里面删除某种类型的数据，大概的表结构类似这样:</p>
<pre><code>CREATE TABLE t (
    id INT,
    tp ENUM (&quot;t1&quot;, &quot;t2&quot;),
    PRIMARY KEY(id)
) ENGINE=INNODB;
</code></pre><p>假设我们需要删除类型为t2的数据，语句可能是这样<code>delete from t where tp = &quot;t2&quot;</code>，这样没啥问题，但我们这张表有5亿数据，好吧，真的是5亿，所以以后别再跟我说MySQL表存储百万级别数据就要分表了，百万太小case了。</p>
<p>这事情我交给了一个小盆友去帮我搞定，他最开始写出了如下的语句<code>delete from t where tp = &quot;t2&quot; limit 1000</code>，使用limit来限制一次删除的个数，可以了，不过这有个很严重的问题，就是越往后，随着t2类型的减少，我们几乎都是全表遍历来删除，所以总的应该是O(n*n)的开销。</p>
<p>于是我让他考虑主键，每次操作的时候，记录当前最大的主键，这样下次就可以从这个主键之后开始删除了，首先 <code>select id from t where id &gt; last_max_select_id and tp = &quot;t2&quot; limit 1000</code>，然后<code>delete from t where id in (ids)</code>，虽然这次优化采用了两条语句，但是通过主键，我们只需要遍历一次表就可以了，总的来说，性能要快的。</p>
<p>但是，实际测试的时候，我们却发现，select这条语句耗时将近30s，太慢了。虽然我们使用了主键，但是MySQL仍然需要不停的读取数据判断条件，加之t2类型的数据在表里面比较少量，所以为了limit 1000这个条件，MySQL需要持续的进行IO读取操作，结果自然是太慢了。</p>
<p>想清楚了这个，其实就好优化了，我们只需要让条件判断在应用层做，MySQL只查询数据返回，语句就是 <code>select id, tp from t where id &gt; last_max_select_id limit 1000</code>，得到结果集之后，自行判断需要删除的id，然后delete。看似我们需要额外处理逻辑，并且网络开销也增大了，但MySQL只是简单的IO读取，非常快，总的来说，性能提升很显著。当然笔者后续还需要更深入的分析。</p>
<p>最后执行，很happy的是，非常快速的就删完了相关数据，而select的查询时间消耗几乎忽略不计。</p>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Go中优雅的关闭HTTP服务]]></title>
    <link href="http://siddontang.com/2015/01/25/stop-server-gracefully/"/>
    <id>http://siddontang.com/2015/01/25/stop-server-gracefully/</id>
    <published>2015-01-25T13:05:04.000Z</published>
    <updated>2015-01-25T13:07:01.000Z</updated>
    <content type="html"><![CDATA[<p>虽然写出7x24小时不间断运行的服务是一件很酷的事情，但是我们仍然在某些时候，譬如服务升级，配置更新等，得考虑如何优雅的结束这个服务。</p>
<p>当然，最暴力的做法直接就是<code>kill -9</code>，但这样直接导致的后果就是可能干掉了很多运行到一半的任务，最终导致数据不一致，这个苦果只有遇到过的人才能深深地体会，数据的修复真的挺蛋疼，有时候还得给用户赔钱啦。</p>
<p>所以，通常我们都是给服务发送一个信号，SIGTERM也行，SIGINTERRUPT也成，反正要让服务知道该结束了。而服务收到结束信号之后，首先会拒绝掉所有外部新的请求，然后等待当前所有正在执行的请求完成之后，在结束。当然很有可能当前在执行一个很耗时间的任务，导致服务长时间不能结束，这时候就得决定是否强制结束了。</p>
<p>具体到go的HTTP Server里面，如何优雅的结束一个HTTP Server呢？</p>
<p>首先，我们需要显示的创建一个listener，让其循环不断的accept新的连接供server处理，为啥不用默认的http.ListenAndServe，主要就在于我们可以在结束的时候通过关闭这个listener来主动的拒绝掉外部新的连接请求。代码如下:</p>
<pre><code>l, _ := net.Listen(&quot;tcp&quot;, address)
svr := http.Server{Handler: handler}
svr.Serve(l)
</code></pre><p>Serve这个函数是个死循环，我们可以在外部通过close对应的listener来结束。</p>
<p>当listener accept到新的请求之后，会开启一个新的goroutine来执行，那么在server结束的时候，我们怎么知道这个goroutine是否完成了呢？</p>
<p>在很早之前，大概go1.2的时候，笔者通过在handler入口处使用sync WaitGroup来实现，因为我们有统一的一个入口handler，所以很容易就可以通过如下方式知道请求是否完成，譬如：</p>
<pre><code>func (h *Handler) ServeHTTP(w ResponseWriter, r *Request) {
    h.svr.wg.Add(1)
    defer h.svr.wg.Done()

    ......
}
</code></pre><p>但这样其实只是用来判断请求是否结束了，我们知道在HTTP 1.1中，connection是能够keepalived的，也就是请求处理完成了，但是connection仍是可用的，我们没有一个好的办法close掉这个connection。不过话说回来，我们只要保证当前请求能正常结束，connection能不能正常close真心无所谓，毕竟服务都结束了，connection自动就close了。但谁叫笔者是典型的处女座呢。</p>
<p>在go1.3之后，提供了一个ConnState的hook，我们能通过这个来获取到对应的connection，这样在服务结束的时候我们就能够close掉这个connection了。该hook会在如下几种ConnState状态的时候调用。</p>
<ul>
<li>StateNew：新的连接，并且马上准备发送请求了</li>
<li>StateActive：表明一个connection已经接收到一个或者多个字节的请求数据，在server调用实际的handler之前调用hook。</li>
<li>StateIdle：表明一个connection已经处理完成一次请求，但因为是keepalived的，所以不会close，继续等待下一次请求。</li>
<li>StateHijacked：表明外部调用了hijack，最终状态。</li>
<li>StateClosed：表明connection已经结束掉了，最终状态。</li>
</ul>
<p>通常，我们不会进入hijacked的状态（如果是websocket就得考虑了），所以一个可能的hook函数如下，参考<a href="http://rcrowley.org/talks/gophercon-2014.html" target="_blank" rel="external">http://rcrowley.org/talks/gophercon-2014.html</a></p>
<pre><code>s.ConnState = func(conn net.Conn, state http.ConnState) {
    switch state {
    case http.StateNew:
        // 新的连接，计数加1
        s.wg.Add(1)
    case http.StateActive:
        // 有新的请求，从idle conn pool中移除
        s.mu.Lock()
        delete(s.conns, conn.LocalAddr().String())
        s.mu.Unlock()
    case http.StateIdle:
        select {
        case &lt;-s.quit:
            // 如果要关闭了，直接Close，否则加入idle conn pool中。
            conn.Close()
        default:
            s.mu.Lock()
            s.conns[conn.LocalAddr().String()] = conn
            s.mu.Unlock()
        }
    case http.StateHijacked, http.StateClosed:
        // conn已经closed了，计数减一
        s.wg.Done()
    }
</code></pre><p>当结束的时候，会走如下流程：</p>
<pre><code>func (s *Server) Close() error {
    // close quit channel, 广播我要结束啦
    close(s.quit)

    // 关闭keepalived，请求返回的时候会带上Close header。客户端就知道要close掉connection了。
    s.SetKeepAlivesEnabled(false)
    s.mu.Lock()

    // close listenser
    if err := s.l.Close(); err != nil {
        return err 
    }

    //将当前idle的connections设置read timeout，便于后续关闭。
    t := time.Now().Add(100 * time.Millisecond)
    for _, c := range s.conns {
        c.SetReadDeadline(t)
    }
    s.conns = make(map[string]net.Conn)
    s.mu.Unlock()

    // 等待所有连接结束
    s.wg.Wait()
    return nil
}
</code></pre><p>好了，通过以上方法，我们终于能从容的关闭server了。但这里仅仅是针对跟客户端的连接，实际还有MySQL连接，Redis连接，打开的文件句柄，等等，总之，要实现优雅的服务关闭，真心不是一件很简单的事情。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Elasticsearch学习笔记]]></title>
    <link href="http://siddontang.com/2015/01/18/elasticsearch-note/"/>
    <id>http://siddontang.com/2015/01/18/elasticsearch-note/</id>
    <published>2015-01-18T06:58:47.000Z</published>
    <updated>2015-01-18T10:05:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Why_Elasticsearch？">Why Elasticsearch？</h2>
<p>由于需要提升项目的搜索质量，最近研究了一下Elasticsearch，一款非常优秀的分布式搜索程序。最开始的一些笔记放到<a href="https://github.com/siddontang/elasticsearch-note" target="_blank" rel="external">github</a>，这里只是归纳总结一下。</p>
<p>首先，为什么要使用Elasticsearch？最开始的时候，我们的项目仅仅使用MySQL进行简单的搜索，然后一个不能索引的like语句，直接拉低MySQL的性能。后来，我们曾考虑过sphinx，并且sphinx也在之前的项目中成功实施过，但想想现在的数据量级，多台MySQL，以及搜索服务本身HA，还有后续扩容的问题，我们觉得sphinx并不是一个最优的选择。于是自然将目光放到了Elasticsearch上面。</p>
<p>根据官网自己的介绍，Elasticsearch是一个分布式搜索服务，提供Restful API，底层基于Lucene，采用多shard的方式保证数据安全，并且提供自动resharding的功能，加之github等大型的站点也采用Elasticsearch作为其搜索服务，我们决定在项目中使用Elasticsearch。</p>
<p>对于Elasticsearch，如果要在项目中使用，需要解决如下问题：</p>
<ol>
<li>索引，对于需要搜索的数据，如何建立合适的索引，还需要根据特定的语言使用不同的analyzer等。</li>
<li>搜索，Elasticsearch提供了非常强大的搜索功能，如何写出高效的搜索语句？</li>
<li>数据源，我们所有的数据是存放到MySQL的，MySQL是唯一数据源，如何将MySQL的数据导入到Elasticsearch？</li>
</ol>
<p>对于1和2，因为我们的数据都是从MySQL生成，index的field是固定的，主要做的工作就是根据业务场景设计好对应的mapping以及search语句就可以了，当然实际不可能这么简单，需要我们不断的调优。</p>
<p>而对于3，则是需要一个工具将MySQL的数据导入Elasticsearch，因为我们对搜索实时性要求很高，所以需要将MySQL的增量数据实时导入，笔者唯一能想到的就是通过row based binlog来完成。而近段时间的工作，也就是实现一个MySQL增量同步到Elasticsearch的服务。</p>
<h2 id="Lucene">Lucene</h2>
<p>Elasticsearch底层是基于Lucene的，Lucene是一款优秀的搜索lib，当然，笔者以前仍然没有接触使用过。:-)</p>
<p>Lucene关键概念：</p>
<ul>
<li>Document：用来索引和搜索的主要数据源，包含一个或者多个Field，而这些Field则包含我们跟Lucene交互的数据。</li>
<li>Field：Document的一个组成部分，有两个部分组成，name和value。</li>
<li>Term：不可分割的单词，搜索最小单元。</li>
<li>Token：一个Term呈现方式，包含这个Term的内容，在文档中的起始位置，以及类型。</li>
</ul>
<p>Lucene使用<a href="http://en.wikipedia.org/wiki/Inverted_index" target="_blank" rel="external">Inverted index</a>来存储term在document中位置的映射关系。<br>譬如如下文档：</p>
<ul>
<li>Elasticsearch Server 1.0 （document 1）</li>
<li>Mastring Elasticsearch （document 2）</li>
<li>Apache Solr 4 Cookbook （document 3）</li>
</ul>
<p>使用inverted index存储，一个简单地映射关系：</p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Count</th>
<th>Docuemnt</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>1</td>
<td><1></1></td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Apache</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Cookbook</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>2</td>
<td><1>.<2></2></1></td>
</tr>
<tr>
<td>Mastering</td>
<td>1</td>
<td><2></2></td>
</tr>
<tr>
<td>Server</td>
<td>1</td>
<td><1></1></td>
</tr>
<tr>
<td>Solr</td>
<td>1</td>
<td><3></3></td>
</tr>
</tbody>
</table>
<p>对于上面例子，我们首先通过分词算法将一个文档切分成一个一个的token，再得到该token与document的映射关系，并记录token出现的总次数。这样就得到了一个简单的inverted index。</p>
<h2 id="Elasticsearch关键概念">Elasticsearch关键概念</h2>
<p>要使用Elasticsearch，笔者认为，只需要理解几个基本概念就可以了。</p>
<p>在数据层面，主要有：</p>
<ul>
<li>Index：Elasticsearch用来存储数据的逻辑区域，它类似于关系型数据库中的table概念。一个index可以在一个或者多个shard上面，同时一个shard也可能会有多个replicas。</li>
<li>Document：Elasticsearch里面存储的实体数据，类似于关系数据中一个table里面的一行数据。<br>document由多个field组成，不同的document里面同名的field一定具有相同的类型。document里面field可以重复出现，也就是一个field会有多个值，即multivalued。</li>
<li>Document type：为了查询需要，一个index可能会有多种document，也就是document type，但需要注意，不同document里面同名的field一定要是相同类型的。</li>
<li>Mapping：存储field的相关映射信息，不同document type会有不同的mapping。</li>
</ul>
<p>对于熟悉MySQL的童鞋，我们只需要大概认为Index就是一个table，document就是一行数据，field就是table的column，mapping就是table的定义就可以了。</p>
<p>Document type这个概念其实最开始也把笔者给弄糊涂了，其实它就是为了更好的查询，举个简单的例子，一个index，可能一部分数据我们想使用一种查询方式，而另一部分数据我们想使用另一种查询方式，于是就有了两种type了。不过这种情况应该在我们的项目中不会出现，所以通常一个index下面仅会有一个type。</p>
<p>在服务层面，主要有：</p>
<ul>
<li>Node: 一个server实例。</li>
<li>Cluster：多个node组成cluster。</li>
<li>Shard：数据分片，一个index可能会存在于多个shards，不同shards可能在不同nodes。</li>
<li>Replica：shard的备份，有一个primary shard，其余的叫做replica shards。</li>
</ul>
<p>Elasticsearch之所以能动态resharding，主要在于它最开始就预先分配了多个shards（貌似是1024），然后以shard为单位进行数据迁移。这个做法其实在分布式领域非常的普遍，<a href="github.com/wandoulabs/codis">codis</a>就是使用了1024个slot来进行数据迁移。</p>
<p>因为任意一个index都可配置多个replica，通过冗余备份的方式保证了数据的安全性，同时replica也能分担读压力，类似于MySQL中的slave。</p>
<h2 id="Restful_API">Restful API</h2>
<p>Elasticsearch提供了Restful API，使用json格式，这使得它非常利于与外部交互，虽然Elasticsearch的客户端很多，但笔者仍然很容易的就写出了一个简易客户端用于项目中，再次印证了Elasticsearch的使用真心很容易。</p>
<p>Restful的接口很简单，一个url表示一个特定的资源，譬如<code>/blog/article/1</code>，就表示一个index为blog，type为aritcle，id为1的document。</p>
<p>而我们使用http标准method来操作这些资源，POST新增，PUT更新，GET获取，DELETE删除，HEAD判断是否存在。</p>
<p>这里，友情推荐<a href="https://github.com/jakubroztocil/httpie" target="_blank" rel="external">httpie</a>，一个非常强大的http工具，个人感觉比curl还用，几乎是命令行调试Elasticsearch的绝配。</p>
<p>一些使用httpie的例子:</p>
<pre><code># create
http POST :9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&#39;[&quot;elasticsearch&quot;]&#39;

# get
http GET :9200/blog/article/1

# update
http PUT :9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&#39;[&quot;elasticsearch&quot;, &quot;hello&quot;]&#39;

# delete
http DELETE :9200/blog/article/1

# exists
http HEAD :9200/blog/article/1
</code></pre><h2 id="索引和搜索">索引和搜索</h2>
<p>虽然Elasticsearch能自动判断field类型并建立合适的索引，但笔者仍然推荐自己设置相关索引规则，这样才能更好为后续的搜索服务。</p>
<p>我们通过定制mapping的方式来设置不同field的索引规则。</p>
<p>而对于搜索，Elasticsearch提供了太多的搜索选项，就不一一概述了。</p>
<p>索引和搜索是Elasticsearch非常重要的两个方面，直接关系到产品的搜索体验，但笔者现阶段也仅仅是大概了解了一点，后续在详细介绍。</p>
<h2 id="同步MySQL数据">同步MySQL数据</h2>
<p>Elasticsearch是很强大，但要建立在有足量数据情况下面。我们的数据都在MySQL上面，所以如何将MySQL的数据导入Elasticsearch就是笔者最近研究的东西了。</p>
<p>虽然现在有一些实现，譬如<a href="https://github.com/jprante/elasticsearch-river-jdbc" target="_blank" rel="external">elasticsearch-river-jdbc</a>，或者<a href="https://github.com/scharron/elasticsearch-river-mysql" target="_blank" rel="external">elasticsearch-river-mysql</a>，但笔者并不打算使用。</p>
<p>elasticsearch-river-jdbc的功能是很强大，但并没有很好的支持增量数据更新的问题，它需要对应的表只增不减，而这个几乎在项目中是不可能办到的。</p>
<p>elasticsearch-river-mysql倒是做的很不错，采用了<a href="https://github.com/noplay/python-mysql-replication" target="_blank" rel="external">python-mysql-replication</a>来通过binlog获取变更的数据，进行增量更新，但它貌似处理MySQL dump数据导入的问题，不过这个笔者真的好好确认一下？话说，python-mysql-replication笔者还提交过<a href="https://github.com/noplay/python-mysql-replication/pull/103" target="_blank" rel="external">pull</a>解决了minimal row image的问题，所以对elasticsearch-river-mysql这个项目很有好感。只是笔者决定自己写一个出来。</p>
<p>为什么笔者决定自己写一个，不是因为笔者喜欢造轮子，主要原因在于对于这种MySQL syncer服务（增量获取MySQL数据更新到相关系统），我们不光可以用到Elasticsearch上面，而且还能用到其他服务，譬如cache上面。所以笔者其实想实现的是一个通用MySQL syncer组件，只是现在主要关注Elasticsearch罢了。</p>
<p>项目代码在这里<a href="https://github.com/siddontang/go-mysql-elasticsearch" target="_blank" rel="external">go-mysql-elasticsearch</a>，仍然处于开发状态，预计下周能基本完成。</p>
<p>go-mysql-elasticsearch的原理很简单，首先使用mysqldump获取当前MySQL的数据，然后在通过此时binlog的name和position获取增量数据。</p>
<p>一些限制：</p>
<ul>
<li>binlog一定要变成row-based format格式，其实我们并不需要担心这种格式的binlog占用太多的硬盘空间，MySQL 5.6之后GTID模式都推荐使用row-based format了，而且通常我们都会把控SQL语句质量，不允许一次性更改过多行数据的。</li>
<li>需要同步的table最好是innodb引擎，这样mysqldump的时候才不会阻碍写操作。</li>
<li>需要同步的table一定要有主键，好吧，如果一个table没有主键，笔者真心会怀疑设计这个table的同学编程水平了。多列主键也是不推荐的，笔者现阶段不打算支持。</li>
<li>一定别动态更改需要同步的table结构，Elasticsearch只能支持动态增加field，并不支持动态删除和更改field。通常来说，如果涉及到alter table，很多时候已经证明前期设计的不合理以及对于未来扩展的预估不足了。</li>
</ul>
<p>更详细的说明，等到笔者完成了go-mysql-elasticsearch的开发，再进行补充。</p>
<h2 id="总结">总结</h2>
<p>最近一周，笔者花了不少时间在Elasticsearch上面，现在算是基本入门了。其实笔者觉得，对于一门不懂的技术，找一份靠谱的资料（官方文档或者入门书籍），蛋疼的对着资料敲一遍代码，不懂的再问google，最后在将其用到实际项目，这门技术就算是初步掌握了，当然精通还得在下点功夫。</p>
<p>现在笔者只是觉得Elasticsearch很美好，上线之后铁定会有坑的，那时候只能慢慢填了。话说，笔者是不是要学习下java了，省的到时候看不懂代码就惨了。:-)</p>
]]></content>
    
    
      <category term="elasticsearch" scheme="http://siddontang.com/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker实践]]></title>
    <link href="http://siddontang.com/2015/01/09/docker-practise/"/>
    <id>http://siddontang.com/2015/01/09/docker-practise/</id>
    <published>2015-01-09T14:48:10.000Z</published>
    <updated>2015-01-09T14:49:18.000Z</updated>
    <content type="html"><![CDATA[<h2 id="起因">起因</h2>
<p>Docker算是现在非常火的一个项目，但笔者对其一直不怎么感冒，毕竟没啥使用场景。只是最近，笔者需要在自己的mac电脑上面安装项目的开发环境，发现需要安装MySQL，LedisDB，xcodis，Redis，Zookeeper等一堆东西，而同样的流程仍然要在Windows的机器上面再来一遍，陡然觉得必须得有一个更好的方式来管理整个项目的开发环境了。自然，笔者将目光放到了Docker上面。</p>
<p>根据官方自己的介绍，Docker其实是一个为开发和运维人员提供构建，分发以及运行分布式应用的开源平台（野心真的不小，难怪CoreOS要新弄一个Rocket来跟他竞争的）。</p>
<p>Docker主要包括Docker Engine，一个轻量级的运行和包管理工具，Docker Hub，一个用来共享和自动化工作流的云服务。实际在使用Docker的工程中，我们通常都是会在Docker Hub上面找到一个base image，编写Dockerfile，构建我们自己的image。所以很多时候，学习使用Docker，我们仅需要了解Docker Engine的东西就可以了。</p>
<p>至于为啥选用Docker，原因还是很明确的，轻量简单，相比于使用VM，Docker实在是太轻量了，笔者在自己的mac air上面同时可以运行多个Docker container进行开发工作，而这个对VM来说是不敢想象的。</p>
<p>后面，笔者将结合自己的经验，来说说如何构建一个MySQL Docker，以及当中踩过的坑。</p>
<h2 id="MySQL_Docker">MySQL Docker</h2>
<p>笔者一直从事MySQL相关工具的开发，对于MySQL的依赖很深，但每次安装MySQL其实是让笔者非常头疼的一件事情，不同平台安装方式不一样，加上一堆设置，很容易就把人搞晕了。所以自然，我的Docker第一次尝试就放到了MySQL上面。</p>
<p>对于mac用户，首先需要安装boot2docker这个工具才能使用Docker，这个工具是挺方便的，但也有点坑，后续会说明。</p>
<p>笔者前面说了，通常使用Docker的方式是在Hub上面找一个base image，虽然Hub上面有很多MySQL的image，但笔者因为开发<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>，需要在MySQL启动的时候传入特定的参数，所以决定自行编写Dockerfile来构建。</p>
<p>首先，笔者使用的base image为ubuntu:14.04，Dockerfile文件很简单，如下:</p>
<pre><code>FROM ubuntu:14.04

# 安装MySQL 5.6，因为笔者需要使用GTID
RUN apt-get update \
    &amp;&amp; apt-get install -y mysql-server-5.6

# 清空apt-get的cache以及MySQL datadir
RUN apt-get clean
RUN rm -rf /var/lib/apt/lists/* /var/lib/mysql

# 使用精简配置，主要是为了省内存，笔者机器至少要跑6个MySQL
ADD my.cnf /etc/mysql/my.cnf

# 这里主要是给mysql_install_db脚本使用
ADD my-default.cnf /usr/share/mysql/my-default.cnf

# 增加启动脚本
ADD start.sh /start.sh
RUN chmod +x /start.sh

# 将MySQL datadir设置成可外部挂载
VOLUME [&quot;/var/lib/mysql&quot;]

# 导出3306端口
EXPOSE 3306

# 启动执行start.sh脚本
CMD [&quot;/start.sh&quot;]
</code></pre><p>我们需要注意，对于MySQL这种需要存储数据的服务来说，一定需要给datadir设置VOLUMN，这样你才能存储数据。笔者当初就忘记设置VOLUMN，结果启动6个MySQL Docker container之后，突然发现这几个MySQL使用的是同一份数据。</p>
<p>如果有VOLUMN, 我们可以在<code>docker run</code>的时候指定对应的外部挂载点，如果没有指定，Docker会在自己的vm目录下面生成一个唯一的挂载点，我们可以通过<code>docker inspect</code>命令详细了解每个container的情况。</p>
<p>对于<code>start.sh</code>，比较简单：</p>
<ul>
<li>判断MySQL datadir下面有没有数据，如果没有，调用<code>mysql_install_db</code>初始化。</li>
<li>允许任意ip都能使用root账号访问，<code>mysql -uroot -e &quot;GRANT ALL ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39; WITH GRANT OPTION;&quot;</code>，否则我们在外部无法连接MySQL。</li>
<li>启动mysql</li>
</ul>
<p>构建好了MySQL Docker image，我们就能使用<code>docker run</code>来运行了，很简单</p>
<pre><code>docker run -d -p 3306:3306 --name=mysql siddontang/mysql:latest
</code></pre><p>这里，我们基于siddontang/mysql这个image创建了一个名叫mysql的container并运行，它会调用<code>start.sh</code>脚本来启动MySQL。</p>
<p>而我们通过<code>docker stop mysql</code>就可以停止mysql container了。</p>
<p>如果笔者需要运行多个MySQL，仅仅需要多新建几个container并运行就可以了，当然得指定对应的端口。可以看到，这种方式非常的简单，虽然使用<code>mysqld_multi</code>也能达到同样的效果，但是如果我需要在新增一个MySQL实例，<code>mysqld_mutli</code>还需要去更改配置文件，以及在对应的MySQL里面设置允许<code>mysqld_multi stop</code>的权限，其实算是比较麻烦的。而这些，在Docker里面，一个<code>docker run</code>就搞定了。</p>
<p>完整的构建代码在这里，<a href="https://github.com/siddontang/mysql-docker" target="_blank" rel="external">mysql-docker</a>，你也可以pull笔者提交到Hub的image <code>siddontang/mysql</code>来直接使用<code>docker pull siddontang/mysql:latest</code>。</p>
<h2 id="Boot2Docker_Pitfall">Boot2Docker Pitfall</h2>
<p>从前面可以看到，Docker的使用是非常方便的，但笔者在使用的时候仍然碰到了一点坑，这里记录一下。</p>
<h3 id="IP">IP</h3>
<p>最开始碰到的就是ip问题，笔者在run的时候做了端口映射，但是外部使用MySQL客户端死活连接不上，而这个只在笔者mac上面出现，linux上面正常，后来发现是boot2docker的问题，我们需要使用<code>boot2docker ip</code>返回的ip来访问container，在笔者的机器上面，这个ip为192.168.59.103。</p>
<h3 id="Volumn">Volumn</h3>
<p>仍然是boot2docker的问题，笔者在<code>docker run</code>的时候，使用<code>-v</code>来将外部的目录绑定到datadir这个VOLUMN上面，这个在linux上面是成功的，可是在mac上面，笔者发现<code>mysql_install_db</code>死活没有权限写入磁盘。后来才知道，boot2docker只允许对自己VM下面的路径进行绑定。鉴于在mac下面仅仅是调试，数据不许持久化保存，这个问题也懒得管了。反正只要不删除掉container，数据还是会在的。</p>
<h2 id="Flatten_Image">Flatten Image</h2>
<p>在使用Dockerfile构建自己的image的时候，对于Dockerfile里面的每一步，Docker都会生成一个layer来对应，也就是每一步都是一次提交，到最后你会发现，生成的image非常的庞大，而当你push这个image到Hub上面的时候，你的所有layer都会提交上去，加之我们国家的网速水平，会让人崩溃的。</p>
<p>所以我们需要精简生成的image大小，也就是flatten，这个Docker官方还没有支持，但至少我们还是有办法的：</p>
<ul>
<li><code>docker export</code> and <code>docker import</code>，通过对特定container的export和import操作，我们可以生成一个无历史的新container，详见<a href="http://tuhrig.de/flatten-a-docker-container-or-image/" target="_blank" rel="external">这里</a>。</li>
<li><a href="https://github.com/jwilder/docker-squash" target="_blank" rel="external">docker-squash</a>，很方便的一个工具，笔者就使用这个进行image的flatten处理。</li>
</ul>
<h2 id="后记">后记</h2>
<p>总的来说，Docker还是很容易上手的，只要我们熟悉了它的命令，Dockerfile的编写以及相应的运行机制，就能很方便的用Docker来进行团队的持续集成开发。而在生产环境中使用Docker，笔者还没有相关的经验，没准后续私有云会采用Docker进行部署。</p>
<p>后续，对于多个Container的交互，以及服务发现，扩容等，笔者也还需要好好研究，CoreOS没准是一个方向，或者研究下<a href="https://github.com/coreos/rocket" target="_blank" rel="external">rocket</a> :-)</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="docker" scheme="http://siddontang.com/tags/docker/"/>
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[MySQL高可用浅析]]></title>
    <link href="http://siddontang.com/2015/01/03/mysql-ha/"/>
    <id>http://siddontang.com/2015/01/03/mysql-ha/</id>
    <published>2015-01-03T13:20:27.000Z</published>
    <updated>2015-01-03T13:25:21.000Z</updated>
    <content type="html"><![CDATA[<p>对于多数应用来说，MySQL都是作为最关键的数据存储中心的，所以，如何让MySQL提供HA服务，是我们不得不面对的一个问题。当master当机的时候，我们如何保证数据尽可能的不丢失，如何保证快速的获知master当机并进行相应的故障转移处理，都是需要我们好好思考的。这里，笔者将结合这段时间做的MySQL proxy以及toolsets相关工作，说说我们现阶段以及后续会在项目中采用的MySQL HA方案。</p>
<h2 id="Replication">Replication</h2>
<p>要保证MySQL数据不丢失，replication是一个很好的解决方案，而MySQL也提供了一套强大的replication机制。只是我们需要知道，为了性能考量，replication是采用的asynchronous模式，也就是写入的数据并不会同步更新到slave上面，如果这时候master当机，我们仍然可能会面临数据丢失的风险。</p>
<p>为了解决这个问题，我们可以使用semi-synchronous replication，semi-synchronous replication的原理很简单，当master处理完一个事务，它会等待至少一个支持semi-synchronous的slave确认收到了该事件并将其写入relay-log之后，才会返回。这样即使master当机，最少也有一个slave获取到了完整的数据。</p>
<p>但是，semi-synchronous并不是100%的保证数据不会丢失，如果master在完成事务并将其发送给slave的时候崩溃，仍然可能造成数据丢失。只是相比于传统的异步复制，semi-synchronous replication能极大地提升数据安全。更为重要的是，它并不慢，MHA的作者都说他们在facebook的生产环境中使用了semi-synchronous（<a href="http://yoshinorimatsunobu.blogspot.ca/2014/04/semi-synchronous-replication-at-facebook.html" target="_blank" rel="external">这里</a>），所以我觉得真心没必要担心它的性能问题，除非你的业务量级已经完全超越了facebook或者google。</p>
<p>如果真的想完全保证数据不会丢失，现阶段一个比较好的办法就是使用<a href="http://galeracluster.com/" target="_blank" rel="external">gelera</a>，一个MySQL集群解决方案，它通过同时写三份的策略来保证数据不会丢失。笔者没有任何使用gelera的经验，只是知道业界已经有公司将其用于生产环境中，性能应该也不是问题。但gelera对MySQL代码侵入性较强，可能对某些有代码洁癖的同学来说不合适了:-)</p>
<p>我们还可以使用<a href="http://drbd.linbit.com/" target="_blank" rel="external">drbd</a>来实现MySQL数据复制，MySQL官方文档有一篇文档有详细<a href="http://dev.mysql.com/doc/refman/5.6/en/ha-drbd.html" target="_blank" rel="external">介绍</a>，但笔者并未采用这套方案，MHA的作者写了一些采用drdb的问题，<a href="https://code.google.com/p/mysql-master-ha/wiki/Other_HA_Solutions#Pacemaker_+_DRBD" target="_blank" rel="external">在这里</a>，仅供参考。</p>
<p>在后续的项目中，笔者会优先使用semi-synchronous replication的解决方案，如果数据真的非常重要，则会考虑使用gelera。</p>
<h2 id="Monitor">Monitor</h2>
<p>前面我们说了使用replication机制来保证master当机之后尽可能的数据不丢失，但是我们不能等到master当了几分钟才知道出现问题了。所以一套好的监控工具是必不可少的。</p>
<p>当master当掉之后，monitor能快速的检测到并做后续处理，譬如邮件通知管理员，或者通知守护程序快速进行failover。</p>
<p>通常，对于一个服务的监控，我们采用keepalived或者heartbeat的方式，这样当master当机之后，我们能很方便的切换到备机上面。但他们仍然不能很即时的检测到服务不可用。笔者的公司现阶段使用的是keepalived的方式，但后续笔者更倾向于使用zookeeper来解决整个MySQL集群的monitor以及failover。</p>
<p>对于任何一个MySQL实例，我们都有一个对应的agent程序，agent跟该MySQL实例放到同一台机器上面，并且定时的对MySQL实例发送ping命令检测其可用性，同时该agent通过ephemeral的方式挂载到zookeeper上面。这样，我们可以就能知道MySQL是否当机，主要有以下几种情况：</p>
<ol>
<li>机器当机，这样MySQL以及agent都会当掉，agent与zookeeper连接自然断开</li>
<li>MySQL当掉，agent发现ping不通，主动断开与zookeeper的连接</li>
<li>Agent当掉，但MySQL未当</li>
</ol>
<p>上面三种情况，我们都可以认为MySQL机器出现了问题，并且zookeeper能够立即感知。agent与zookeeper断开了连接，zookeeper触发相应的children changed事件，监控到该事件的管控服务就可以做相应的处理。譬如如果是上面前两种情况，管控服务就能自动进行failover，但如果是第三种，则可能不做处理，等待机器上面crontab或者supersivord等相关服务自动重启agent。</p>
<p>使用zookeeper的好处在于它能很方便的对整个集群进行监控，并能即时的获取整个集群的变化信息并触发相应的事件通知感兴趣的服务，同时协调多个服务进行相关处理。而这些是keepalived或者heartbeat做不到或者做起来太麻烦的。</p>
<p>使用zookeeper的问题在于部署起来较为复杂，同时如果进行了failover，如何让应用程序获取到最新的数据库地址也是一个比较麻烦的问题。</p>
<p>对于部署问题，我们要保证一个MySQL搭配一个agent，幸好这年头有了docker，所以真心很简单。而对于第二个数据库地址更改的问题，其实并不是使用了zookeeper才会有的，我们可以通知应用动态更新配置信息，或者使用proxy来解决。</p>
<p>虽然zookeeper的好处很多，但如果你的业务不复杂，譬如只有一个master，一个slave，zookeeper可能并不是最好的选择，没准keepalived就够了。</p>
<h2 id="Failover">Failover</h2>
<p>通过monitor，我们可以很方便的进行MySQL监控，同时在MySQL当机之后通知相应的服务做failover处理，假设现在有这样的一个MySQL集群，a为master，b，c为其slave，当a当掉之后，我们需要做failover，那么我们选择b，c中的哪一个作为新的master呢？</p>
<p>原则很简单，哪一个slave拥有最近最多的原master数据，就选哪一个作为新的master。我们可以通过<code>show slave status</code>这个命令来获知哪一个slave拥有最新的数据。我们只需要比较两个关键字段<code>Master_Log_File</code>以及<code>Read_Master_Log_Pos</code>，这两个值代表了slave读取到master哪一个binlog文件的哪一个位置，binlog的索引值越大，同时pos越大，则那一个slave就是能被提升为master。这里我们不讨论多个slave可能会被提升为master的情况。</p>
<p>在前面的例子中，假设b被提升为master了，我们需要将c重新指向新的master b来开始复制。我们通过<code>CHANGE MASTER TO</code>来重新设置c的master，但是我们怎么知道要从b的binlog的哪一个文件，哪一个position开始复制呢？</p>
<h3 id="GTID">GTID</h3>
<p>为了解决这一个问题，MySQL 5.6之后引入了GTID的概念，即uuid:gid，uuid为MySQL server的uuid，是全局唯一的，而gid则是一个递增的事务id，通过这两个东西，我们就能唯一标示一个记录到binlog中的事务。使用GTID，我们就能非常方便的进行failover的处理。</p>
<p>仍然是前面的例子，假设b此时读取到的a最后一个GTID为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:23</code>，而c的为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:15</code>，当c指向新的master b的时候，我们通过GTID就可以知道，只要在b中的binlog中找到GTID为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:15</code>这个event，那么c就可以从它的下一个event的位置开始复制了。虽然查找binlog的方式仍然是顺序查找，稍显低效暴力，但比起我们自己去猜测哪一个filename和position，要方便太多了。</p>
<p>google很早也有了一个<a href="https://code.google.com/p/google-mysql-tools/wiki/GlobalTransactionIds" target="_blank" rel="external">Global Transaction ID</a>的补丁，不过只是使用的一个递增的整形，<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">LedisDB</a>就借鉴了它的思路来实现failover，只不过google貌似现在也开始逐步迁移到MariaDB上面去了。</p>
<p>MariaDB的GTID实现跟MySQL 5.6是不一样的，这点其实比较麻烦，对于我的MySQL工具集<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>来说，意味着要写两套不同的代码来处理GTID的情况了。后续是否支持MariaDB再看情况吧。</p>
<h3 id="Pseudo_GTID">Pseudo GTID</h3>
<p>GTID虽然是一个好东西，但是仅限于MySQL 5.6+，当前仍然有大部分的业务使用的是5.6之前的版本，笔者的公司就是5.5的，而这些数据库至少长时间也不会升级到5.6的。所以我们仍然需要一套好的机制来选择master binlog的filename以及position。</p>
<p>最初，笔者打算研究<a href="https://code.google.com/p/mysql-master-ha/" target="_blank" rel="external">MHA</a>的实现，它采用的是首先复制relay log来补足缺失的event的方式，但笔者不怎么信任relay log，同时加之MHA采用的是perl，一个让我完全看不懂的语言，所以放弃了继续研究。</p>
<p>幸运的是，笔者遇到了<a href="https://github.com/outbrain/orchestrator" target="_blank" rel="external">orchestrator</a>这个项目，这真的是一个非常神奇的项目，它采用了一种<a href="http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid" target="_blank" rel="external">Pseudo GTID</a>的方式，核心代码就是这个</p>
<pre><code>create database if not exists meta;

drop event if exists meta.create_pseudo_gtid_view_event;

delimiter ;;
create event if not exists
  meta.create_pseudo_gtid_view_event
  on schedule every 10 second starts current_timestamp
  on completion preserve
  enable
  do
    begin
      set @pseudo_gtid := uuid();
      set @_create_statement := concat(&#39;create or replace view meta.pseudo_gtid_view as select \&#39;&#39;, @pseudo_gtid, &#39;\&#39; as pseudo_gtid_unique_val from dual&#39;);
      PREPARE st FROM @_create_statement;
      EXECUTE st;
      DEALLOCATE PREPARE st;
    end
;;

delimiter ;

set global event_scheduler := 1;
</code></pre><p>它在MySQL上面创建了一个事件，每隔1s，就将一个uuid写入到一个view里面，而这个是会记录到binlog中的，虽然我们仍然不能像GTID那样直接定位到一个event，但也能定位到一个1s的区间了，这样我们就能在很小的一个区间里面对比两个MySQL的binlog了。</p>
<p>继续上面的例子，假设c最后一次出现uuid的位置为s1，我们在b里面找到该uuid，位置为s2，然后依次对比后续的event，如果不一致，则可能出现了问题，停止复制。当遍历到c最后一个binlog event之后，我们就能得到此时b下一个event对应的filename以及position了，然后让c指向这个位置开始复制。</p>
<p>使用Pseudo GTID需要slave打开<code>log-slave-update</code>的选项，考虑到GTID也必须打开该选项，所以个人感觉完全可以接受。</p>
<p>后续，笔者自己实现的failover工具，将会采用这种Pseudo GTID的方式实现。</p>
<p>在《MySQL High Availability》这本书中，作者使用了另一种GTID的做法，每次commit的时候，需要在一个表里面记录gtid，然后就通过这个gtid来找到对应的位置信息，只是这种方式需要业务MySQL客户端的支持，笔者不很喜欢，就不采用了。</p>
<h2 id="后记">后记</h2>
<p>MySQL HA一直是一个水比较深的领域，笔者仅仅列出了一些最近研究的东西，有些相关工具会尽量在<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>中实现。</p>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的2014]]></title>
    <link href="http://siddontang.com/2014/12/27/my-2014/"/>
    <id>http://siddontang.com/2014/12/27/my-2014/</id>
    <published>2014-12-27T05:21:03.000Z</published>
    <updated>2014-12-27T05:21:38.000Z</updated>
    <content type="html"><![CDATA[<p>每到年末，都要进行一次总结了，看看今年都做了哪些事情，有啥提高，明年目标是什么样的，大概计划怎样。</p>
<h2 id="家人">家人</h2>
<p>今年对我来说最大的两件事情就是我奶奶的离世以及我女儿的出生。我奶奶是在正月初一走的，可惜我没在旁边，因为要照顾快分娩的老婆。得到消息之后，立刻定了第二天的机票赶回老家送了奶奶最后一程。我爷爷是在五十年前的大年三十去世的，整整五十年零一天，真的像是冥冥中自有天注定似的。家里面突然少了一个人，陡然觉得心里空荡荡了许多。</p>
<p>在奶奶去世之后的二十五天，我的女儿出生了，家里面又多了一个人，当了爸爸，责任重了，压力大了。要考虑多多赚钱给她更好的生活了。</p>
<p>养育孩子真的是一件挺辛苦的事情，这里真的要感谢一下我的老婆，真的辛苦了。</p>
<p>等我女儿大了懂事了，我也会跟她说说她太奶奶的事情，虽然离她很遥远，但这是我宝贵的回忆。</p>
<h2 id="技术">技术</h2>
<p>今年我仍然继续着我的开源之路，比较欣慰的是弄的<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">LedisDB</a>终于算是小有成功，被用在生产环境中，而且还有一些国外用户使用。不过比较让我郁闷的是这个东西在国内收到很多嘲讽，谩骂。后来跟一些中国其他开源作者交流了，发现几乎也都是这样。想想我们的心态也还真是好。</p>
<p>除了NoSQL，我还尝试在MySQL领域耕耘，于是就有了一个MySQL proxy，<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">mixer</a>，这个项目其实我很看好前景的，并且也有了很多关注，只是因为一些其他方面的原因我没有继续开发了。不过后来一些用户告诉我他们正在使用mixer里面的一些代码进行自己的proxy开发，这让我感到很欣慰。弄开源，最想的其实就是有人使用，得到认可。</p>
<p>当然今年还做了很多一些小的开源组件，其实也都是一直围绕着NoSQL，MySQL上面来做的。</p>
<p>今年在技术上面最大的感触就是要走出去，跟人多交流，站在别人的肩膀上面。自己造轮子固然好，但是有时候基于别人的好的东西再搭积木，没准更好。一个最好的例子就是<a href="https://github.com/siddontang/xcodis" target="_blank" rel="external">xcodis</a>，我一直想让LedisDB支持集群，然后也想到使用proxy的方式，只是一直没时间去弄，这时候豌豆荚开源了codis，而这个就是我需要的东西，于是我在codis的基础上面直接弄了一个支持LedisDB的集群proxy出来。</p>
<p>今年为了提升自己的算法水平，蛋疼的把leetcode上面的题目全做了一遍，虽然一些是google出来的，但也至少让自己学到了很多。现在也正在写一本<a href="https://www.gitbook.io/book/siddontang/leetcode-solution/" target="_blank" rel="external">Leetcode题解</a>的书，希望明年能搞定,当然不是为了出版，只是为了更好的提升自己的算法水平。</p>
<h2 id="分享">分享</h2>
<p>今年秉着走出去的原则，参加了两场分享会，一场是在腾讯的技术沙龙讲LedisDB，不过自己演讲水平太挫，效果特差。另一场是在珠海北理工讲web service的开发，忽悠了半天，后来发现，现在的学生盆友重点关注的是iOS，android这些的东西。</p>
<p>这两场分享让我锻炼了一下口才，其实还蛮不错的，如果明年有机会，也争取参加一下。</p>
<h2 id="工作">工作</h2>
<p>今年的工作主要集中在推送服务器以及go服务重构上面，当时写push，设想的是能给整个公司提供推送服务，只是计划没有变化快，最后也成了一个鸡肋产品。</p>
<p>以前我们的服务是用openresty + python，随着系统越来越复杂，这套架构也有了很多弊端，所以我们决定使用go完全重构，现在仍在进行中。</p>
<p>今年整个公司变动挺大的，我的老大，一个11年的员工也出去创业了。想想比较唏嘘，当初面试我的两个老大，把我拉进来的都走了。</p>
<p>明年我的担子比较重，因为go的重构是我负责，明年铁定要上的，到时候很多问题都需要我来协调处理了。</p>
<h2 id="展望">展望</h2>
<p>扯了这么多，其实发现2014年也没做什么，但就这么快速的过去了，那2015年了，干点啥呢：</p>
<ul>
<li>育儿，这个发现真挺重要的，孩子马上一岁了，逐渐懂事了，所以铁定要多花时间陪孩子了。</li>
<li>继续开源，仍然用go开发，还是NoSQL和MySQL方向，没准MQ也会有。重点完成mysql proxy的后续开发。</li>
<li>练习英语，现在跟外国盆友交流只敢打字，说话什么的那是不可能的。所以这方面一定要加强。奶爸推荐的一些学习方法还是挺不错的，要强迫自己学习。另外，争取看完全英文的哈利波特，或者其他入门级的英文读物。</li>
<li>完成LeetCode题解这本书，前面已经说了，这是对我自己一段时间算法学习的总结。</li>
<li>争取每周一篇文章，技术的，人文的都可以，今年零零散散写了一点，但还是不够。明年争取能在medium上面用英文多写几篇文章，今年只写了可怜的两篇。</li>
<li>docker，这玩意现在太火了，我也正在推进开发中使用docker，不过没准能在生产环境中也用到，需要好好研究。</li>
<li>深入学习网络以及Linux底层知识，这方面要加强，现在只是知道大概，稍微深入一点就不懂了，做高性能服务器开发，得掌握。</li>
<li>读书，不是技术书籍，争取每月读一本好书，小说也行，哲学，经济，历史都成。</li>
<li>锻炼，好吧，现在身体太差，不能这样懒了。跑步也行，游泳也成，总之要动起来了。</li>
</ul>
<h2 id="最后">最后</h2>
<p>2014年就快过去了，马上迎来2015，希望明年越来越好。</p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习zookeeper]]></title>
    <link href="http://siddontang.com/2014/12/03/learn-zookeeper/"/>
    <id>http://siddontang.com/2014/12/03/learn-zookeeper/</id>
    <published>2014-12-03T12:49:37.000Z</published>
    <updated>2014-12-03T12:51:23.000Z</updated>
    <content type="html"><![CDATA[<p>最近研究了一下zookeeper（后续以zk简称），对于一个自认为泡在服务器领域多年的老油条来说，现在才开始关注zk这个东西，其实有点晚了，但没办法，以前的工作经历让我压根用不到这个玩意。只是最近因为要考虑做ledisdb的cluster方案，以及重新考虑mixer的协调管理，才让我真正开始尝试去了解zk。</p>
<h2 id="什么是zookeeper">什么是zookeeper</h2>
<p>根据官网的介绍，zookeeper是一个分布式协调服务，主要用来处理分布式系统中各系统之间的协作问题的。</p>
<p>其实这么说有点抽象，初次接触zk，很多人真不知道用它来干啥，你可以将它想成一个总控节点（当然它能用多机实现自身的HA），能对所有服务进行操作。这样就能实现对整个分布式系统的统一管理。</p>
<p>譬如我现在有n台机器，需要动态更新某一个配置，一些做法可能是通过puppet或者salt将配置先分发到不同机器，然后运行指定的reload命令。zk的做法可能是所有服务都监听一个配置节点，直接更改这个节点的数据，然后各个服务就能收到更新消息，然后同步最新的配置，再自行reload了。</p>
<p>上面只是一个很简单的例子，其实通过它并不能过多的体现zk的优势（没准salt可能还更简单），但zk不光只能干这些，还能干更awesome的事情。网上有太多关于zk应用场景一览的文章了，这里就不详细说明，后续我只会说一下自己需要用zk解决的棘手问题。</p>
<h2 id="架构">架构</h2>
<p>zk使用类paxos算法来保证其HA，每次通过选举得到一个master用来处理client的请求，client可以挂载到任意一台zk server上面，因为paxos这种是强一致同步算法，所以zk能保证每一台server上面数据都是一致的。架构如下：</p>
<pre><code>                                                                     
                      +-------------------------------+                         
                      |                               |                         
              +----+--++          +----+---+        +-+--+---+                  
              | server |          | server |        | server |                  
              |        +----------+ master +--------+        |                  
              +--^--^--+          +----^---+        +----^---+                  
                 |  |                  |                 |                      
                 |  |                  |                 |                      
                 |  |                  |                 |                      
           +-----+  +-----+            +------+          +---------+            
           |              |                   |                    |            
           |              |                   |                    |            
      +----+---+        +-+------+         +--+-----+           +--+-----+      
      | client |        | client |         | client |           | client |      
      +--------+        +--------+         +--------+           +--------+
</code></pre><h2 id="Data_Model">Data Model</h2>
<p>zk内部是按照类似文件系统层级方式进行数据存储的，就像这样：</p>
<pre><code>                        +---+             
                        | / |             
                        +++-+             
                         ||               
                         ||               
          +-------+------++----+-------+  
          | /app1 |            | /app2 |  
          +-+--+--+            +---+---+  
            |  |                   |      
            |  |                   |      
            |  |                   |      
+----------++ ++---------+    +----+-----+
| /app1/p1 |  | /app1/p2 |    | /app2/p1 |
+----------+  +----------+    +----------+
</code></pre><p>对于任意一个节点，我们称之为znode，znode有很多属性，譬如<code>Zxid</code>（每次更新的事物ID）等，具体可以详见zk的文档。znode有ACL控制，我们可以很方便的设置其读写权限等，但个人感觉对于内网小集群来说意义不怎么大，所以也就没深入研究。</p>
<p>znode有一种Ephemeral Node，也就是临时节点，它是session有效的，当session结束之后，这个node自动删除，所以我们可以用这种node来实现对服务的监控。譬如一个服务启动之后就向zk挂载一个ephemeral node，如果这个服务崩溃了，那么连接断开，session无效了，这个node就删除了，我们也就知道该服务出了问题。</p>
<p>znode还有一种Sequence Node，用来实现序列化的唯一节点，我们可以通过这个功能来实现一个简单地leader服务选举，譬如每个服务启动的时候都向zk注册一个sequence node，谁最先注册，zk给的sequence最小，这个最小的就是leader了，如果leader当掉了，那么具有第二小sequence node的节点就成为新的leader。</p>
<h3 id="Znode_Watch">Znode Watch</h3>
<p>我们可以watch一个znode，用来监听对应的消息，zk会负责通知，但只会通知一次。所以需要我们再次重新watch这个znode。那么如果再次watch之前，znode又有更新了，client不是收不到了吗？这个就需要client不光要处理watch，同时也需要适当的主动get相关的数据，这样就能保证得到最新的消息了。也就是消息系统里面典型的推拉结合的方式。推只是为了提升性能，快速响应，而拉则为了更好的保证消息不丢失。</p>
<p>但是，我们需要注意一点，zk并不能保证client收到消息之后同时处理，譬如配置文件更新，zk可能通知了所有client，但client并不能全部在同一个时间同时reload，所以为了处理这样的问题，我们需要额外的机制来保证，这个后续说明。</p>
<p>watch只能应用于data（通过get，exists函数）以及children（通过getChildren函数）。也就是监控znode数据更新以及znode的子节点的改变。</p>
<h2 id="API">API</h2>
<p>zk的API时很简单的，如下：</p>
<ul>
<li>create</li>
<li>delete</li>
<li>exists</li>
<li>set data</li>
<li>get data</li>
<li>get chilren</li>
<li>sync</li>
</ul>
<p>就跟通常的文件系统操作差不多，就不过多说明了。</p>
<h2 id="Example">Example</h2>
<p>总的来说，如果我们不深入zk的内部实现，譬如paxos等，zk还是很好理解的，而且使用起来很简单。通常我们需要考虑的就是用zk来干啥，而不是为了想引入一个牛的新特性而用zk。</p>
<h3 id="Lock">Lock</h3>
<p>用zk可以很方便的实现一个分布式lock，记得最开始做企业群组盘的时候，我需要实现一个分布式lock，然后就用redis来弄了一个，其实当时就很担心redis单点当掉的问题，如果那时候我就引入了zk，可能就没这个担心了。</p>
<p>官方文档已经很详细的给出了lock的实现流程：</p>
<ol>
<li>create一个类似path/lock-n的临时序列节点</li>
<li>getChilren相应的path，注意这里千万不能watch，不然惊群很恐怖的</li>
<li>如果1中n是最小的，则获取lock</li>
<li>否则，调用exists watch到上一个比自己小的节点，譬如我现在n是5，我就可能watch node-4</li>
<li>如果exists失败，表明前一个节点没了，则进入步骤2，否则等待，直到watch触发重新进入步骤2</li>
</ol>
<h3 id="Codis">Codis</h3>
<p>最近在考虑ledisdb的cluster方案，本来也打算用proxy来解决的，然后就在想用zk来处理rebalance的问题，结果这时候codis横空出世，发现不用自己整了，于是就好好的研究了一下codis的数据迁移问题。其实也很简单：</p>
<ol>
<li>config发起pre migrate action</li>
<li>proxy接收到这个action之后，将对应的slot设置为pre migrate状态，同时等待config发起migrate action</li>
<li>config等待所有的proxy返回pre migrate之后，发起migrate action</li>
<li>proxy收到migrate action，将对应的slot设置为migrate状态</li>
</ol>
<p>上面这些，都是通过zk来完成的，这里需要关注一下为啥要有pre migrate这个状态，如果config直接发起migrate，那么zk并不能保证proxy同一时间全部更新成migrate状态，所以我们必须有一个中间状态，在这个中间状态里面，proxy对于特定的slot不会干任何事情，只能等待config将其设置为migrate。虽然proxy对于相应slot一段时间无法处理外部请求，但这个时间是很短的（不过此时config当掉了就惨了）。config知道所有proxy都变成pre migrate状态之后，就可以很放心的发送migrate action了。因为这时候，proxy只有两种可能，变成migrate状态，能正常工作，仍然还是pre migrate状态，不能工作，也自然不会对数据造成破坏。</p>
<p>其实上面也就是一个典型的2PC，虽然仍然可能有隐患，譬如config当掉，但并不会对实际数据造成破坏。而且config当掉了我们也能很快知晓并重新启动，所以问题不大。</p>
<h2 id="总结">总结</h2>
<p>总的来说，zk的使用还是挺简单的，只要我们知道它到底能用到什么地方，那zk就真的是分布式开发里面一把瑞士军刀了。不过我挺不喜欢装java那套东西，为了zk也没办法，虽然go现在也有etcd这些类zk的东西了，但毕竟还没经受过太多的考验，所以现在还是老老实实的zk吧。</p>
]]></content>
    
    
      <category term="zookeeper" scheme="http://siddontang.com/tags/zookeeper/"/>
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[深入浅出Web Service]]></title>
    <link href="http://siddontang.com/2014/10/16/head-first-web-service/"/>
    <id>http://siddontang.com/2014/10/16/head-first-web-service/</id>
    <published>2014-10-16T14:39:47.000Z</published>
    <updated>2014-10-16T14:45:13.000Z</updated>
    <content type="html"><![CDATA[<p>应<a href="http://my.oschina.net/javayou" target="_blank" rel="external">红薯</a>的邀约，决定给某大学的童鞋讲讲Web Service相关知识，鉴于是第一次在学校献丑，所以还是老老实实的准备，先把类似逐字稿的东西写出来，然后在准备PPT吧。</p>
<p>关于Web service，这个话题太广太泛，加之我也只熟悉一些特定的领域，所以准备从两方面入手，1，什么是Web service，就当是概念性的介绍，让大家有个相关认识。2，则是根据一个简单的例子，告诉大家如何构建一个Web service服务。</p>
<h2 id="什么是Web_service">什么是Web service</h2>
<p>首先根据Wiki的定义：<strong>A Web Service is a method of communication between two electronic devices over a network.</strong> </p>
<p>简单来说，Web Service就是基于网络不同设备之间互相通信的一种方式。Web Service是一个软件服务，它提供很多API，而客户端通过Web协议进行调用从而完成相关的功能。</p>
<p>Web service并不是一个新奇的概念，相反从很早的分布式计算，到网格计算，到现在的云，都或多或少的有着Web service的影子。只不过随着近几年一浪高过一浪的互联网热潮以及Google，Amazon等公司的大力推动，Web service变得愈发流行。</p>
<p>越来越多的公司开始提供Web service，而同时又有更多的公司基于这些Web service提供了更加上层的Web service。</p>
<p>Amazon的S3（Simple Storage Service）是一个文件存储服务，用户通过S3将文件存放到Amazon的服务器上面，Amazon负责保证该文件的安全（包括不被别人获取，不丢失等）。而Drew Houston则在S3的基础上，构造了一个令人惊奇的同步网盘：Dropbox，同时，Dropbox又将相关API提供出去，供其他的Application其同步服务。</p>
<p>可以看到，正是因为有了越来越多的Web services，才让我们现在的互联网生活变得越发精彩。</p>
<h2 id="实现一个简单的Web_service">实现一个简单的Web service</h2>
<p>好了，上面扯了这么多，是不是心痒痒想自己开发一个Web service？开发一个Web service并不是那么容易的事情，尤其是涉及到分布式之后。不过我觉得一个小例子没准就能说明很多东西。当然我自认并不是Web service的专家（这年头专家架构师太多，我只能算打酱油的），很多东西难免疏漏，并且一些设计也会带有很强烈的个人色彩，如果大家有啥更好的认识，欢迎跟我讨论（妹子优先！）。</p>
<p>一个简单的例子：KV Storage Service，后面就叫KV吧。类似于S3，只是我们不是存文件，而是元数据。后面我们就用KV来表明服务的名字吧。</p>
<p>对于KV来说，它只会涉及到三种操作，如果用代码表示如下：</p>
<pre><code>//根据指定的key获取对应的value
Get(key)

//设置key的值为value，如果key本来存在，则更新，否则新建
Put(key, value)

//删除key
Delete(key)
</code></pre><h3 id="交互协议">交互协议</h3>
<p>既然是Web service，自然选用HTTP来做交互，比起自己实现一套不通用的协议，或者使用Google Protocol Buffers这些的，HTTP具有太多的优势，虽然它的性能稍微有点差，数据量稍微有点臃肿，但几乎所有的浏览器以及数不清的库能直接支持，想想都有点小激动了。</p>
<p>所以我们唯一要做的，就是设计好我们的API，让外面更方便的使用Web service。</p>
<h3 id="API">API</h3>
<p>根据wiki的定义，Web service通常有两种架构方式，RESTful和Arbitrary（RPC，SOAP，etc）。</p>
<p>REST是Representational state transfer的缩写，而满足REST架构模型的我们通常称之为Restful：</p>
<ul>
<li>使用URI来表示资源，譬如<code>http://example.com/user/1</code> 代表ID为1的user。</li>
<li>使用标准HTTP方法GET，POST，PUT，DELETE等来操作资源，譬如<code>Get http://example.com/user/1</code> 来获取user 1的信息，而使用<code>Delete http://example.com/user/1</code> 来删除user 1。</li>
<li>支持资源的多种表现形式，譬如上例Get中设置Content-Type为json，让服务端返回json格式的user信息。</li>
</ul>
<p>相对于Restful，另一种则是Arbitrary的，我不熟悉SOAP，这里就以RPC为例。</p>
<p>RPC就是remote procedure call，它通过在HTTP请求中显示的制定需要调用的过程名字以及参数来与服务端进行交互。仍然是上面的例子，如果我们需要得到用户的信息，可能就是这样 <code>Get http://example.com/getuser?userid=1</code>, 如果要删除一个用户，没准是这样<code>Get http://example.com/delUser?userid=1</code>。</p>
<p>那选择何种架构呢？在这里，我倾向使用Restful架构模型，很大原因在于它理解起来很容易，而且实现简单，而现在越来越多的Web service提供的API采用的是Restful模式，从另一个方面也印证了它的流行。</p>
<p>所以这个Web service的接口就是这样了：</p>
<pre><code>GET http://kv.com/key
DELETE http://kv.com/key
POST http://kv.com/key -dvalue
PUT http://kv.com/key -dvalue
</code></pre><p>上面POST和PUT可以等价，如果key存在，则用value覆盖，不存在则新建。</p>
<h3 id="架构">架构</h3>
<p>好了，扯了这么多，我们也要开始搭建我们的Web service了。因为采用的是HTTP协议，所以我们可以直接使用现成的HTTP server来帮我们处理HTTP请求。譬如nginx，apache，不过用go或者python直接写一个也不是特别困难的事情。</p>
<p>我们还需要一个storage server用来存放key-value，mysql可以，redis也行，或者我的<a href="http://ledisdb.com" target="_blank" rel="external">ledisdb</a>，谁叫红薯说可以打广告的。</p>
<p>最开始，我们就只有一台机器，启动一个nginx用来处理HTTP请求，然后启动一个ledisdb用来存放数据。然后开始对外happy的提供服务了。</p>
<p>KV开始工作的很好，突然有一天，我们发现随着用户量的增大，一台机器处理不过来了。好吧，我们在加一台机器，将nginx和ledisdb放到不同的机器上面。</p>
<p>可是好景不长，用户量越来越多，压力越来越大，我们需要再加机器了，因为nginx是一个无状态的服务，所以我们很容易的将其扩展到多台机器上面去运行，最外层通过DNS或者LVS来做负载均衡。但是对于有状态的服务，譬如上面的ledisdb，可不能这么简单的处理了。好吧，我们终于要开始扯到分布式了。</p>
<h3 id="CAP">CAP</h3>
<p>在聊分布式之前，我们需要知道CAP定理，因为在设计分布式系统的时候，CAP都是必须得面对的。</p>
<ul>
<li>Consistency，一致性</li>
<li>Avaliability，可用性</li>
<li>Partition tolerance，分区容忍性</li>
</ul>
<p>CAP的核心就在于在分布式系统中，你不可能同时满足CAP，而只能满足其中两项，但在分布式中，P是铁定存在的，所以我们设计系统的时候就需要在C和A之间权衡。</p>
<p>譬如，对于MySQL,它最初设计的时候就没考虑分区P，所以很好的满足CA，所以做过MySQL proxy方面工作的童鞋应该都清楚，要MySQL支持分布式是多么的蛋疼。</p>
<p>而对于一般的NoSQL，则是倾向于采用AP，但并不是说不管C，只是允许短时间的数据不一致，但能达到最终一致。</p>
<p>而对于需要强一致的系统，则会考虑牺牲A来满足CP，譬如很多系统必须写多份才算成功，</p>
<h3 id="Replication">Replication</h3>
<p>对于前面提到的Ledisdb，因为涉及到数据存放，本着不要把鸡蛋放到一个篮子里面的原则，我们也不能将数据放到一台机器上面，不然当机了就happy了。而解决这个的办法就是replication。</p>
<p>熟悉MySQL或者Redis的童鞋对replication应该都不会陌生，它们的replication都采用的是异步的方式，也就是在一段时间内不满足数据一致性，但铁定能达到最终一致性。</p>
<p>但如果真想支持同步的replication，怎么办呢？谁叫我们容不得数据半点丢失。通常有几种做法：</p>
<ul>
<li>2PC，3PC</li>
<li>Paxos，Raft</li>
</ul>
<p>因为这方面的坑很深，就不在累述，不过我是很推崇Raft的，相比于2PC，3PC，以及Paxos，Raft足够简单，并且很好理解。有机会在说明吧。</p>
<h3 id="水平扩展">水平扩展</h3>
<p>好了，通过replication解决了ledisdb数据安全问题，但总有一天，一台机器顶不住了，我们要考虑将ledisdb的数据进行拆分到多台机器。通常做法如下：</p>
<ul>
<li>最简单的做法，hash(key) % num，num是机器的数量，但这种做法在添加或者删除机器的时候会造成rehash，导致大量的数据迁移。</li>
<li>一致性hash，它相对于传统的hash，在添加或者删除节点的时候，它能尽可能的少的进行数据迁移。不过终归还是有数据流动的。</li>
<li>路由映射表，不同于一致性hash，我们在外部自己负责维护一张路由表，这样添加删除节点的时候只需要更改路由表就可以了，相对于一致性hash，个人感觉更加可控。</li>
</ul>
<p>我个人比较喜欢预分配+路由表的方式来进行水平扩展，所谓预分配，就是首先我就将数据切分到n个（譬如1024）shard，开始这些shard可以在一个node里面，随着node的增加，我们只需要迁移相关的shard，同时更新路由表就可以了。这种方式个人感觉灵活性最好，但对程序员要求较高，需要写出能自动处理resharding的健壮代码。</p>
<p>好了，解决了replication，解决了水平扩展，很长一段时间我们都能happy，当然坑还是挺多的，遇到了慢慢再填吧。</p>
<h2 id="没有提到的关键点">没有提到的关键点</h2>
<ul>
<li>Cache，无论怎样，cache在服务器领域都是一个非常关键的东西，用好了cache，你的服务能处理更多的并发访问。facebook这篇paper <a href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf" target="_blank" rel="external">Scaling Memcache at Facebook</a>专门讲解了相关知识，那是绝对的干货。</li>
<li>消息队列，当并发量大了之后，光靠同步的API调用已经满足不了整个系统的性能需求，这时候就该MQ上场了，譬如RabbitMQ都是不错的选择。</li>
<li>很多很多其他的。。。。。。。这里就不列举了。</li>
</ul>
<h2 id="总结">总结</h2>
<p>上面只是我对于Web service一点浅显的见解，如果里面的知识稍微对你有用，那我已经感到非常高兴了。但就像实践才是检验真理的唯一标准一样，理论知道的再多，还不如先弄一个Web service来的实在，反正现在国内阿里云，腾讯云，百度云啥的都不缺，缺的只是跑在上面的好应用。</p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[LedisDB Replication设计]]></title>
    <link href="http://siddontang.com/2014/10/05/ledisdb-replication/"/>
    <id>http://siddontang.com/2014/10/05/ledisdb-replication/</id>
    <published>2014-10-05T13:58:59.000Z</published>
    <updated>2014-10-05T14:00:17.000Z</updated>
    <content type="html"><![CDATA[<p>对于使用SQL或者NoSQL的童鞋来说，replication都是一个避不开的话题，通过replication，能极大地保证你的数据安全性。毕竟谁都知道，不要把鸡蛋放在一个篮子里，同理，也不要把数据放到一台机器上面，不然机器当机了你就happy了。</p>
<p>在分布式环境下，对于任何数据存储系统，实现一套好的replication机制是很困难的，毕竟<a href="http://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="external">CAP</a>的限制摆在那里，我们不可能实现出一套完美的replication机制，只能根据自己系统的实际情况来设计和对CAP的取舍。</p>
<p>对于replication更详细的说明与解释，这里推荐<a href="http://book.mixu.net/distsys/index.html" target="_blank" rel="external">Distributed systems<br>for fun and profit</a>，后面，我会根据LedisDB的实际情况，详细的说明我在LedisDB里面使用的replication是如何实现的。</p>
<h2 id="BinLog">BinLog</h2>
<p>最开始的时候，Ledisdb采用的是类似MySQL通用binlog的replication机制，即通过binlog的filename + position来决定需要同步的数据。这套方式实现起来非常简单，但是仍然有一些不足，主要就在于hierarchical replication情况下如果master当掉，选择合适的slave提升为master是比较困难的。举个最简单的例子，假设A为master，B，C为slave，如果A当掉了，我们会在B，C里面选择同步数据最多的那个，但是是哪一个呢？这个问题，在MySQL的replication中也会碰到。</p>
<h2 id="MySQL_GTID">MySQL GTID</h2>
<p>在MySQL 5.6之后，引入了GTID（Global transaction ID）的概念来解决上述问题，它通过<code>Source:ID</code>的方式来在binlog里面表示一个唯一的transaction。Source为当前server的uuid，这个是全局唯一的，而ID则是该server内部的transaction ID（采用递增保证唯一）。具体到上面那个问题，采用GTID，如果A当掉了，我们只需要在B和C的binlog里面查找比较最后一个A这个uuid的transaction id的大小，譬如B的为uuid:10，而C的为uuid:30，那么铁定我们会选择C为新的master。</p>
<p>当然使用GTID也有相关的限制，譬如slave也必须写binlog等，但它仍然足够强大，解决了早期MySQL replication的时候一大摊子的棘手问题。但LedisDB并不准备使用，主要就在于应用场景没那么复杂的情况，我需要的是一个更加简单的解决方案。</p>
<h2 id="Google_Global_Transaction_ID">Google Global Transaction ID</h2>
<p>早在MySQL的GTID之前，google的一个MySQL版本就已经使用了<a href="https://code.google.com/p/google-mysql-tools/wiki/GlobalTransactionIds" target="_blank" rel="external">global transaction id</a>，在binlog里面，它对于任何的transaction，使用了group id来唯一标示。group id是一个全局的递增ID，由master负责维护生成。当master当掉之后，我们只需要看slave的binlog里面谁的group id最大，那么那一个就是能被选为master了。</p>
<p>可以看到，这套方案非常简单，但是限制更多，譬如slave端的binlog只能由replication thread写入，不支持Multi-Masters，不支持circular replication等。但我觉得它已经足够简单高效，所以LedisDB准备参考它来实现。</p>
<h2 id="Raft">Raft</h2>
<p>弄过分布式的童鞋应该都或多或少的接触过Paxos（至少我是没完全弄明白的），而<a href="http://raftconsensus.github.io/" target="_blank" rel="external">Raft</a>则号称是一个比Paxos简单得多的分布式一致性算法。</p>
<p>Raft通过replicated log来实现一致性，假设有A，B，C三台机器，A为Leader，B和C为follower，（其实也就是master和slave的概念）。A的任何更新，都必须首先写入Log（每个Log有一个LogID，唯一标示，全局递增），然后将其Log同步到至少Follower，然后才能在A上面提交更新。如果A当掉了，B和C重新选举，如果哪一台机器当前的LogID最大，则成为Leader。看到这里，是不是有了一种很熟悉的感觉？</p>
<p>LedisDB在支持consensus replication上面，参考了Raft的相关做法。</p>
<h2 id="名词解释">名词解释</h2>
<p>在详细说明LedisDB replication的实现前，有必要解释一些关键字段。</p>
<ul>
<li>LogID：log的唯一标示，由master负责生成维护，全局递增。</li>
<li>LastLogID：当前程序最新的logid，也就是记录着最后一次更新的log。</li>
<li>FirstLogID：当前程序最老的logid，之前的log已经被清除了。</li>
<li>CommitID：当前程序已经处理执行的log。譬如当前LastLogID为10，而CommitID为5，则还有6，7，8，9，10这几个log需要执行处理。如果CommitID = LastLogID，则证明程序已经处于最新状态，不再需要处理任何log了。</li>
</ul>
<h2 id="LedisDB_Replication">LedisDB Replication</h2>
<p>LedisDB的replication实现很简单，仍然是上面的例子，A，B，C三台机器，A为master，B和C为slave。</p>
<p>当master有任何更新，master会做如下事情：</p>
<ol>
<li>记录该更新到log，logid = LastLogID + 1，LastLogID = logid</li>
<li>同步该log到slaves，等待slaves的确认返回，或者超时</li>
<li>提交更新</li>
<li>更新CommitID = logid</li>
</ol>
<p>上面还需要考虑到错误处理的情况。</p>
<ul>
<li>如果1失败，记录错误日志，然后我们会认为该次更新操作失败，直接返回。</li>
<li>如果3失败，不更新CommitID返回，因为这时候CommitID小于LastLogID，master进入read only模式，replication thread尝试执行log，如果能执行成功，则更新CommitID，变成可写模式。</li>
<li>如果4失败，同上，因为LedisDB采用的是Row-Base Format的log格式，所以一次更新操作能够幂等多次执行。</li>
</ul>
<p>对于slave</p>
<p>如果是首次同步，则进入全同步模式：</p>
<ol>
<li>master生成一个snapshot，连同当前的LastLogID一起发送给slave。</li>
<li>slave收到该dump文件之后，load载入，同时更新CommitID为dump文件里面的LastLogID。</li>
</ol>
<p>然后进入增量同步模式，如果slave已经有相关log，则直接进入增量同步模式。</p>
<p>在增量模式下面，slave向master发送sync命令，sync的参数为下一个需要同步的log，如果slave当前没有binlog（譬如上面提到的全同步情况），则logid = CommitID + 1， 否则logid = LastLogID + 1。</p>
<p>master收到sync请求之后，有如下处理情况：</p>
<ul>
<li>sync的logid小于FirstLogID，master没有该log，slave收到该错误重新进入全同步模式。</li>
<li>master有该sync的log，于是将log发送给slave，slave收到之后保存，并再次发送sync获取下一个log，同时该次请求也作为ack告知master同步该log成功。</li>
<li>sync的log id已经大于LastLogID了，表明master和slave的状态已经到达一致，没有log可以同步了，slave将会等待新的log直到超时再次发送sync。</li>
</ul>
<p>在slave端，对于接受到的log，由replication thread负责执行，并更新CommitID。</p>
<p>如果master当机，我们只需要选择具有最大LastLogID的那个slave为新的master就可以了。</p>
<h2 id="Limitation">Limitation</h2>
<p>总的来说，这套replication机制很简单，易于实现，但是仍然有许多限制。</p>
<ul>
<li>不支持Multi-Master，因为同时只能有一个地方进行全局LogID的生成。不过我真的很少见到Multi-Master这样的架构模式，即使在MySQL里面。</li>
<li>不支持Circular-Replication，slave写入的log id不允许小于当前的LastLogID，这样才能保证只同步最新的log。</li>
<li>没有自动master选举机制，不过我觉得放到外部去实现更好。</li>
</ul>
<h2 id="Async/Sync_Replication">Async/Sync Replication</h2>
<p>LedisDB是支持强一致性的同步replication的，如果配置了该模式，那么master会等待slave同步完成log之后再提交更新，这样我们就能保证当master当机之后，一定有一台slave具有跟master一样的数据。但在实际中，可能因为网络环境等问题，master不可能一直等待slave同步完成log，所以通常都会有一个超时机制。所以从这点来看，我们仍然不能保证数据的强一致性。</p>
<p>使用同步replication机制会极大地降低master的写入性能，如果对数据一致性不敏感的业务，其实采用异步replication就可以了。</p>
<h2 id="Failover">Failover</h2>
<p>LedisDB现在没有自动的failover机制，master当机之后，我们仍然需要外部的干预来选择合适的slave（具有最大LastLogID那个），提升为master，并将其他slave重新指向该master。后续考虑使用外部的keeper程序来处理。而对于keeper的单点问题，则考虑使用raft或者zookeeper来处理。</p>
<h2 id="后记">后记</h2>
<p>虽然LedisDB现在已经支持replication，但仍然需要在生产环境中检验完善。</p>
<p>LedisDB是一个采用Go实现的高性能NoSQL，接口类似Redis，现在已经用于生产环境，欢迎大家使用。</p>
<p><a href="http://ledisdb.com" target="_blank" rel="external">Official Website</a></p>
<p><a href="http://github.com/siddontang/ledisdb" target="_blank" rel="external">Github</a></p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="nosql" scheme="http://siddontang.com/tags/nosql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Reinventing the wheel?]]></title>
    <link href="http://siddontang.com/2014/08/12/reinventing-the-wheel/"/>
    <id>http://siddontang.com/2014/08/12/reinventing-the-wheel/</id>
    <published>2014-08-12T15:03:49.000Z</published>
    <updated>2014-08-12T15:04:31.000Z</updated>
    <content type="html"><![CDATA[<p><script async src="https://static.medium.com/embed.js"></script><a class="m-story" data-collapsed="true" href="https://medium.com/@siddontang/8241f1ba9068" target="_blank" rel="external">Reinventing the wheel?</a></p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[How do I develop Libtnet - a fast network library]]></title>
    <link href="http://siddontang.com/2014/08/10/how-do-I-develop-libtnet/"/>
    <id>http://siddontang.com/2014/08/10/how-do-I-develop-libtnet/</id>
    <published>2014-08-10T11:51:37.000Z</published>
    <updated>2014-08-10T12:43:27.000Z</updated>
    <content type="html"><![CDATA[<p><script async src="https://static.medium.com/embed.js"></script><a class="m-story" data-collapsed="true" href="https://medium.com/@siddontang/630212843c87" target="_blank" rel="external">How do I develop Libtnet: a fast network library</a></p>
]]></content>
    
    
      <category term="c++" scheme="http://siddontang.com/tags/c++/"/>
    
      <category term="libtnet" scheme="http://siddontang.com/tags/libtnet/"/>
    
      <category term="c++" scheme="http://siddontang.com/categories/c++/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用gopkg进行Go包管理]]></title>
    <link href="http://siddontang.com/2014/07/29/gopkg-introduction/"/>
    <id>http://siddontang.com/2014/07/29/gopkg-introduction/</id>
    <published>2014-07-28T16:01:01.000Z</published>
    <updated>2014-08-09T07:11:47.000Z</updated>
    <content type="html"><![CDATA[<p>在使用go的过程中，我们有时候会引入一些第三方库来使用，而通常的方式就是使用 <strong>go get</strong> ，但是这种方式有一个很严重的问题，如果第三方库更新了相关接口，很有可能你就无法使用了，所以我们一套很好地包管理机制。</p>
<p>在<a href="http://blog.csdn.net/siddontang/article/details/25601611" target="_blank" rel="external">读生产环境下go语言最佳实践有感</a>一文中，我介绍过soundcloud公司的做法，直接将第三库的代码check下来，放到自己工程的vendor目录里面，或者使用godep。</p>
<p>不过现在，我发现了一种更好的包管理方式<a href="http://labix.org/gopkg.in" target="_blank" rel="external">gopkg</a>。它通过约定使用带有版本号的url来让go tool去check指定的版本库，虽然现在只支持github的go repositories，但是我觉得已经足够强大。</p>
<p>一个很简单的例子，我们通过如下方式获取go的yaml包</p>
<pre><code>go get gopkg.in/yaml.v1
</code></pre><p>而实际上，该yaml包对应的地址为：</p>
<pre><code>https://github.com/go-yaml/yaml
</code></pre><p>yaml.v1表明版本为v1，而在github上面，有一个对应的v1 branch。</p>
<p>gopkg支持的url格式很简单：</p>
<pre><code>gopkg.in/pkg.v3      → github.com/go-pkg/pkg (branch/tag v3, v3.N, or v3.N.M)
gopkg.in/user/pkg.v3 → github.com/user/pkg   (branch/tag v3, v3.N, or v3.N.M)
</code></pre><p>我们使用v.N的方式来定义一个版本，然后再github上面对应的建立一个同名的分支。gopkg支持<strong>(vMAJOR[.MINOR[.PATCH]])</strong>这种类型的版本模式，如果存在多个major相同的版本，譬如v1，v1.0.1，v1.1.2，那么gopkg会选用最高级别的v1.1.2使用，譬如有如下版本：</p>
<ul>
<li>v1</li>
<li>v2.0</li>
<li>v2.0.3</li>
<li>v2.1.2</li>
<li>v3</li>
<li>v3.0</li>
</ul>
<p>那么gopkg对应选用的方式如下：</p>
<ul>
<li>pkg.v1 -&gt; v1</li>
<li>pkg.v2 -&gt; v2.1.2</li>
<li>pkg.v3 -&gt; v3.0</li>
</ul>
<p>gopkg不建议使用v0，也就是0版本号。</p>
<p>gopkg同时列出了一些建议，在更新代码之后是否需要升级主版本或者不需要，一些必须升级主版本的情况：</p>
<ul>
<li>删除或者重命名了任何的导出接口，函数，变量等。</li>
<li>给接口增加，删除或者重命名函数</li>
<li>给函数或者接口增加参数</li>
<li>更改函数或者接口的参数或者返回值类型</li>
<li>更改函数或者接口的返回值个数</li>
<li>更改结构体</li>
</ul>
<p>而一下情况，则不需要升级主版本号：</p>
<ul>
<li>增加导出接口，函数或者变量</li>
<li>给函数或者接口的参数名字重命名了</li>
<li>更改结构体</li>
</ul>
<p>上面都提到了更改结构体，譬如我给一个结构体增加字段，就可能不需要升级主版本，但是如果删除结构体的一个导出字段，那就必须要升级了。如果只是单纯的更改改结构体里面非导出字段的东西，也不需要升级。</p>
<p>更加详细的信息，请直接查看<a href="http://labix.org/gopkg.in" target="_blank" rel="external">gopkg</a></p>
<p>可以看到，gopkg使用了一种很简单地方式让我们方便的对go pakcage进行版本管理。于是我也依葫芦画瓢，给我的log package做了一个v1版本的，你可以直接</p>
<pre><code>go get gopkg.in/siddontang/go-log.v1/log
</code></pre>]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="gopkg" scheme="http://siddontang.com/tags/gopkg/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[LeetCode程序员面试技巧]]></title>
    <link href="http://siddontang.com/2014/07/22/leetcode-interview-tips-for-programmers/"/>
    <id>http://siddontang.com/2014/07/22/leetcode-interview-tips-for-programmers/</id>
    <published>2014-07-22T15:01:29.000Z</published>
    <updated>2014-08-08T16:11:39.000Z</updated>
    <content type="html"><![CDATA[<h2 id="起因">起因</h2>
<p>写在开头，脑袋铁定秀逗了，历时20多天，刷完了leetcode上面151道题目（当然很多是google的），感觉自己对算法和数据结构算是入门了，但仍然还有很多不清楚的地方，于是有了对于每道题目写分析的冲动。不过在看到leetcode上面的文章之后，决定先从翻译入手，顺带再写写自己做题的心得体会。今天是第一篇：程序员面试技巧。</p>
<p>如果你主修计算机科学，那么在你工作的时候会碰到很多有难度的编程问题。当你去找工作的时候，你会有很多的面试，而面试官通常很喜欢问你很多技术性的问题，以下就是三类主要的题型：</p>
<ul>
<li>算法/数据结构</li>
<li>特定编程语言/通用的计算机知识</li>
<li>脑筋急转弯</li>
</ul>
<p>后续我将详细的讨论上面这几点：</p>
<h2 id="算法/数据结构">算法/数据结构</h2>
<p>你必须深刻的理解以下几种数据结构：数组，链表，二叉树，哈希表等。你必须明确地知道什么时候该使用数组或者链表，譬如在实现一个列表翻转的时候。</p>
<p>面试官通常会问你一些算法问题用以检验你的编程解决问题能力，一些算法问题如下：</p>
<ul>
<li>在一个句子里面反转单词。</li>
<li>在一个单词里面反转字母。（尝试使用迭代或者递归解决）</li>
<li>如何在一个列表里面找到重复的数字。</li>
<li>生成数字n的全排列。（小提示：递归）</li>
<li>找到两个排好序数组的中间值。（小提示：分治法）</li>
<li>如何进行拼写检查并验证单词。（小提示：编辑距离）</li>
</ul>
<p>（译者吐槽，说句实话，上面几个问题如果没做题大部分我还真答不出来。）</p>
<h2 id="特定编程语言/通用的计算机知识">特定编程语言/通用的计算机知识</h2>
<p>如果你应聘的是C++职位，需要准备好应对很多C++的问题，其它编程语言也一样。如果你应聘的职位不需要特定的编程语言，那么你可能会被问到通用的计算机知识，譬如在堆和栈上面创建对象的区别。</p>
<p>我经常碰到的一些C++问题如下：</p>
<ul>
<li>什么是虚函数？怎么实现的？使用虚函数会有啥性能问题？</li>
<li>什么是虚析构函数？为什么需要他们？如果你不实现成虚析构会出现什么问题？</li>
<li>什么是拷贝构造函数？它什么时候被调用？</li>
<li>malloc和new的区别是什么？</li>
</ul>
<p>（译者吐槽，上面这些感觉还算靠谱，至少没问虚拟继承是啥！我面试的时候通常还会问很多STL的东西，毕竟这在C++里面已经是很基础的了。Template这些的就算了，有时候都能把自己绕晕，还是别坑面试者了。）</p>
<h2 id="脑筋急转弯">脑筋急转弯</h2>
<p>一些面试官很喜欢用一个脑筋急转弯来结束面试。这个其实很坑爹的，不过还是老老实实的准备一些吧。</p>
<p>一些经典的问题：</p>
<ul>
<li><p>两个鸡蛋问题：</p>
<p>  你有两个相同的鸡蛋，然后还有100层楼等你去爬。你需要知道最高到哪一层扔下鸡蛋，鸡蛋不会摔碎。（鸡蛋语录：为啥受伤的总是蛋蛋？）。很有可能第一层就碎了，也可能到了100层才碎。你需要知道最多尝试几次就能找到答案。</p>
</li>
<li><p>尽可能快的在一个byte里面反转bit。</p>
</li>
<li><p>你有5个相同的罐子，都装着球，其中有4个里面每个球都是重10g，另一个灌每个球重9g，你有一台电子称，只称一次，找到那个重9g球的罐子。</p>
</li>
</ul>
<p>（译者吐槽，上面这些脑筋急转弯问题感觉就是算法问题，哪里是脑筋急转弯，明显的忽悠！）</p>
<h2 id="总结">总结</h2>
<p>当然这是译者自己的，原文在上面就结束了，可以看到，作为一个程序员，我们其实是幸运的，因为我们能够接触如此多的挑战，并不断提升自己。可我们同时也是郁闷的，很多时候往往都有这样的错觉，数据结构和算法在很多工作中并不用到，但是为啥偏偏就面这个，linus貌似说过：“Bad programmers worry about the code. Good programmers worry about data structures and their relationships”，而在《大教堂与集市》这本书里面，作者直接说明“Smart data structures and dumb code works a lot better than the other way around”。有时候当写程序写多了，自然就发现算法和数据结构的重要性了。</p>
<p>另外，现在无论哪个公司，除非你是大牛级别的，面试几乎不会脱离上面那些东西，所以还是老老实实的学习吧，骚年。</p>
]]></content>
    
    
      <category term="leetcode" scheme="http://siddontang.com/tags/leetcode/"/>
    
      <category term="algorithm" scheme="http://siddontang.com/tags/algorithm/"/>
    
      <category term="leetcode" scheme="http://siddontang.com/categories/leetcode/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[一个Go可变参数问题]]></title>
    <link href="http://siddontang.com/2014/07/20/golang-func-vargs/"/>
    <id>http://siddontang.com/2014/07/20/golang-func-vargs/</id>
    <published>2014-07-20T11:03:19.000Z</published>
    <updated>2014-08-08T15:58:02.000Z</updated>
    <content type="html"><![CDATA[<p>几天前纠结了一个蛋疼的问题，在go里面函数式支持可变参数的，譬如…T，go会创建一个slice，用来存放传入的可变参数，那么，如果创建一个slice，例如a，然后以a…这种方式传入，go会不会还会新建一个slice，将a的数据全部拷贝一份过去？</p>
<p>如果a很大，那么将会造成很严重的性能问题，不过后来想想，可能是自己多虑了，于是查看go的文档，发现如下东西：</p>
<blockquote>
<p>Passing arguments to … parameters</p>
<p>If f is variadic with a final parameter p of type …T, then within f the type of p is equivalent to type []T. If f is invoked with no actual arguments for p, the value passed to p is nil. Otherwise, the value passed is a new slice of type []T with a new underlying array whose successive elements are the actual arguments, which all must be assignable to T. The length and capacity of the slice is therefore the number of arguments bound to p and may differ for each call site.</p>
<p>Given the function and calls</p>
<pre><code>func Greeting(prefix string, who ...string)
Greeting(&quot;nobody&quot;)
Greeting(&quot;hello:&quot;, &quot;Joe&quot;, &quot;Anna&quot;, &quot;Eileen&quot;)
</code></pre><p>within Greeting, who will have the value nil in the first call, and []string{“Joe”, “Anna”, “Eileen”} in the second.</p>
<p>If the final argument is assignable to a slice type []T, it may be passed unchanged as the value for a …T parameter if the argument is followed by …. In this case no new slice is created.</p>
<p>Given the slice s and call</p>
<pre><code>s := []string{&quot;James&quot;, &quot;Jasmine&quot;}
Greeting(&quot;goodbye:&quot;, s...)
</code></pre><p>within Greeting, who will have the same value as s with the same underlying array.</p>
</blockquote>
<p>也就是说，如果我们传入的是slice…这种形式的参数，go不会创建新的slice。写了一个简单的例子验证：</p>
<pre><code>package main

import &quot;fmt&quot;

func t(args ...int) {
    fmt.Printf(&quot;%p\n&quot;, args)
}

func main() {
    a := []int{1,2,3}
    b := a[1:]

    t(a...)
    t(b...)

    fmt.Printf(&quot;%p\n&quot;, a)
    fmt.Printf(&quot;%p\n&quot;, b)
}

//output
0x1052e120
0x1052e124
0x1052e120
0x1052e124
</code></pre><p>可以看到，可变参数args的地址跟实际外部slice的地址一样，用的同一个slice。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[celery最佳实践]]></title>
    <link href="http://siddontang.com/2014/07/20/celery-best-practices/"/>
    <id>http://siddontang.com/2014/07/20/celery-best-practices/</id>
    <published>2014-07-20T11:03:19.000Z</published>
    <updated>2014-08-09T07:12:29.000Z</updated>
    <content type="html"><![CDATA[<p>作为一个Celery使用重度用户，看到<a href="https://denibertovic.com/posts/celery-best-practices/" target="_blank" rel="external">Celery Best Practices</a>这篇文章，不由得菊花一紧。干脆翻译出来，同时也会加入我们项目中celery的实战经验。</p>
<p>通常在使用Django的时候，你可能需要执行一些长时间的后台任务，没准你可能需要使用一些能排序的任务队列，那么Celery将会是一个非常好的选择。</p>
<p>当把Celery作为一个任务队列用于很多项目中后，作者积累了一些最佳实践方式，譬如如何用合适的方式使用Celery，以及一些Celery提供的但是还未充分使用的特性。</p>
<h2 id="1，不要使用数据库作为你的AMQP_Broker">1，不要使用数据库作为你的AMQP Broker</h2>
<p>数据库并不是天生设计成能用于AMQP broker的，在生产环境下，它很有可能在某时候当机（PS，当掉这点我觉得任何系统都不能保证不当吧！！！）。</p>
<p>作者猜想为啥很多人使用数据库作为broker主要是因为他们已经有一个数据库用来给web app提供数据存储了，于是干脆直接拿来使用，设置成Celery的broker是很容易的，并且不需要再安装其他组件（譬如RabbitMQ）。</p>
<p>假设有如下场景：你有4个后端workers去获取并处理放入到数据库里面的任务，这意味着你有4个进程为了获取最新任务，需要频繁地去轮询数据库，没准每个worker同时还有多个自己的并发线程在干这事情。</p>
<p>某一天，你发现因为太多的任务产生，4个worker不够用了，处理任务的速度已经大大落后于生产任务的速度，于是你不停去增加worker的数量。突然，你的数据库因为大量进程轮询任务而变得响应缓慢，磁盘IO一直处于高峰值状态，你的web应用也开始受到影响。这一切，都因为workers在不停地对数据库进行DDOS。</p>
<p>而当你使用一个合适的AMQP（譬如RabbitMQ）的时候，这一切都不会发生，以RabbitMQ为例，首先，它将任务队列放到内存里面，你不需要去访问硬盘。其次，consumers（也就是上面的worker）并不需要频繁地去轮询因为RabbitMQ能将新的任务推送给consumers。当然，如果RabbitMQ真出现问题了，至少也不会影响到你的web应用。</p>
<p>这也就是作者说的不用数据库作为broker的原因，而且很多地方都提供了编译好的RabbitMQ镜像，你都能直接使用，譬如<a href="https://registry.hub.docker.com/search?q=rabbitmq" target="_blank" rel="external">这些</a>。</p>
<p>对于这点，我是深表赞同的。我们系统大量使用Celery处理异步任务，大概平均一天几百万的异步任务，以前我们使用的mysql，然后总会出现任务处理延时太严重的问题，即使增加了worker也不管用。于是我们使用了redis，性能提升了很多。至于为啥使用mysql很慢，我们没去深究，没准也还真出现了DDOS的问题。</p>
<h2 id="2，使用更多的queue（不要只用默认的）">2，使用更多的queue（不要只用默认的）</h2>
<p>Celery非常容易设置，通常它会使用默认的queue用来存放任务（除非你显示指定其他queue）。通常写法如下：</p>
<pre><code>@app.task()
def my_taskA(a, b, c):
    print(&quot;doing something here...&quot;)

@app.task()
def my_taskB(x, y):
    print(&quot;doing something here...&quot;)
</code></pre><p>这两个任务都会在同一个queue里面执行，这样写其实很有吸引力的，因为你只需要使用一个decorator就能实现一个异步任务。作者关心的是taskA和taskB没准是完全两个不同的东西，或者一个可能比另一个更加重要，那么为什么要把它们放到一个篮子里面呢？（鸡蛋都不能放到一个篮子里面，是吧！）没准taskB其实不怎么重要，但是量太多，以至于重要的taskA反而不能快速地被worker进行处理。增加workers也解决不了这个问题，因为taskA和taskB仍然在一个queue里面执行。</p>
<h2 id="3，使用具有优先级的workers">3，使用具有优先级的workers</h2>
<p>为了解决2里面出现的问题，我们需要让taskA在一个队列Q1，而taskB在另一个队列Q2执行。同时指定<strong>x</strong> workers去处理队列Q1的任务，然后使用其它的workers去处理队列Q2的任务。使用这种方式，taskB能够获得足够的workers去处理，同时一些优先级workers也能很好地处理taskA而不需要进行长时间的等待。</p>
<p>首先手动定义queue</p>
<pre><code>CELERY_QUEUES = (
    Queue(&#39;default&#39;, Exchange(&#39;default&#39;), routing_key=&#39;default&#39;),
    Queue(&#39;for_task_A&#39;, Exchange(&#39;for_task_A&#39;), routing_key=&#39;for_task_A&#39;),
    Queue(&#39;for_task_B&#39;, Exchange(&#39;for_task_B&#39;), routing_key=&#39;for_task_B&#39;),
)
</code></pre><p>然后定义routes用来决定不同的任务去哪一个queue</p>
<pre><code>CELERY_ROUTES = {
    &#39;my_taskA&#39;: {&#39;queue&#39;: &#39;for_task_A&#39;, &#39;routing_key&#39;: &#39;for_task_A&#39;},
    &#39;my_taskB&#39;: {&#39;queue&#39;: &#39;for_task_B&#39;, &#39;routing_key&#39;: &#39;for_task_B&#39;},
}
</code></pre><p>最后再为每个task启动不同的workers</p>
<pre><code>celery worker -E -l INFO -n workerA -Q for_task_A
celery worker -E -l INFO -n workerB -Q for_task_B
</code></pre><p>在我们项目中，会涉及到大量文件转换问题，有大量小于1mb的文件转换，同时也有少量将近20mb的文件转换，小文件转换的优先级是最高的，同时不用占用很多时间，但大文件的转换很耗时。如果将转换任务放到一个队列里面，那么很有可能因为出现转换大文件，导致耗时太严重造成小文件转换延时的问题。</p>
<p>所以我们按照文件大小设置了3个优先队列，并且每个队列设置了不同的workers，很好地解决了我们文件转换的问题。</p>
<h2 id="4，使用Celery的错误处理机制">4，使用Celery的错误处理机制</h2>
<p>大多数任务并没有使用错误处理，如果任务失败，那就失败了。在一些情况下这很不错，但是作者见到的多数失败任务都是去调用第三方API然后出现了网络错误，或者资源不可用这些错误，而对于这些错误，最简单的方式就是重试一下，也许就是第三方API临时服务或者网络出现问题，没准马上就好了，那么为什么不试着重试一下呢？</p>
<pre><code>@app.task(bind=True, default_retry_delay=300, max_retries=5)
def my_task_A():
    try:
        print(&quot;doing stuff here...&quot;)
    except SomeNetworkException as e:
        print(&quot;maybe do some clenup here....&quot;)
        self.retry(e)
</code></pre><p>作者喜欢给每一个任务定义一个等待多久重试的时间，以及最大的重试次数。当然还有更详细的参数设置，自己看文档去。</p>
<p>对于错误处理，我们因为使用场景特殊，例如一个文件转换失败，那么无论多少次重试都会失败，所以没有加入重试机制。</p>
<h2 id="5，使用Flower">5，使用Flower</h2>
<p><a href="http://celery.readthedocs.org/en/latest/userguide/monitoring.html#flower-real-time-celery-web-monitor" target="_blank" rel="external">Flower</a>是一个非常强大的工具，用来监控celery的tasks和works。</p>
<p>这玩意我们也没怎么使用，因为多数时候我们都是直接连接redis去查看celery相关情况了。貌似挺傻逼的对不，尤其是celery在redis里面存放的数据并不能方便的取出来。</p>
<h2 id="6，没事别太关注任务退出状态">6，没事别太关注任务退出状态</h2>
<p>一个任务状态就是该任务结束的时候成功还是失败信息，没准在一些统计场合，这很有用。但我们需要知道，任务退出的状态并不是该任务执行的结果，该任务执行的一些结果因为会对程序有影响，通常会被写入数据库（例如更新一个用户的朋友列表）。</p>
<p>作者见过的多数项目都将任务结束的状态存放到sqlite或者自己的数据库，但是存这些真有必要吗，没准可能影响到你的web服务的，所以作者通常设置<strong>CELERY_IGNORE_RESULT = True</strong>去丢弃。</p>
<p>对于我们来说，因为是异步任务，知道任务执行完成之后的状态真没啥用，所以果断丢弃。</p>
<h2 id="7，不要给任务传递_Database/ORM_对象">7，不要给任务传递 Database/ORM 对象</h2>
<p>这个其实就是不要传递Database对象（例如一个用户的实例）给任务，因为没准序列化之后的数据已经是过期的数据了。所以最好还是直接传递一个user id，然后在任务执行的时候实时的从数据库获取。</p>
<p>对于这个，我们也是如此，给任务只传递相关id数据，譬如文件转换的时候，我们只会传递文件的id，而其它文件信息的获取我们都是直接通过该id从数据库里面取得。</p>
<h2 id="最后">最后</h2>
<p>后面就是我们自己的感触了，上面作者提到的Celery的使用，真的可以算是很好地实践方式，至少现在我们的Celery没出过太大的问题，当然小坑还是有的。至于RabbitMQ，这玩意我们是真没用过，效果怎么样不知道，至少比mysql好用吧。</p>
<p>最后，附上作者的一个Celery Talk <a href="https://denibertovic.com/talks/celery-best-practices/" target="_blank" rel="external">https://denibertovic.com/talks/celery-best-practices/</a>。</p>
]]></content>
    
    
      <category term="python" scheme="http://siddontang.com/tags/python/"/>
    
      <category term="celery" scheme="http://siddontang.com/tags/celery/"/>
    
      <category term="python" scheme="http://siddontang.com/categories/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的编程语言经历]]></title>
    <link href="http://siddontang.com/2014/06/22/program-lang-experience/"/>
    <id>http://siddontang.com/2014/06/22/program-lang-experience/</id>
    <published>2014-06-22T13:29:26.000Z</published>
    <updated>2014-08-09T07:15:37.000Z</updated>
    <content type="html"><![CDATA[<p>Alan Perlis 说过：“一种不改变你编程的思维方式的语言，不值得去学。”，虽然写了这么多年程序，用了这么多的语言，但我自认还没悟道编程语言如何改变我的思维方式。</p>
<p>几天前，我需要用python来为<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">ledisdb</a>写一个客户端，我突然发现，对于c++，go这种语言，我如果需要实现一个功能，首先想到的是问题是代码应该怎么写。但是当我使用python的时候，我首先考虑的问题是在哪里去找一个库用来解决我的问题。可能这就是使用不同语言带给我的不同思考方式吧。</p>
<p>我的编程语言经历没有那么复杂，没用过很多，但是其实也够我受的了，尤其是在不同语言语法糖之间切换的时候，有种让人崩溃的感觉。没准我应该升级一下我的大脑cpu，使其能够更快速的进行中断处理。</p>
<h2 id="c">c</h2>
<p>我是从大学才开始学习编程的，相比现在的小朋友来说，可以叫做输在了起跑线上面。谁叫以前生活在山区，没机会接触电脑这玩意。</p>
<p>我的第一门编程语言是c，不同于很多童鞋使用的谭浩强神书，我用的是周纯杰的&lt;<c语言程序设计>&gt;，不知道每年有多少同学受到过它的摧残，当然还有那哥们蹩脚的普通话。</c语言程序设计></p>
<p>在大学里面，很多同学的c的毕业设计都是我帮其完成，但我始终觉得自己仍然是个半吊子，除了c的基础稍微强一点之外，很多方面譬如操作系统，算法等完全不会。（现在随着工作年限的增加让我越发后悔，当初怎么就不稍微学点这些知识，尤其是编译原理。）</p>
<p>我几乎没怎么用c开发过项目，只在tencent可怜的维护过别人的c项目，但至少能看懂c代码，这就够了。</p>
<p>因为大多数时候，我用的是c++，而不是c来解决我的问题。</p>
<h2 id="c++">c++</h2>
<p>c++是我工作使用的第一门语言，也是我使用时间最长的一门语言，都七年之痒了，不过还是有点不离不弃的。</p>
<p>以前上学的时候有一句口头禅，叫学好c++，走遍天下都不怕。但是有几个人能把它学好的？所以千万别说自己精通c++，那会被人鄙视的。</p>
<p>我使用c++可以分为三个阶段：</p>
<h3 id="类c阶段">类c阶段</h3>
<p>这个阶段主要是我第一份工作的时候，那时候才毕业，c的烙印很深，面向对象除了有个概念，真正是啥完全不知道。所以最喜欢的方式还是写着一堆函数来解决问题，当初VIA身边那帮c++的牛人竟然能忍受我这样的代码，真佩服他们。</p>
<h3 id="面向对象阶段">面向对象阶段</h3>
<p>后来去了第二家公司linekong，开始做游戏，才开始真正意义上的用c++写代码了。</p>
<p>记得最开始去第一家公司面试的时候，被问了啥是面向对象，当时不假思索的答了继承，多态和封装。</p>
<p>啥叫封装？整一个class，把该包的都包了，一个同事曾告诉我，他见过有几万行代码的class，看来我这个几千行的太小儿科了。</p>
<p>啥叫继承？先来一个父类，干脆叫bird，有一个fly方法，再来一个子类，叫duck吧，继承了bird，不过duck会fly吗？一个父类不够，再来一个，搞个多重继承，什么？出现了菱形继承，那干脆在来一个virtual继承得了。</p>
<p>啥叫多态？不就是virtual function，然后父类指针能在运行时根据实际情况调用相应的子类实现。那c++的多态是怎么实现的？看看&lt;&lt;深度探索c++对象模型&gt;&gt;不就行了。</p>
<p>这段时间，可以算是我写c++代码最多的时候，都快写到吐了，尤其还要忍受那龟速的编译。我们竟然都实现了直接通过汇编改c++的虚表，使其调用自己的函数这种变态的东西。在那时候我就得出结论，如果不想早死，尽量别用这个东西写代码。可是到如今我都在不停的慢性自杀。</p>
<h3 id="现代C++阶段">现代C++阶段</h3>
<p>不知道从什么时候开始，我突然觉得我应该来点modern c++的编写方式了，c++0x都出了，还不玩一下就晚了。当然新特性那么多，不可能全部都拿来用的，Bjarne Stroustrup貌似都说过，c++0x应该算是另一门语言了。</p>
<p>于是哥就走上了伪modern c++之路，class还是需要的，不然数据怎么封装。继承吗，比重减轻吧，最好采用面向接口的编程方式。而多态，能不用就不用吧，反而觉得bing + function的方式实现的delegate模型反而更方便，没准也更酷哟。</p>
<p>shared_ptr，weak_ptr是需要用的了，c++没有gc，所以一套好的内存管理方式是必不可少的，每次new之后还要记得delete也比较烦，最严重的是可能忘记那就内存泄露了。</p>
<p>于是，我就自认为我进化了，最典型的例子就是我写的高性能网络库<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>，感觉很modern了。</p>
<h2 id="lua">lua</h2>
<p>最开始知道lua，是云风那本编程感悟的书，当时可是菊花一紧，觉得这东西是啥，为什么能跟c结合到一起使用？</p>
<p>后来自己开发游戏了，才发现lua真的是一门很强大的语言，短小精悍，嵌入简单，性能超强，完全是作为游戏脚本语言开发的不二人选。不过我最开始使用lua的经历不怎么happy，最开始就是接手了一个c++与lua的粘合层库，关于这个库的传说，见这里<a href="http://blog.codingnow.com/2008/08/lua_is_not_c_plus_plus.html" target="_blank" rel="external">Lua 不是 C++</a>。后来，在踩了无数个坑，填了无数个坑之后，我终于弄得相对稳定了。貌似现在我以前的同事还在使用，不过正如我在<a href="http://blog.csdn.net/siddontang/article/details/18727645" target="_blank" rel="external">lua c函数注册器</a>中说明的那样，对于语言的交互，简单一点才好。现在以前做的游戏已经开源，见<a href="https://xp-dev.com/svn/YBTX_Public/" target="_blank" rel="external">这里</a>，那个传说中的蛋疼粘合层也见了世面。当然，我可不会告诉你们好多搓代码是我写的。</p>
<p>后来，在现在的公司，因为项目的需要，我们急需解决python的很多性能大坑问题，于是我开始推广使用openresty，一个用lua包裹的nginx，用了之后，腰不痛了，腿不痛了，性能妥妥的。</p>
<p>因为lua，我第一次尝到了在代码里面直接写配置的便捷，用一个table搞定，相比起来，c++处理ini，json这些的弱爆了。另外，动态语言的热更新机制使其代码升级异常方便，不过你得非常小心lua的闭包，没准你重新加载了代码运行还是老样子。</p>
<p>lua是一个动态语言，所以不用我们管内存释放问题，但是仍然可能会有引用泄露，在开发游戏的时候，为了解决我们程序lua内存泄露的问题，我曾经干过直接从_G递归遍历，扫描整个lua数据的事情。相比在c++使用valgrind这些程序的工具，lua配套的东西还是太小儿科了。</p>
<p>lua的调试也是一个大头问题，我曾今写过几个lua的调试器，例如<a href="https://github.com/siddontang/luahelper" target="_blank" rel="external">这个</a>，甚至都支持了类似gdb那样ctrl+c之后动态的设置断点，可是仍然没觉得比gdb方便，所以多数时候，我都是写log为主。</p>
<h2 id="python">python</h2>
<p>虽然小时候吃过很多蛇，但是蟒蛇可是从来没吃过的，现在看来python味道还不错。</p>
<p>我是来了kingsoft之后才开始正式使用python的。对于为啥使用python，我曾跟拉我进来的技术老大讨论过，他直接就说，开发快速，上手容易。</p>
<p>python开发的快速很大程度是建立在丰富的第三方库上面的，我们也使用了很多库，譬如tornado，gevent，django等，但是正如我最开始说的，因为我们有太多的选择，面对一个问题的时候，往往考虑的是如何选择一个库，而不是自己如何实现，这其实在某种程度上面使得很多童鞋知其然而不知其所以然。这点，lua可能是另一个极端，lua的定位在于嵌入式和高性能，所以自然地，你得自己动手造轮子（当然，现在也有很多好的第三方库了），虽然有时候写起来很不方便，但是至少自己很清楚程序怎么跑的。</p>
<p>当然，我绝对没有贬低python的意思，我很喜欢这门语言，用它来开发了很多东西，同时也知道很多公司使用python构建了很多大型稳定的系统（我们的产品应该也算吧）。</p>
<p>只是现在我越发觉得，看起来简单的语言，如果没有扎实的基本功底，写出来的东西也很烂，而python，恰恰给人放了一个很大的烟雾弹，你以为它有多容易，其实它是在玩你。</p>
<h2 id="go">go</h2>
<p>好了，终于开始说go了，let’s go！！！</p>
<p>我使用go的历史不长，可能也就一年多，但是它现在完全成了我最爱的语言，go具有了python开发的迅速，同时也有着c运行的性能。（当然，还是有差距的！）</p>
<p>网上有太多的语言之争，包括go，有人恨，有人爱。但萝卜白菜，各有所爱，对于我来说，能帮我解决问题，让我用着舒服的语言就是好语言。</p>
<p>go非常适用于服务端程序开发，比起用c++开发，我陡然觉得有一种很幸福的感觉，譬如对于网络编程，在c++里面，我需要自己写epoll的事件处理，而且这种异步的机制完全切分了整个逻辑，使得代码不怎么好写，我在开发<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>的时候感触尤其深刻。但是在go里面，因为天生coroutine的支持，使得异步代码同步化了，非常利于代码的编写。</p>
<p>现在我的主要在项目中推动go的使用，我们已经用go搭建了一个高性能的推送服务器，后续还有几个系统会上线，而且开发的进度并不比使用python差，另外也很稳定，这让我对go的未来充满了期待。</p>
<p>我也用go写了很多的开源程序，也算是拿的出手了，譬如：</p>
<ul>
<li><a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">ledisdb</a>：一个基于leveldb的提供类似redis高级数据结果的高性能NoSQL，真挺绕口的，简单点就是一个高性能NoSQL。</li>
<li><a href="https://github.com/siddontang/mixer" target="_blank" rel="external">Mixer</a>：一个mysql-proxy，现在支持通用的mysql命令代理，读写分离，以及自动主备切换。后续将要参考vitess支持分区，为此一直在恶补编译原理的知识。</li>
<li><a href="https://github.com/siddontang/go-log" target="_blank" rel="external">go-log</a>：一个类似python log模块的东西，支持多种handler，以及不同的log级别。</li>
</ul>
<p>还有一些，可以参考我的<a href="https://github.com/siddontang" target="_blank" rel="external">github</a>，譬如<a href="https://github.com/siddontang/moonmq" target="_blank" rel="external">moonmq</a>（一个高性能push模型的消息服务器），<a href="https://github.com/siddontang/polaris" target="_blank" rel="external">polaris</a>（一个类似tornado的restful web框架），因为go，我开始热衷于开源了，并且认识了很多的好基友，这算得上一个很大的收获吧。</p>
<h2 id="其它">其它</h2>
<p>好了，说完了上面我的长时间使用语言（至少一年以上），我也用了很多其他的语言，现在虽然使用时间比较短，但不排除后续会长久使用。</p>
<h3 id="Objective-C">Objective-C</h3>
<p>因为我家终于有了苹果三件套，所以决定开发app了，首要的任务就是得学会使用Objective-C。我承认这真是一门奇葩的语言，如果没有xcode的自动补齐，我真不知道写起来是神马样的感觉。</p>
<p>而且第一次见到了一门推荐写函数名，变量名就像写文章的语言，至少我写c++，go这些的一个函数名字不会写成一个句子。</p>
<p>我现在在自学iOS的开发，慢慢在整出点东西，毕竟答应给老婆的iphone做点啥的。后续干脆就写一个《小白学iOS》系列blog吧（如果我有精力！），反正对于iOS，我真是一个小白。</p>
<h3 id="java">java</h3>
<p>好吧，我也在android上面写过程序，build到我的S3上面去过，对于java，我的兴趣真不大，貌似自己还买过两本《java编程思想》，那时候脑袋铁定秀逗了。</p>
<p>但是不得不承认，java在服务器领域具有非常强的优势，很多很多的大企业采用java作为其服务器的编程语言。典型的就是淘宝，据传杭州的很多软件公司都不用java的，你用java就等于给淘宝培养人才了。（不过我发现他们很多基础组件譬如TFS这些的可是c++的哟！）</p>
<p>java是门好语言，只是我个人不怎么喜欢，可能我就是太小众了，只对c语言体系的感兴趣。所以很多公司我去不了，哈哈！</p>
<h3 id="erlang">erlang</h3>
<p>受《计算机程序的构造与解释》影像，我一直想学一门函数式编程语言，最开始玩的是elisp，谁叫以前我是个深度的emacser（后来竟然变成了一个vimer，再后来就是sublimer，这世界真神奇）。</p>
<p>后来还是决定好好学习一下erlang，也第一次领略到了函数式编程的魅力。自己唯一用erlang开发过的东西就是bt下载的客户端，不过后来发现用处不大就没继续进行下去了。（好吧，我承认当时想下岛国的东西）</p>
<p>学习erlang之后最大的优势在于现在能看懂很多优秀的erlang项目，譬如我在做<a href="https://github.com/siddontang/moonmq" target="_blank" rel="external">moonmq</a>以及公司的推送服务的时候，研究了rabbitmq，这玩意可是用erlang写的，而我竟然还能看懂，太佩服我了。</p>
<h3 id="还有么？">还有么？</h3>
<p>想想自己还学了哪些语言？貌似没了，不知道awk算不算一门。看来我会得语言真不多。</p>
<h2 id="后续可能会学的">后续可能会学的</h2>
<p>逆水行舟，不进则退，计算机发展这么迅速，我也需要不断提升自己，其中学习一门新的语言可能是一个很好的提升途径，至少能为我打开一扇门。譬如，如果掌握了日文，就能更好的理解岛国片的精髓。我不会日文，所以还是个门外汉。</p>
<h3 id="ruby">ruby</h3>
<p>ruby是一门很优雅的语言，很多大神级别的人物推荐，github貌似也是ruby的幕后推手。</p>
<p>因为ROR的兴起，使得ruby更加流行。看来，一个好的框架库对于语言的推广帮助真挺大的。相比而言，python有django，tornado等，光选择适合自己的就得费点时间。</p>
<p>ruby可以算是一门完全面向对象的语言，连number这种的都是对象，而且看了几本Matz的书，觉得这哥们挺不错的，对技术的感悟很深，所以更让我有兴趣去了解ruby了。</p>
<h3 id="javascript">javascript</h3>
<p>作为一个技术人员，没一个自己的个人网站怎么行，我的阿里云都是包年买的（明年还是买国外的vps吧），自己的个人站点还无影无踪。</p>
<p>好吧，我完全不会javscript，看着css就头疼，没准我从一开始想自己写代码搭建个人站点这个步子就迈的太大，扯着蛋了。如果先用一个开源的框架搭建起来，再自己调整完善，可能会更好。但无论怎样，javascript这门语言还是要学习了解的，尤其是以后随着html5的流行，加之node.js疯狂流行，这门语言也会愈发的发光发热。</p>
<h3 id="C_Sharp">C Sharp</h3>
<p>其实本来不准备跟ms的东西扯上关系的，虽然vs是一个很强大的开发工具，但是我自从换成mac之后就不准备再迁回windows。</p>
<p>只是c#我可能是必须要学会的，因为那个坑爹的unity3d，虽然unity3d也提供了其它语言的支持（譬如伪javascript），但是大量的开发者还是选用了c#，至少在中国我问过很多朋友，都妥妥的用c#，既然这样，我也只能考虑学习使用了。</p>
<p>至于我为啥蛋疼的想玩unity3d，毕竟干了很多年游戏开发，一直有自己弄一个简单小游戏的梦想，还是妥妥的unity3d吧。</p>
<h3 id="自己造一个？">自己造一个？</h3>
<p>语言千千万，我不可能全部学会的，而且以后没准因为业务的需要，没准都会自己造一门语言，也就是DSL。不过这个貌似还离我比较遥远，编译原理的东西太差了（说多了都是泪呀）。自己写词法分析还成，后面就菜了。这也是<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">Mixer</a>一直没啥进展的原因。不过已经买了龙书，在学习屠龙秘籍，希望成为顶尖高手吧。</p>
<h2 id="后记">后记</h2>
<p>写了这么多，看来随着年岁的增加，越来越啰嗦了。不是有一句古话：吾生也有涯，而知也无涯 。以有涯随无涯，殆已。不过不停地追逐不也是乐趣一件？</p>
<p>只是，现在我首先要做的就是向我老婆申请资金升级电脑了吧！</p>
]]></content>
    
    
      <category term="c++" scheme="http://siddontang.com/tags/c++/"/>
    
      <category term="lua" scheme="http://siddontang.com/tags/lua/"/>
    
      <category term="python" scheme="http://siddontang.com/tags/python/"/>
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[高性能NoSQL LedisDB设计2]]></title>
    <link href="http://siddontang.com/2014/06/14/ledisdb-design-2/"/>
    <id>http://siddontang.com/2014/06/14/ledisdb-design-2/</id>
    <published>2014-06-14T02:47:05.000Z</published>
    <updated>2014-08-08T16:09:57.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">ledisdb</a>现在已经支持replication机制，为ledisdb的高可用做出了保障。</p>
<h2 id="使用">使用</h2>
<p>假设master的ip为10.20.187.100，端口6380，slave的ip为10.20.187.101，端口为6380.</p>
<p>首先我们需要master打开binlog支持，在配置文件中指定：</p>
<pre><code>use_bin_log : true
</code></pre><p>在slave的机器上面我们可以通过配置文件指定slaveof开启replication，或者通过命令slaveof显示的开启或者关闭。</p>
<pre><code>slaveof 10.20.187.100 6380
</code></pre><p>ledisdb的replication机制参考了redis以及mysql的相关实现，下面简单说明。</p>
<h2 id="redis_replication">redis replication</h2>
<p>redis的replication机制主要介绍在<a href="http://redis.io/topics/replication" target="_blank" rel="external">这里</a>，已经说明的很详细了。</p>
<ul>
<li>slave向master发送sync命令</li>
<li>master将其当前的数据dump到一个文件，同时在内存中缓存新增的修改命令</li>
<li>当数据dump完成，master就将其发送给slave</li>
<li>slave接受完成dump数据之后，将其本机先前的数据清空，然后在导入dump的数据</li>
<li>master再将先前缓存的命令发送给slave</li>
</ul>
<p>在redis2.8之后，为了防止断线导致重新生成dump，redis增加了psync命令，在断线的时候master会记住当前的同步状态，这样下次就能进行断点续传了。</p>
<h2 id="mysql_replication">mysql replication</h2>
<p>mysql的replication主要是通过binlog的同步来完成的。在master的任何数据更新，都会写入binlog，至于binlog的格式这里不再累述。</p>
<p>假设binlog的basename为mysql，index文件名字为mysql-bin.index，该文件记录着当前所有的binlog文件。</p>
<p>binlog有max file size的配置，当binlog写入的的文件大小超过了该值，mysql就会生成一个新的binlog文件。当mysql服务重启的时候，也会生成一个新的binlog文件。</p>
<p>在Percona的mysql版本中，binlog还有一个max file num的设置，当binlog的文件数量超过了该值，mysql就会删除最早的binlog。</p>
<p>slave有一个master.info的文件，用以记录当前同步master的binlog的信息，主要就是当前同步的binlog文件名以及数据偏移位置，这样下次重新同步的时候就能从该位置继续进行。</p>
<p>slave同步的数据会写入relay log中，同时在后台有另一个线程将relay log的数据存入mysql。</p>
<p>因为master的binlog可能删除，slave同步的时候可能会出现binlog丢失的情况，mysql通过<a href="http://dev.mysql.com/doc/refman/5.0/en/backup-policy.html" target="_blank" rel="external">dump+binlog</a>的方式解决，其实也就是slave完全的dump master数据，在生成的dump中也同时会记录当前的binlog信息，便于下次继续同步。</p>
<h2 id="ledisdb_replication">ledisdb replication</h2>
<p>ledisdb的replication机制参考了redis以及mysql，支持fullsync以及增量sync。</p>
<p>master没有采用aof机制，而是使用了binlog，通过指定max file size以及max file num用来控制binlog的总体大小，这样我就无需关心aof文件持续增大需要重新rewrite的过程了。</p>
<p>binlog文件名格式如下：</p>
<pre><code>ledis-bin.0000001
ledis-bin.0000002
</code></pre><p>binlog文件名的后缀采用数字递增，后续我们使用index来表示。</p>
<p>slave端也有一个master.info文件，因为ledisdb会严格的保证binlog文件后缀的递增，所以我们只需要记录当前同步的binlog文件后缀的index即可。</p>
<p>整个replication流程如下：</p>
<ul>
<li>当首次同步或者记录的binlog信息因为master端binlog删除导致不一致的时候，slave会发送fullsync进行全同步。</li>
<li>master收到fullsync信息之后，会将当前的数据以及binlog信息dump到文件，并将其发送给slave。</li>
<li>slave接受完成整个dump文件之后，清空所有数据，同时将dump的数据导入leveldb，并保存当前dump的binlog信息。</li>
<li><p>slave通过sync命令进行增量同步，sync命令格式如下：</p>
<pre><code>  sync binlog-index binlog-pos
</code></pre><p> master通过index定位到指定的binlog文件，并seek至pos位置，将其后面的binlog数据发送给slave。</p>
</li>
<li>slave接收到binlog数据，导入leveldb，如果sync没有收到任何新增数据，1s之后再次sync。</li>
</ul>
<p>对于最后一点，最主要就是一个问题，即master新增的binlog如何让slave进行同步。对于这点无非就是两种模型，push和pull。</p>
<p>对于push来说，任何新增的数据都能非常及时的通知slave去获取，而pull模型为了性能考虑，不可能太过于频繁的去轮询，略有延时。</p>
<p>mysql采用的是push + pull的模式，当binlog有更新的时候，仅仅通知slave有了更新，slave则是通过pull拉取实际的数据。但是为了支持push，master必须得维持slave的一些状态信息，这稍微又增加了一点复杂度。</p>
<p>ledisdb采用了非常简单的一种方式，定时pull，使用1s的间隔，这样既不会因为轮询太过频繁导致性能开销增大，同时也能最大限度的减少当机数据丢失的风险。</p>
<h2 id="总结">总结</h2>
<p>ledisdb的replication机制才刚刚完成，后续还有很多需要完善，但足以使其成为一个高可用的nosql选择了。</p>
<p>ledisdb的网址在这里<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">https://github.com/siddontang/ledisdb</a>，希望感兴趣的童鞋共同参与。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="nosql" scheme="http://siddontang.com/tags/nosql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[最近几个bug记录]]></title>
    <link href="http://siddontang.com/2014/06/04/record-bugs/"/>
    <id>http://siddontang.com/2014/06/04/record-bugs/</id>
    <published>2014-06-04T08:56:27.000Z</published>
    <updated>2014-08-08T16:20:13.000Z</updated>
    <content type="html"><![CDATA[<p>记录最近出的几个bug</p>
<h2 id="connection_reset_by_peer">connection reset by peer</h2>
<p>最近服务器经常性的出现connection reset by peer的错误，开始我们只是以为小概率的网络断开导致的，可是随着压力的增大，每隔2分钟开始出现一次，这就不得不引起我们的重视了。</p>
<p>我们的业务很简单，lvs负责负载均衡（采用的是DR模式），keepalive timeout设置的为2分钟，后面支撑两台推送服务（后面叫做push），客户端首先通过lvs路由到某台push之后，频繁的向其发送推送消息。</p>
<p>客户端使用的是python request（底层基于urllib3），首先我很差异出了这样的错误竟然没有重试，因为写代码的童鞋告诉我会有重试机制的。于是翻了一下request的代码，竟然发现默认的重试是0，一下子碉堡了。</p>
<p>不过，即使改了重试，仍然没有解决reset by peer的问题。通常出现这种情况，很大的原因在于客户端使用的是keep alive长连接保活tcp，但是服务器端关闭了该连接。可是我们的服务器实现了定时ping的保活机制，应该也不会出现的。</p>
<p>然后我将目光投向了lvs，因为它的timeout设置的为2分钟，而reset by peer这个错误也是两分钟一个，所以很有可能就是我们的定时ping机制不起作用，导致lvs直接close掉了连接。</p>
<p>于是查看push自己的代码，陡然发现我们自己设置的定时ping的时间是3分钟，顿时无语了，于是立刻改成1分钟，重启push，世界清静了。</p>
<h2 id="ifconfig_overruns">ifconfig overruns</h2>
<p>push换上新的机器之后，（性能妥妥的强悍），我们竟然发现推送的丢包率竟然上升了，一下子碉堡了，觉得这事情真不应该发生的。通常这种情况发生在cpu处理网络中断响应不过来。但是我们可是妥妥的24核cpu，并且开启了irqbalance。</p>
<p>好不，用cat /proc/interrupts之后，发现所有的网卡中断都被cpu0处理了，irqbalance完全没有起作用。google之后发现，有些网卡在PCI-MSI模式下面irqbalance无效，而我们的网卡恰好是PCI-MSI模式的。</p>
<p>没办法，关停irqbalance，手动设置网卡中断的SMP_AFFINITY，一下子世界清静了。</p>
<h2 id="总结">总结</h2>
<p>可以发现，最近出的几次蛋疼的事情都是在运维层面上面出现的，实际测试也测不出来，碰到这样的问题，只能通过log这些的慢慢摸索排查了。当然也给了我一个教训，任何error级别的log都应该重视，不应该想当然的忽略。</p>
]]></content>
    
    
  </entry>
  
  <entry>
    <title><![CDATA[高性能NoSQL LedisDB设计1]]></title>
    <link href="http://siddontang.com/2014/05/30/ledisdb-design-1/"/>
    <id>http://siddontang.com/2014/05/30/ledisdb-design-1/</id>
    <published>2014-05-30T04:28:39.000Z</published>
    <updated>2014-08-08T16:09:38.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">ledisdb</a>是一个用go实现的基于leveldb的高性能nosql数据库，它提供多种数据结构的支持，网络交互协议参考redis，你可以很方便的将其作为redis的替代品，用来存储大于内存容量的数据（当然你的硬盘得足够大！）。</p>
<p>同时ledisdb也提供了丰富的api，你可以在你的go项目中方便嵌入，作为你app的主要数据存储方案。</p>
<h2 id="与redis的区别">与redis的区别</h2>
<p>ledisdb提供了类似redis的几种数据结构，包括kv，hash，list以及zset，（set因为我们用的太少现在不予支持，后续可以考虑加入），但是因为其基于leveldb，考虑到操作硬盘的时间消耗铁定大于内存，所以在一些接口上面会跟redis不同。</p>
<p>最大的不同在于ledisdb对于在redis里面可以操作不同数据类型的命令，譬如（del，expire），是只支持kv操作的。也就是说，对于del命令，ledisdb只支持删除kv，如果你需要删除一个hash，你得使用ledisdb额外提供的hclear命令。</p>
<p>为什么要这么设计，主要是性能考量。leveldb是一个高效的kv数据库，只支持kv操作，所以为了模拟redis中高级的数据结构，我们需要在存储kv数据的时候在key前面加入相关数据结构flag。</p>
<p>譬如对于kv结构的key来说，我们按照如下方式生成leveldb的key：</p>
<pre><code>func (db *DB) encodeKVKey(key []byte) []byte {
    ek := make([]byte, len(key)+2)
    ek[0] = db.index
    ek[1] = kvType
    copy(ek[2:], key)
    return ek
}
</code></pre><p>kvType就是kv的flag，至于第一个字节的index，后面我们在讨论。   </p>
<p>如果我们需要支持del删除任意类型，可能的一个做法就是在另一个地方存储该key对应的实际类型，然后del的时候根据查出来的类型再去做相应处理。这不光损失了效率，也提高了复杂度。</p>
<p>另外，在使用ledisdb的时候还需要明确知道，它只是提供了一些类似redis接口，并不是redis，如果想用redis的全部功能，这个就有点无能为力了。</p>
<h2 id="db_select">db select</h2>
<p>redis支持select的操作，你可以根据你的业务选择不同的db进行数据的存放。本来ledisdb只打算支持一个db，但是经过再三考虑，我们决定也实现select的功能。</p>
<p>因为在实际场景中，我们不可能使用太多的db，所以select db的index默认范围就是[0-15]，也就是我们最多只支持16个db。redis默认也是16个，但是你可以配置更多。不过我们觉得16个完全够用了，到现在为止，我们的业务也仅仅使用了3个db。</p>
<p>要实现多个db，我们开始定了两种方案：</p>
<ul>
<li>一个db使用一个leveldb，也就是最多ledisdb将打开16个leveldb实例。</li>
<li>只使用一个leveldb，每个key的第一个字节用来标示该db的索引。</li>
</ul>
<p>这两种方案我们也不知道如何取舍，最后决定采用使用同一个leveldb的方式。可能我们觉得一个leveldb可以更好的进行优化处理吧。</p>
<p>所以我们任何leveldb key的生成第一个字节都是存放的该db的index信息。</p>
<h2 id="KV">KV</h2>
<p>kv是最常用的数据结构，因为leveldb本来就是一个kv数据库，所以对于kv类型我们可以很简单的处理。额外的工作就是生成leveldb对应的key，也就是前面提到的encodeKVKey的实现。</p>
<h2 id="Hash">Hash</h2>
<p>hash可以算是一种两级kv，首先通过key找到一个hash对象，然后再通过field找到或者设置相应的值。</p>
<p>在ledisdb里面，我们需要将key跟field关联成一个key，用来存放或者获取对应的值，也就是key:field这种格式。</p>
<p>这样我们就将两级的kv获取转换成了一次kv操作。</p>
<p>另外，对于hash来说，（后面的list以及zset也一样），我们需要快速的知道它的size，所以我们需要在leveldb里面用另一个key来实时的记录该hash的size。</p>
<p>hash还必须提供keys，values等遍历操作，因为leveldb里面的key默认是按照内存字节升序进行排列的，所以我们只需要找到该hash在leveldb里面的最小key以及最大key，就可以轻松的遍历出来。</p>
<p>在前面我们看到，我们采用的是key:field的方式来存入leveldb的，那么对于该hash来说，它的最小key就是<strong>“key:”</strong>，而最大key则是<strong>“key;”</strong>，所以该hash的field一定在<strong>“(key:, key;)”</strong>这个区间范围。至于为什么是<strong>“;”</strong>，因为它比<strong>“:”</strong>大1。所以<strong>“key:field”</strong>一定小于<strong>“key;”</strong>。后续zset的遍历也采用的是该种方式，就不在说明了。</p>
<h2 id="List">List</h2>
<p>list只支持从两端push，pop数据，而不支持中间的insert，这样主要是为了简单。我们使用key:sequence的方式来存放list实际的值。</p>
<p>sequence是一个int整形，相关常量定义如下：</p>
<pre><code>listMinSeq     int32 = 1000
listMaxSeq     int32 = 1&lt;&lt;31 - 1000
listInitialSeq int32 = listMinSeq + (listMaxSeq-listMinSeq)/2
</code></pre><p>也就是说，一个list最多存放1&lt;&lt;31 - 2000条数据，至于为啥是1000，我说随便定得你信不？</p>
<p>对于一个list来说，我们会记录head seq以及tail seq，用来获取当前list开头和结尾的数据。</p>
<p>当第一次push一个list的时候，我们将head seq以及tail seq都设置为listInitialSeq。</p>
<p>当lpush一个value的时候，我们会获取当前的head seq，然后将其减1，新得到的head seq存放对应的value。而对于rpush，则是tail seq + 1。</p>
<p>当lpop的时候，我们会获取当前的head seq，然后将其加1，同时删除以前head seq对应的值。而对于rpop，则是tail seq - 1。</p>
<p>我们在list里面一个meta key来存放该list对应的head seq，tail seq以及size信息。</p>
<h2 id="ZSet">ZSet</h2>
<p>zset可以算是最为复杂的，我们需要使用三套key来实现。</p>
<ul>
<li>需要用一个key来存储zset的size</li>
<li>需要用一个key:member来存储对应的score</li>
<li>需要用一个key:score:member来实现按照score的排序</li>
</ul>
<p>这里重点说一下score，在redis里面，score是一个double类型的，但是我们决定在ledisdb里面只使用int64类型，原因一是double还是有浮点精度问题，在不同机器上面可能会有误差（没准是我想多了），另一个则是我不确定double的8字节memcmp是不是也跟实际比较结果一样（没准也是我想多了），其实更可能的原因在于我们觉得int64就够用了，实际上我们项目也只使用了int的score。</p>
<p>因为score是int64的，我们需要将其转成大端序存储（好吧，我假设大家都是小端序的机器），这样通过memcmp比较才会有正确的结果。同时int64有正负的区别，负数最高位为1，所以如果只是单纯的进行binary比较，那么负数一定比正数大，这个我们通过在构建key的时候负数前面加”&lt;”，而正数（包括0）加”=”来解决。所以我们score这套key的格式就是这样：</p>
<pre><code>key&lt;score:member //&lt;0
key=score:member //&gt;=0
</code></pre><p>对于zset的range处理，其实就是确定某一个区间之后通过leveldb iterator进行遍历获取，这里我们需要明确知道的事情是leveldb的iterator正向遍历的速度和逆向遍历的速度完全不在一个数量级上面，正向遍历快太多了，所以最好别去使用zset里面带有rev前缀的函数。</p>
<h2 id="总结">总结</h2>
<p>总的来说，用leveldb来实现redis那些高级的数据结构还算是比较简单的，同时根据我们的压力测试，发现性能还能接受，除了zset的rev相关函数，其余的都能够跟redis保持在同一个数量级上面，具体可以参考ledisdb里面的性能测试报告以及运行ledis-benchmark自己测试。</p>
<p>后续ledisdb还会持续进行性能优化，同时提供expire以及replication功能的支持，预计6月份我们就会实现。</p>
<p>ledisdb的代码在这里<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">https://github.com/siddontang/ledisdb</a>，希望感兴趣的童鞋共同参与。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="nosql" scheme="http://siddontang.com/tags/nosql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[LedisDB嵌入使用介绍]]></title>
    <link href="http://siddontang.com/2014/05/20/ledisdb-embed-introduction/"/>
    <id>http://siddontang.com/2014/05/20/ledisdb-embed-introduction/</id>
    <published>2014-05-20T00:44:58.000Z</published>
    <updated>2014-08-08T16:10:20.000Z</updated>
    <content type="html"><![CDATA[<p>ledisdb现在可以支持嵌入式使用。你可以将其作为一个独立的lib（类似leveldb）直接嵌入到你自己的应用中去，而无需在启动单独的服务。</p>
<p>ledisdb提供的API仍然类似redis接口。首先，你需要创建ledis对象：</p>
<pre><code>import &quot;github.com/siddontang/ledisdb/ledis&quot;

configJson = []byte(&#39;{
    &quot;data_db&quot; : 
    {
        &quot;path&quot; : &quot;/tmp/testdb&quot;,
        &quot;compression&quot;:true,
        &quot;block_size&quot; : 32768,
        &quot;write_buffer_size&quot; : 2097152,
        &quot;cache_size&quot; : 20971520
    }    
}
&#39;)

l, _ := ledis.Open(configJson)
</code></pre><p>data_db就是数据存储的leveldb位置，简单起见，所有的size配置全部使用byte作为单位。</p>
<p>然后我们选择一个db使用，</p>
<pre><code>db, _ := l.Select(0)
</code></pre><p>类似redis，我们也只支持数字类型的db，最多16个db，索引范围为[0-15]。支持太多的db真没啥意义。</p>
<p>下面是一些简单的例子：</p>
<h2 id="kv">kv</h2>
<pre><code>db.Set(key, value)
db.Get(key)
db.SetNX(key, value)
db.Incr(key)
db.IncrBy(key, 10)
db.Decr(key)
db.DecrBy(key, 10)

db.MSet(KVPair{key1, value1}, KVPair{key2, value2})
db.MGet(key1, key2)
</code></pre><h2 id="list">list</h2>
<pre><code>db.LPush(key, value1, value2, value3)
db.RPush(key, value4, value5, value6)

db.LRange(key, 1, 10)
db.LIndex(key, 10)

db.LLen(key)
</code></pre><h2 id="hash">hash</h2>
<pre><code>db.HSet(key, field1, value1)
db.HMSet(key, FVPair{field1, value1}, FVPair{field2, value2})

db.HGet(key, field1)

db.HGetAll()
db.HKeys()
</code></pre><h2 id="zset">zset</h2>
<pre><code>db.ZAdd(key, ScorePair{score1, member1}, ScorePair{score2, member2})

db.ZCard(key)

//range by score [0, 100], withscores = true and no limit
db.ZRangeByScore(key, 0, 100, true, 0, -1)

//range by score [0, 100], withscores = true and limit offset = 10, count = 10
db.ZRangeByScore(key, 0, 100, true, 10, 10)

db.ZRank(key, member1)

db.ZCount(key, member1)
</code></pre><p>ledisdb的源码在这里<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">https://github.com/siddontang/ledisdb</a>，欢迎反馈。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="nosql" scheme="http://siddontang.com/tags/nosql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
</feed>
