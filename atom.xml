<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Siddon's Blog]]></title>
  <subtitle><![CDATA[My thought for program]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://siddontang.com/"/>
  <updated>2015-08-03T05:40:38.180Z</updated>
  <id>http://siddontang.com/</id>
  
  <author>
    <name><![CDATA[SiddonTang]]></name>
    <email><![CDATA[siddontang@gmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[SmartStack Airbnb的自动服务发现和注册框架]]></title>
    <link href="http://siddontang.com/2015/08/03/smartstack-introduction/"/>
    <id>http://siddontang.com/2015/08/03/smartstack-introduction/</id>
    <published>2015-08-03T05:38:36.000Z</published>
    <updated>2015-08-03T05:40:25.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Micro-service引发的问题">Micro-service引发的问题</h2>
<p>在几年前做企业快盘的时候，为了构建一个高可用的分布式系统，我们采用了一个模块一个服务，不同服务之间通过HTTP交互的架构模型，除了数据存储服务（MySQL，Redis等），我们其他的服务都是无状态的，这样就能非常方便的进行水平扩展，满足不断增加的业务需求。现在才知道，这种架构模式就是俗称的micro-service，如果当初我们就能拿这些概念出去忽悠，没准能让自己的产品更加高大上一点的。:-)</p>
<p>micro-service虽然方便，毕竟各个模块是相互独立，我们可以独立开发，独立部署，只要约定好相互之间的HTTP Restful API就成了。但是，随着服务的增多，我们会面临一个问题，就是某一个服务到底在哪里？我们如何才能发现该服务并进行调用。</p>
<p>在项目初期，服务数量不多的情况下面，我们可以将所有服务的地址写到一个配置文件里面（或者更改hosts），部署升级的时候通过puppet或者其它工具进行整个系统的更新，但这样，就会面临一个问题，任何增加删除服务的操作，都可能引起整个系统的更新。随着系统规模的扩大，服务数量的增多，谁都知道不可行了。</p>
<p>Airbnb的工程师一定也碰到了类似了问题，否则他们不会开发了SmartStack。关于SmartStack的详细介绍，可以参考这篇文章<a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud/" target="_blank" rel="external">SmartStack: Service Discovery in the Cloud</a>，国内的oschina已经有相关翻译<a href="http://www.oschina.net/translate/smartstack-service-discovery-cloud" target="_blank" rel="external">SmartStack 介绍 —— 云端的服务发现</a>，所以我就不用考虑再将这篇文章重复翻译一遍了。下面我只是想说说我的一些理解。</p>
<h2 id="不怎么好的解决方案">不怎么好的解决方案</h2>
<p>Airbnb的这篇文章同时列出了一些不怎么好的解决方案，个人觉得对我们设计分布式系统也是很有借鉴意义的。</p>
<h3 id="DNS">DNS</h3>
<p>DNS应该是一个非常简单地解决方案了，一个服务配置一个Domain，通过DNS解析，客户端对某个服务的请求就能被路由到某一台具体的机器上面处理。</p>
<p>虽然很简单，但是DNS也有很严重的问题，首先就是延时，如果我们更新了DNS，我们无法保证一些客户端能及时的收到DNS变更的消息。同时，在机器上面，DNS通常都有缓存，所以更加增大了变更DNS的延时。</p>
<p>另外，DNS是随机路由的，我们不能自定义自己的路由算法，所以很有可能我们会面临一个服务里面，一些机器繁忙的在处理请求，而另一些机器则几乎被闲置。</p>
<p>还有一个很严重的问题，一些应用程序譬如Nginx在程序启动的时候会缓存DNS的结果，也就是如果不做任何特殊处理，DNS的任何变更对当前运行中的Nginx是完全无效的。</p>
<p>综上，如果服务的节点动态变更比较频繁，使用DNS来进行服务发现并不是一个很好的解决方案。但对于一些节点长时间不会变动的服务，譬如Zookeeper，使用DNS则是一个比较好的方式。</p>
<h3 id="中心化的负载均衡">中心化的负载均衡</h3>
<p>我们可以使用一个中心化的负载均衡器来进行服务的路由。所有的路由信息都存储到这个负载均衡器里面。但我们如何发现这个负载均衡器，通常的做法就是DNS，不过这又会出现上面DNS的问题。</p>
<p>同时，因为这个负载均衡器是一个中心化的节点，必然面临单点性能问题，而且如果负载均衡器当掉了，我们就会面临整个系统不可用的问题了（这时候完全不知道其他服务在哪里了）。</p>
<p>因为负载均衡器是一个单点，所以我们需要考虑将其做HA处理，另外，我们还需要考虑性能问题。如果有钱，考虑购入F5，不过这种硬件路由方案真心很贵。LVS, Nginx或者HAProxy也是一个不错的选择。如果在AWS上面，可以考虑使用ELB，但只能针对外网IP。</p>
<p><strong>其实中心化的负载均衡方案，后续可以演化为无状态proxy方案，我们通过zookeeper或者其他coordinator来存储整个集群的路由信息，并通过2PC的方式同步更新到所有proxy上面，因为proxy是无状态，所以非常方便的进行水平扩展。当然proxy的发现也是一个问题，我们可以使用DNS，也可以使用LVS。</strong></p>
<h3 id="服务自己内部注册并发现">服务自己内部注册并发现</h3>
<p>其实依赖zookeeper等协调服务来处理的。服务会自己注册到zookeeper上面，其他的服务节点通过zookeeper的watch方式实时的感知到整个集群的变化。</p>
<p>当然，使用这种方式也是有限制的。如果我们使用同一种语言进行开发，譬如java，那么每个服务嵌入一个zookeeper的client library是很容易的，但如果用了不同的语言，譬如有go，node.js，等等，让所有服务都能很好的跟zookeeper进行交互就比较困难了。</p>
<p>同时，如果我们希望一些其他第三方应用（譬如Nginx）也能享受到zookeeper的好处，这就比较麻烦了，因为这些服务压根不支持zookeeper。</p>
<p>另外，即使zookeeper存储了整个路由信息，我们仍然没有很好的办法定制路由算法，即使连最简单地round-robin都没法很好的支持，譬如一个客户端将第一个请求发给了第一个节点，如何将第二个请求发给第二个节点？</p>
<p>总之，单纯地使用zookeeper是不可能的，但正如我在前面说的，<strong>我们可以引入一个无状态的proxy，由proxy负责跟zookeeper打交道。</strong></p>
<h2 id="SmartStack:_Nerve_and_Synapse">SmartStack: Nerve and Synapse</h2>
<p>Airbnb列出了通用的三种不可取的做法，我想他们应该是都尝试过并且踩了坑，所以才有了后续的SmartStack。:-）</p>
<p>SmartStack由两个模块构成，nerve和synapse，当然还依赖zookeeper和haproxy。</p>
<p>部署的时候，service跟nerve一起部署，client则跟synapse以及local haproxy一起部署。</p>
<p>nerve负责管控其对应的service，并通过ephemeral的方式挂载到zookeeper上面，这样如果nerve所在的机器当掉，或者nerve负责的service挂掉了（nerve会每隔一段时间进行service存活检测），nerve与zookeeper的连接就会断开，外部就能感知节点的动态变更了。</p>
<p>synapse负责watch zookeeper的变更，当获取到节点变更事件之后，将最新的路由信息更新到本机local haproxy上面，客户端要访问对应的service，都是通过local haproxy路由到相应节点进行访问。</p>
<p>可以看到，SmartStack是一个非常简单地实现方式，但是它很好的解决了分布式系统中服务的发现与注册问题。SmartStack通过nerve进行服务的注册以及注销，通过synapse + local haproxy的方式进行服务的发现。如果一个client要访问对应的服务，只能通过local haproxy，这里local haproxy有点类似于中心化的负载均衡器，但是它仅仅限于本机，所以不存在DNS以及单点等问题。</p>
<h2 id="SmartStack的不足">SmartStack的不足</h2>
<p>SmartStack的实现是很巧妙的，并且非常的简单，但是它也有一些自身的问题，最主要的就是友好的可用性问题。这套系统依赖了zookeeper，需要额外部署nerve，synapse以及local haproxy，运维上面略显复杂，相比较而言，Consul（Hashicorp的分布式服务发现程序）就只有一个二进制文件，部署更加方便。（后续如果有时间，可以写写SmartStack vs Consul）</p>
<p>为什么我特别关注运维，主要在于现在我们开发的一个分布式kv：RebornDB也存在同样的问题，运维步骤略显繁琐，导致很多时候我们开发自己都烦如何很好的构建一个可运行环境。</p>
<p>不过，这年头，因为有了docker以及mesos，kubernate等技术，没准运维的复杂度会降低很多。</p>
<h2 id="总结">总结</h2>
<p>总的来说，SmartStack是一个很好的服务发现解决方案，原理非常简单，但功能却很强大。不过，如果我们将local haproxy往上提升变成一个haproxy cluster，这不就跟我前面说的无状态proxy差不多呢？没准后续也可以用go整一个出来，毕竟SmartStack的ruby代码看得我挺头大的。 </p>
]]></content>
    
    
  </entry>
  
  <entry>
    <title><![CDATA[浮点数字节序比较]]></title>
    <link href="http://siddontang.com/2015/08/03/double-lexicographic-comparison/"/>
    <id>http://siddontang.com/2015/08/03/double-lexicographic-comparison/</id>
    <published>2015-08-03T05:37:34.000Z</published>
    <updated>2015-08-03T05:38:20.000Z</updated>
    <content type="html"><![CDATA[<p>在开发LedisDB的时候，我曾考虑将zset的score使用跟redis一样的double类型，但是却没想好如何将double在底层LevelDB或者RocksDB下存储，使其能够支持zset中zrangebyscore等命令，所以只能考虑使用int64类型来代替。但在开发qdb的时候，最开始我们仍然只是支持int64，但最终通过努力，支持了double，使其能跟redis的zset api完全兼容。其实后来发现，实现很简单。</p>
<h2 id="LevelDB和ZSet">LevelDB和ZSet</h2>
<p>这里需要简单说明一下LevelDB（RocksDB只是它的一个衍生优化版本，但最核心基本的原理还是一样的）。</p>
<p>LevelDB是一个高性能的KV存储库，数据在LevelDB里面是根据key来进行排序存储的，而key和value是任意的字节数组。在外部看来，LevelDB里面的数据就是一个有序map，map里面的key是顺序存储的，如下：</p>
<pre><code>{
    &quot;key1&quot; : &quot;value1&quot;,
    &quot;key2&quot; : &quot;value2&quot;,
    ...
}
</code></pre><p>在redis里面，zset是一个有序集合，每一个member都对应一个score，member是唯一的，score则可能重复。我们可以通过score进行很多处理，譬如获取<code>[score1, score2]</code>这个区间的所有元素，或者获取某个member对应的score再整个zset里面的rank。为了实现上面的需求，我们需要在内部用一个有序的数据结构来存储score以及其对应的member，redis使用skip list来进行处理，那如何将这样的映射关系在LevelDB里面很好的处理呢？</p>
<p>因为LevelDB将数据是按照key顺序存储的，所以我们只需要将score的信息绑定到key上面，就能很容易的实现一个简易的skip list。对于一个zset里面的member，可能在LevelDB里面实际的key结构如下：</p>
<p><code>key:score:member</code> </p>
<p>对于一个zset来说，key和member都是arbitrary bytes array，可以很方便的绑定到LevelDB的key上面，但是score可是double的，如何绑定上去参与排序呢？</p>
<h2 id="LedisDB_ZSet_int64_score">LedisDB ZSet int64 score</h2>
<p>再说double之前，先来聊聊LedisDB的做法，LedisDB使用的score是int64，对于一个int64来说，计算机内部是使用8 bytes进行存储的，下面是例子，会打印不同int64数据在小端序和大端序下8字节array十六进制表现：</p>
<pre><code class="go">import &quot;fmt&quot;
import &quot;math&quot;
import &quot;encoding/binary&quot;

func p(a int64) {
    b1 := make([]byte, 8)
    b2 := make([]byte, 8)

    binary.LittleEndian.PutUint64(b1, uint64(a))
    binary.BigEndian.PutUint64(b2, uint64(a))

    fmt.Printf(&quot;%0x\t%0x\n&quot;, b1, b2)
}

func main() {
    p(0)
    p(1)
    p(0xFFFF0000)
    p(math.MaxInt64)

    p(-1)
    p(-200)
    p(-300)
    p(math.MinInt64)
}
</code></pre>
<p>输出结果如下：</p>
<pre><code>0000000000000000    0000000000000000

0100000000000000    0000000000000001
0000ffff00000000    00000000ffff0000
ffffffffffffff7f    7fffffffffffffff

ffffffffffffffff    ffffffffffffffff
38ffffffffffffff    ffffffffffffff38
d4feffffffffffff    fffffffffffffed4
0000000000000080    8000000000000000
</code></pre><p>第一列是小端序打印的结果，第二列是大端序打印的结果。首先我们可以看到，对于0来说，无论什么端序，都是全0，对于正整数来说，我们可以看到，0xFFFF0000是铁定大于1的，但是在对应的小端序存储的bytes array里面，会发现如果按照字节序进行排序0100000000000000是大于0000ffff00000000的，而大端序的结果0000000000000001是小于00000000ffff0000的。对于负整数也是一样，-200是大于-300的，但是小端序的结果是小于，而大端序的结果是大于。</p>
<p>所以我们可以知道，对于一个整数，我们需要使用大端序的方式将其绑定到LevelDB的key上面，这样才能参与正常的排序。但是我们又发现，如果直接这样处理，负数的大端序结果是铁定大于正数的，譬如-1的ffffffffffffffff就大于1的0000000000000001，所以为了正常的处理整数的排序，我们只需要在key上面加一个前缀标志就可以了。LedisDB里面做了如下处理：</p>
<pre><code>key&lt;38ffffffffffffff:member
key&lt;ffffffffffffffff:member
key=0000000000000000:member
key=0000000000000001:member
</code></pre><p>因为<code>=</code>的ascii值大于<code>&lt;</code>，我们将正整数和0前面加上<code>=</code>，将负数前面加上<code>&lt;</code>，这样在进行字节序排序的时候，正数和0一定能排在负数的后面，也就是完全能满足从小到大排序的需求了。</p>
<h2 id="QDB_ZSet_double_score">QDB ZSet double score</h2>
<p>上面可以知道，我们使用大端序加上一个前缀标志，就能很好的处理int64类型的score的排序，但double可没有这么简单，不然LedisDB早就支持了。</p>
<p>首先我们来看看double类型IEEE754规范，对于一个double来说，使用8 bytes存储，格式如下：</p>
<ul>
<li>符号: 1 bit （0为正，1为负）</li>
<li>指数: 11 bits</li>
<li>分数: 52 bits</li>
</ul>
<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/IEEE_754_Double_Floating_Point_Format.svg/1236px-IEEE_754_Double_Floating_Point_Format.svg.png" alt="double format"></p>
<p>对于一个64 bits的double，如果指数为e，那么它对应的值计算方式为:</p>
<p><img src="http://upload.wikimedia.org/math/9/7/c/97c7aee4ad33a9763c30f49628648779.png" alt=""></p>
<p>如果指数越大，那么double的值越大，如果分数越大，在相同指数的情况下面double值也越大。所以我们如果将double使用大端序存放到8字节array里面，我们就能直接进行字节序比较，但这个仅限于正的double情况下。因为对于负的double，它除了最高位bit为1这一点不一样之外，其余完全跟相反的正数底层存储方式一模一样，所以如果我们直接将其转为大端序比较，会发现结果是相反的。一个简单地例子:</p>
<pre><code class="go">func d(a float64) {
    b := make([]byte, 8)

    binary.BigEndian.PutUint64(b, math.Float64bits(a))

    fmt.Printf(&quot;%064b\n&quot;, binary.BigEndian.Uint64(b))
}

func main() {
    d(0)

    d(1)
    d(2)

    d(-1)
    d(-2)
}
</code></pre>
<p>结果如下：</p>
<pre><code>0000000000000000000000000000000000000000000000000000000000000000
0011111111110000000000000000000000000000000000000000000000000000
0100000000000000000000000000000000000000000000000000000000000000
1011111111110000000000000000000000000000000000000000000000000000
1100000000000000000000000000000000000000000000000000000000000000
</code></pre><p>这里在插入一片广告文章，推荐阅读，<a href="http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm" target="_blank" rel="external">这里</a>。</p>
<p>当初LedisDB就是因为没有办法很好的处理负double的排序比较问题，才采用了int64，但是真的没有办法吗？为啥int64类型的负数能通过大端序进行字节序比较呢？我们知道，在计算机内部，负数是使用补码存储的，也就是反码加1，负数的反码，就是在源码的基础上，符号位不变，其余各个位取反。对于-1来说，它的源码为[10000001]，反码为[11111110]，补码为[11111111]。（为了简单，使用int8表示的）。就因为它有了取反的这一步，我们才能直接用大端序字节序比较。</p>
<p>所以对于double的负数，我们也仅仅需要干一件事情，就是取反，那么它的大端序字节序比较就是正常的了。在QDB里面，对于一个double，我们做了如下处理:</p>
<pre><code class="go">// We can not use lexicographically bytes comparison for negative and positive float directly.
// so here we will do a trick below.
func float64ToUint64(f float64) uint64 {
    u := math.Float64bits(f)
    if f &gt;= 0 {
        u |= 0x8000000000000000
    } else {
        u = ^u
    }
    return u
}

func uint64ToFloat64(u uint64) float64 {
    if u&amp;0x8000000000000000 &gt; 0 {
        u &amp;= ^uint64(0x8000000000000000)
    } else {
        u = ^u
    }
    return math.Float64frombits(u)
}
</code></pre>
<p>如果一个double数据值大于等于0，那么我们将最高位bit设置为1，而对于负数，我们直接取反，就这一步简单的操作，就能让我们完美的支持double的score了。</p>
<h2 id="总结">总结</h2>
<p>让QDB的zset支持double score，其实很简单的一件事情，但我在开发LedisDB的时候竟然没有想到如何去解决。我们很多时候，往往追求高大上的东西，譬如架构，设计模式等，但忽略了计算机最基本的底层知识（上面这些其实也就是大小端序，补码，反码等），所以有时候还真的重新好好把最基本的东西给回顾一下。</p>
<p>最后，在推荐一下QDB，QDB是一个类似Redis的KV数据库，底层使用LevelDB/RocksDB作为存储引擎，解决了Redis内存限制问题，同时能支持string，hash，list，set和zset这几种数据结构，性能卓越，你也可以认为它是LedisDB的升级版本。</p>
<p>网址 <a href="https://github.com/reborndb/qdb" target="_blank" rel="external">https://github.com/reborndb/qdb</a>。 </p>
]]></content>
    
    
  </entry>
  
  <entry>
    <title><![CDATA[我为什么从python转向go]]></title>
    <link href="http://siddontang.com/2015/05/16/why-python-to-go/"/>
    <id>http://siddontang.com/2015/05/16/why-python-to-go/</id>
    <published>2015-05-16T08:13:34.000Z</published>
    <updated>2015-05-19T05:57:43.000Z</updated>
    <content type="html"><![CDATA[<p>应puppet大拿刘宇的邀请，我去西山居运维团队做了一个简短分享，谈谈为什么我要将我们的项目从python转向go。</p>
<p>坦白的讲，在一帮python用户面前讲为什么放弃python转而用go其实是一件压力蛮大的事情，语言之争就跟vim和emacs之争一样，是一个永恒的无解话题，稍微不注意就可能导致粉丝强烈地反击。所以我只会从我们项目实际情况出发，来讲讲为什么我最终选择了go。</p>
<h2 id="为什么放弃python">为什么放弃python</h2>
<p>首先，我其实得说说为什么我们会选择python。在我加入企业快盘团队之前，整个项目包括更早的金山快盘都是采用python进行开发的。至于为什么这么选择，当时的架构师葱头告诉我，主要是因为python上手简单，开发迅速。对于团队里面大部分完全没服务端开发经验的同学来说，python真的是一个很好的选择。</p>
<p>python的简单高效，我是深有体会的。当时私有云项目也就几个程序员，但是我们要服务多家大型企业，进行定制化的开发，多亏了python，我们才能快速出活。后来企业快盘挂掉之后，我们启动轻办公项目，自然也使用python进行了原始版本的构建。</p>
<p>python虽然很强大，但我们在使用的时候也碰到了一些问题，主要由如下几个方面：</p>
<ul>
<li><p>动态语言</p>
<p>  python是一门动态强类型语言。但是，仍然可能出现int + string这样的运行时错误，因为对于一个变量，在写代码的时候，我们有时候很容易就忘记这个变量到底是啥类型的了。</p>
<p>  在python里面，可以允许同名函数的出现，后一个函数会覆盖前一个函数，有一次我们系统一个很严重的错误就是因为这个导致的。</p>
<p>  上面说到的这些，静态语言在编译的时候就能帮我们检测出来，而不需要等到运行时出问题才知道。虽然我们有很完善的测试用例，但总有case遗漏的情况。所以每次出现运行时错误，我心里都想着如果能在编译的时候就发现该多好。</p>
</li>
<li><p>性能</p>
<p>  其实这个一直是很多人吐槽python的地方，但python有它适合干的事情，硬是要用python进行一些高性能模块的开发，那也有点难为它了。</p>
<p>  python的GIL导致无法真正的多线程，大家可能会说我用多进程不就完了。但如果一些计算需要涉及到多进程交互，进程之间的通讯开销也是不得不考虑的。</p>
<p>  无状态的分布式处理使用多进程很方便，譬如处理http请求，我们就是在nginx后面挂载了200多个django server来处理http的,但这么多个进程自然导致整体机器负载偏高。</p>
<p>  但即使我们使用了多个django进程来处理http请求，对于一些超大量请求，python仍然处理不过来。所以我们使用openresty，将高频次的http请求使用lua来实现。可这样又导致使用两种开发语言，而且一些逻辑还得写两份不同的代码。</p>
</li>
<li><p>同步网络模型</p>
<p>  django的网络是同步阻塞的，也就是说，如果我们需要访问外部的一个服务，在等待结果返回这段时间，django不能处理任何其他的逻辑（当然，多线程的除外）。如果访问外部服务需要很长时间，那就意味着我们的整个服务几乎在很长一段时间完全不可用。</p>
<p>  为了解决这个问题，我们只能不断的多开django进程，同时需要保证所有服务都能快速的处理响应，但想想这其实是一件很不靠谱的事情。</p>
</li>
<li><p>异步网络模型</p>
<p>  tornado的网络模型是异步的，这意味着它不会出现django那样因为外部服务不可用导致这个服务无法响应的问题。话说，比起django，我可是非常喜欢tornado的，小巧简单，以前还写过几篇深入剖析tornado的文章了。</p>
<p>  虽然tornado是异步的，但是python的mysql库都不支持异步，这也就意味着如果我们在tornado里面访问数据库，我们仍然可能面临因为数据库问题造成的整个服务不可用。</p>
<p>  其实异步模型最大的问题在于代码逻辑的割裂，因为是事件触发的，所以我们都是通过callback进行相关处理，于是代码里面就经常出现干一件事情，传一个callback，然后callback里面又传callback的情况，这样的结果就是整个代码逻辑非常混乱。</p>
<p>  python没有原生的协程支持，虽然可以通过gevent，greenlet这种的上patch方式来支持协程，但毕竟更改了python源码。另外，python的yield也可以进行简单的协程模拟，但毕竟不能跨堆栈，局限性很大，不知道3.x的版本有没有改进。</p>
</li>
<li><p>开发运维部署</p>
<p>  当我第一次使用python开发项目，我是没成功安装上项目需要的包的，光安装成功mysql库就弄了很久。后来，是一位同事将他整个python目录打包给我用，我才能正常的将项目跑起来。话说，现在有了docker，是多么让人幸福的一件事情。</p>
<p>  而部署python服务的时候，我们需要在服务器上面安装一堆的包，光是这一点就让人很麻烦，虽然可以通过puppet，salt这些自动化工具解决部署问题，但相比而言，静态编译语言只用扔一个二进制文件，可就方便太多了。</p>
</li>
<li><p>代码失控</p>
<p>  python非常灵活简单，写c几十行代码才能搞定的功能，python一行代码没准就能解决。但是太简单，反而导致很多同学无法对代码进行深层次的思考，对整个架构进行细致的考量。来了一个需求，啪啪啪，键盘敲完开速实现，结果就是代码越来越混乱，最终导致了整个项目代码失控。</p>
<p>  虽然这也有我们自身的原因，譬如没好的代码review机制，没有好的项目规范，但个人感觉，如果一个程序员没经过良好的编码训练，用python很容易就写出烂的代码，因为太自由了。</p>
<p>  当然，我这里并不是说用python无法进行大型项目的开发，豆瓣，dropbox都是很好的例子，只是在我们项目中，我们的python代码失控了。</p>
</li>
</ul>
<p>上面提到的都是我们在实际项目中使用python遇到的问题，虽然最终都解决了，但是让我愈发的觉得，随着项目复杂度的增大，流量性能压力的增大，python并不是一个很好的选择。</p>
<h2 id="为什么选择go">为什么选择go</h2>
<p>说完了python，现在来说说为什么我们选择go。其实除了python，我们也有其他的选择，java，php，lua(openresty)，但最终我们选择了go。</p>
<p>虽然java和php都是最好的编程语言（大家都这么争的），但我更倾向一门更简单的语言。而openresty，虽然性能强悍，但lua仍然是动态语言，也会碰到前面说的动态语言一些问题。最后，前金山许式伟用的go，前快盘架构师葱头也用的go，所以我们很自然地选择了go。</p>
<p>go并不是完美，一堆值得我们吐槽的地方。</p>
<ul>
<li><p>error，好吧，如果有语言洁癖的同学可能真的受不了go的语法，尤其是约定的最后一个返回值是error。项目里面经常会充斥这样的代码:</p>
<pre><code class="go">  if _, err := w.Write(data1); err != nil {
      returun err
  }
  if _, err := w.Write(data2); err != nil {
      returun err
  }
</code></pre>
<p>  难怪有个梗是对于一个需求，java的程序员在写配置的时候，go程序员已经写了大部分代码，但是当java的程序员写完的时候，go程序员还在写<code>err != nil</code>。</p>
<p>  这方面，<a href="https://blog.golang.org/errors-are-values" target="_blank" rel="external">errors-are-values</a>倒是推荐了一个不错的解决方案。</p>
</li>
<li><p>包管理，go的包管理太弱了，只有一个go get，也就是如果不小心更新了一个外部库，很有可能就导致现有的代码编译不过了。虽然已经有很多开源方案，譬如godep以及现在才出来的gb等，但毕竟不是官方的。貌似google也是通过vendor机制来管理第三方库的。希望go 1.5或者之后的版本能好好处理下这个问题。</p>
</li>
<li><p>GC，java的GC发展20年了，go才这么点时间，gc铁定不完善。所以我们仍然不能随心所欲的写代码，不然在大请求量下面gc可能会卡顿整个服务。所以有时候，该用对象池，内存池的一定要用，虽然代码丑了点，但好歹性能上去了。</p>
</li>
<li><p>泛型，虽然go有inteface，但泛型的缺失会让我们在实现一个功能的时候写大量的重复代码，譬如int32和int64类型的sort，我们得为分别写两套代码，好冗余。go 1.4之后有了go generate的支持，但这种的仍然需要自己根据go的AST库来手动写相关的parser，难度也挺大的。虽然也有很多开源的generate实现，但毕竟不是官方的。</p>
</li>
</ul>
<p>当然还有很多值得吐槽的地方，就不一一列举了，但是go仍旧有它的优势。</p>
<ul>
<li>静态语言，强类型。静态编译能帮我们检查出来大量的错误，go的强类型甚至变态到不支持隐式的类型转换。虽然写代码感觉很别扭，但减少了犯错的可能。</li>
<li>gofmt，应该这是我知道的第一个官方提供统一格式化代码工具的语言了。有了gofmt，大家的代码长一个样了，也就没有花括号到底放到结尾还是新开一行这种蛋疼的代码风格讨论了。因为大家的代码风格一样，所以看go的代码很容易。</li>
<li>天生的并行支持，因为goroutine以及channel，用go写分布式应用，写并发程序异常的容易。没有了蛋疼的callback导致的代码逻辑割裂，代码逻辑都是顺序的。</li>
<li>性能，go的性能可能赶不上c，c++以及openresty，但真的也挺强悍的。在我们的项目中，现在单机就部署了一个go的进程，就完全能够胜任以前200个python进程干的事情，而且CPU和MEM占用更低。</li>
<li>运维部署，直接编译成二进制，扔到服务器上面就成，比python需要安装一堆的环境那是简单的太多了。当然，如果有cgo，我们也需要将对应的动态库给扔过去。</li>
<li>开发效率，虽然go是静态语言，但我个人感觉开发效率真的挺高，直觉上面跟python不相上下。对于我个人来说，最好的例子就是我用go快速开发了非常多的开源组件，譬如ledisdb，go-mysql等，而这些最开始的版本都是在很短的时间里面完成的。对于我们项目来说，我们也是用go在一个月就重构完成了第一个版本，并发布。</li>
</ul>
<h2 id="实际项目中一些Go_Tips">实际项目中一些Go Tips</h2>
<p>到现在为止，我们几乎所有的服务端项目都已经转向go，当然在使用的时候也遇到了一些问题，列出来算是经验分享吧。</p>
<ul>
<li>godep，我们使用godep进行第三方库管理，但是godep我碰到的最大的坑就是build tag问题，如果一个文件有build tag，godep很有可能就会忽略这个文件。</li>
<li>IO deadline，如果能自己在应用层处理的都自己处理，go的deadline内部是timer来控制，但timer内部采用一个array来实现的heap，全局共用一个锁，如果大并发量，并且timer数量过多，timeout变动太频繁，很容易就引起性能问题。</li>
<li>GC，这个前面也说了，多用内存池，对象池，另外，我还发现，如果对象的生命周期跟goroutine一致，对性能的提升也不错，也在go的group问过相关问题，大家猜测可能是因为一些对象其实是在goroutine的8k栈上面分配的，所以一起回收没有额外GC了。</li>
<li>Go gob，如果要做RPC服务，gob并不是一个很好的选择，首先就跟python的pickle不通用，然后为了做不同系统的数据传入，任何包都必须带上类型的详细信息，size太大。go里面现在还没一套官方的RPC方案，gRPC貌似有上位的可能。</li>
</ul>
<h2 id="总结">总结</h2>
<p>虽然我现在选择了go，但是并不表示我以后不会尝试其他的语言。语言没有好坏，能帮我解决问题的就是好语言。但至少在很长的一段时间，我都会用go来进行开发。Let’ go!!!</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Use Hashicorp Raft to build a Redis sentinel]]></title>
    <link href="http://siddontang.com/2015/05/03/use-hashicorp-raft-to-build-a-redis-sentinel/"/>
    <id>http://siddontang.com/2015/05/03/use-hashicorp-raft-to-build-a-redis-sentinel/</id>
    <published>2015-05-03T02:56:11.000Z</published>
    <updated>2015-05-03T02:56:59.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Redis_Sentinel">Redis Sentinel</h2>
<p>We use Redis not only for cache, but also storing important data, and we build up a Master/Slave replication topology to guarantee data security.</p>
<p>Master/Slave architecture works well, but sometimes we need a more powerful high availability solution. If master is down, we must check this immediately, reselect a new master from the slaves and do failover.</p>
<p>The official Redis supplies a solution named redis-sentinel, which is very powerful to use. But I still want to build my own sentinel solution, why?</p>
<ul>
<li>I want to monitor not only Redis but also LedisDB, maybe other services using Redis serialization Protocol too.</li>
<li>I want to embed it into xcodis or other go service easily.</li>
<li>I want to study some consensus algorithms and use them in practice.<br>Sentinel Cluster and Election</li>
</ul>
<p>Building a single sentinel application is easy: checking master every second, and do failover when master is down. But if the sentinel is down too, how do we do?</p>
<p>Using sentinel cluster is a good choice, if one sentinel is down, other sentinel will still work. But let’s consider below scenario, if two sentinels in the cluster both see the master is down, and do failover at same time, sentinel1 may select slave1 as master, but sentinel2 may select slave2 as master, this may be a horrible thing for us.</p>
<p>A common use way is to elect a leader sentinel in the cluster and let it monitor and do failover. So we need a consensus algorithm helping us do this thing.</p>
<h2 id="Paxos_and_Raft">Paxos and Raft</h2>
<p>Paxos may be the most famous consensus algorithm in the world, many companies use it in their distributed system. However, Paxos is very hard to understand and if you write a paxos lib by yourself, you even cann’t testify its correctness easily. Luckly, we have zookeeper, an open source centralized service based on Paxos. We can use zookeeper to manage our clusters like electing a leader.</p>
<p>Raft was born on 2013 in Stanford, it’s very new but awesome. Raft is easy to understand, everyone reading the Raft paper can write its own Raft implentation easily than Paxos. Now many projects use Raft, like Etcd, InfluxDB, CockroachDB, etc…</p>
<p>The above projects I list using Raft all use Go, and I will develop my own redis sentinel with Go too, so I decide to use Raft.</p>
<h2 id="Use_Hashicorp_Raft">Use Hashicorp Raft</h2>
<p>There are some Go raft projects, but I prefer Hashicorp Raft which is easy to be integrated in other project, and this package is used in Consul product and has already been tested in production environment (maybe!).</p>
<p>The create raft function declaration is below:</p>
<pre><code>func NewRaft(conf *Config, fsm FSM, logs LogStore, stable StableStore, snaps SnapshotStore, peerStore PeerStore, trans Transport) (*Raft, error)
</code></pre><p>Although it looks a little complex, it’s still easy to use, we only need do following things:</p>
<ul>
<li>Create a configuration using raft own DefaultConfig function. We should know that raft should be used with at least three nodes, but if we just want to try it with only one node, or first start a raft node, than add others later, we must set EnableSingleNode to true.</li>
<li>Define our own FSM struct, FSM is a state machine applying replicated log, generating point-in-time snapshot, and restoring from a snapshot. In our sentinel, the only data need to care is all Redis masters, whenever we add a master, remove a master or reset all masters, we should let all sentinels know. So my FSM struct is very easy, like below:</li>
</ul>
<pre><code>type masterFSM struct {
    sync.Mutex

    // below holding all Redis master addresses
    masters map[string]struct{}
}
</code></pre><ul>
<li>Define our own FSMSnapshot struct. In our sentinel, this is a list of masters at some point. The struct like this:</li>
</ul>
<pre><code>type masterSnapshot struct {
    masters []string
}
</code></pre><ul>
<li>Create a log storage storing and retrieving logs and a stable storage storing key configurations. Hashicorp supplies a LMDB lib and a BoltDB lib for both storage, we use BoltDB because of the pure Go implementation.</li>
<li>Create a snapshot storage saving FSM snapshot, we use raft own NewFileSnapshotStore generating a file saving this.</li>
<li>Create a peer storage storing all raft nodes, we use raft own NewJSONPeers generating a file saving all nodes with JSON format.</li>
<li>Create a transport allowing a raft node to communicate with other nodes, we use raft own NewTCPTransport generating a TCP transport.</li>
</ul>
<p>After do that, we can create a raft, we can use LeaderCh and Leader function to check whether a raft node is leader or not. Only the leader node can handle operations. If the leader is down, raft can re-elect a new leader.</p>
<p>You can see the source <a href="https://github.com/siddontang/redis-failover/blob/master/failover/raft.go" target="_blank" rel="external">here</a> for more information.</p>
<h2 id="Summary">Summary</h2>
<p>Our redis sentinel is named redis-failover, although it looks a little simple and needs improvement, it still the first trial and later we will use raft in more projects, maybe instead of zookeeper.</p>
<p>redis-failover: <a href="https://github.com/siddontang/redis-failover" target="_blank" rel="external">https://github.com/siddontang/redis-failover</a></p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[构建一个简易的中心化锁服务]]></title>
    <link href="http://siddontang.com/2015/04/25/build-up-a-tiny-lock-service/"/>
    <id>http://siddontang.com/2015/04/25/build-up-a-tiny-lock-service/</id>
    <published>2015-04-25T08:35:15.000Z</published>
    <updated>2015-04-25T08:35:54.000Z</updated>
    <content type="html"><![CDATA[<h2 id="为什么需要锁服务？">为什么需要锁服务？</h2>
<p>有时候，在分布式系统中，不同的服务实例需要操作同一份资源，所以我们需要一套机制保证对该资源并发操作的数据一致性。</p>
<p>最通常的做法，就是lock。各个服务在操作资源之前，首先lock住该资源，处理完成之后在释放lock。也就是说，我们通过lock使得并行操作串行化，保证了资源的数据一致性。</p>
<h2 id="为什么要实现重新实现一个？">为什么要实现重新实现一个？</h2>
<p>虽然现在有很多不错的锁服务实现，譬如基于Zookeeper，Etcd，甚至Redis，但这里我仍然想自己实现一个简易的lock服务，主要有以下几点原因：</p>
<h3 id="Multi_Lock">Multi Lock</h3>
<p>同时获取多个lock，有时候我们想同时对多个资源进行操作，所以需要多把lock，但是无论是Zookeeper还是Redis，我们都需要对其进行多次调用，影响整体性能。</p>
<h3 id="Hierarchical_Lock">Hierarchical Lock</h3>
<p>支持hierarchical lock，我称之为path lock，我们的系统是有类似文件夹的概念的，假设现在我们需要往文件夹a/b/c下面增加一个文件，我们就必须得保证另一个进程不会同时操作该文件夹以及其祖先和后继（譬如删除a，或者a/b，或者也往a/b/c下面增加文件），但是允许操作其兄弟（譬如在a/b/d下面增加文件）。</p>
<p>仍然是上面那个例子，假设我们需要操作a/b/c，首先我们需要对a加read lock，然后对a/b加read lock，最后在对a/b/c加write lock，如果这个通过Zookeeper来实现，真心很麻烦，而且性能存疑。</p>
<h2 id="TLock">TLock</h2>
<p>tlock是一个简单的中心化lock service，现阶段为了性能没有High Availability支持，所以如果当掉了后果还是有点严重的。:-)</p>
<p>tlock支持两种模式，key和path，key就是最通常的对某个资源进行操作，而path则是我上面说的Hierarchical Lock。同时tlock支持Multi Lock。</p>
<p>tlock提供Restful API以及RESP(Redis Serialization Protocol) API，所以你可以通过任意HTTP客户端或者Redis客户端使用。</p>
<p>一个简单的HTTP例子：</p>
<pre><code>// shell1

// 同时lock a，b，c三个资源，如果30s之后仍没lock成功，返回超时错误
// 返回lockid供后续unlock使用
POST http://localhost/lock?names=a,b,c&amp;type=key&amp;timeout=30

// Unlock
DELETE http://localhost/lock?id=lockid

// shell2
POST http://localhost/lock?names=a,b,c&amp;type=key&amp;timeout=30

DELETE http://localhost/lock?id=lockid
</code></pre><p>一个简单的RESP例子：</p>
<pre><code># shell1 redis-cli
redis&gt;LOCK abc TYPE key TIMEOUT 10
redis&gt;lockid
redis&gt;UNLOCK lockid
redis&gt;OK

# shell2 redis-cli 
redis&gt;LOCK abc TYPE key TIMEOUT 10
// 一直挂起直到shell1 unlock 
redis&gt;lockid
redis&gt;UNLOCK lockid
redis&gt;OK
</code></pre><p>tlock同时提供了RESP的客户端：</p>
<pre><code>import &quot;github.com/siddontang/tlock&quot;

client := NewRESPClient(addr)
locker, _ := client.GetLocker(&quot;key&quot;, &quot;abc&quot;)
locker.Lock()
locker.Unlock()
</code></pre><h2 id="Todo">Todo</h2>
<p>可以看到，tlock是一个非常简单的服务，虽然是一个单点并且没有HA支持，但是已经能满足我们项目的需求，毕竟我们只是需要一个简单的锁服务。</p>
<p>后续，我可能会基于Zookeeper尝试实现path lock，虽然这样HA能够保证，但是性能怎样，到时候压测了再说吧。</p>
]]></content>
    
    
  </entry>
  
  <entry>
    <title><![CDATA[Dive into MySQL Replication]]></title>
    <link href="http://siddontang.com/2015/03/28/dive-into-mysql-replication/"/>
    <id>http://siddontang.com/2015/03/28/dive-into-mysql-replication/</id>
    <published>2015-03-28T08:48:35.000Z</published>
    <updated>2015-03-28T08:49:15.000Z</updated>
    <content type="html"><![CDATA[<h1 id="Dive_into_MySQL_replication_protocol">Dive into MySQL replication protocol</h1>
<h2 id="Preface">Preface</h2>
<p>Let’s consider following scenario, we store huge data in MySQL and want to know data changes immediately(data inserted, deleted or updated), then do something for these changes like updating associated cache data.</p>
<p>Using MySQL trigger + UDF may be a feasible way, but I have not used it and would not recommend this. If we have many tables, maintaining so many triggers is horrible. At the same time, UDF may fail so that we will lost some changes, and even worse, a bad UDF implementation may block service or even crash MySQL.</p>
<p>We need a better solution, and this is binlog. MySQL records every changes into binlog, so if we can sync the binlog, we will know the data changes immediately, then do something. </p>
<p>Using rsync may be a good way, but I prefer another: acting as a slave and using MySQL replication protocol to sync.</p>
<h2 id="MySQL_Packet">MySQL Packet</h2>
<p>First, we must know how to send or receive data with MySQL, and this is packet:</p>
<p>Split the data into packets of size 16MB.<br>Prepend to each chunk a packet header.  </p>
<p>A packet header has 3 bytes for following payload data length and 1 byte for sequence ID. </p>
<p>The sequence ID is incremented with each packet between client and server communication, it starts at 0 and is reset to 0 when a new command begins, e.g, we send a packet, the sequence ID is 0, and the response packet sequence ID must be 1, if not, some error happens between the communication. </p>
<p>So a packet looks like below:</p>
<pre><code>3              payload length
1              Sequence ID
string[len]    payload
</code></pre><p>As you see, sending data with packet is very easy, I have implemented a base library in <a href="https://github.com/siddontang/go-mysql" title="A MySQL toolset" target="_blank" rel="external">go-mysql</a> packet pkg.</p>
<h2 id="Connection_phase">Connection phase</h2>
<p>When we first connect to MySQL server, we must do a handshake to let server authorizing us to communicate with it later, this is called connection phase. </p>
<p>Let’s consider  a popular common connection phase.</p>
<ul>
<li>Client connects server using socket connect API.</li>
<li>Server sends a initial handshake packet which contains server’s capabilities and a 20 bytes random salt.</li>
<li>Client answers with a handshake response packet, telling server its capabilities and a  20 bytes scrambled password.</li>
<li>Server response ok packet or err packet. </li>
</ul>
<p>Although MySQL supports many authentication method, I only use username + password, you can see codes in <a href="https://github.com/siddontang/go-mysql" title="A MySQL toolset" target="_blank" rel="external">go-mysql</a> client and server pkg.</p>
<p>Now we know how to send/receive data, how to establish the connection, and it’s time to move on for syncing binlog. :-)</p>
<h2 id="Register_a_Slave">Register a Slave</h2>
<p>When we want to sync binlog, we must register a slave at master first, that is acting as a pseudo slave. </p>
<p>We only need to send COM_REGISTER_SLAVE command, the payload is:</p>
<pre><code>1              COM_REGISTER_SLAVE
4              server-id
1              slaves hostname length
string[$len]   slaves hostname
1              slaves user len
string[$len]   slaves user
1              slaves password len
string[$len]   slaves password
2              slaves mysql-port
4              replication rank
4              master-id
</code></pre><p>We must use an unique server id for our pseudo slave, I prefer using a ID bigger than 1000 which is different from our real MySQL servers. </p>
<h2 id="Dump_BinLog_from_master">Dump BinLog from master</h2>
<p>After register, we should send COM_BINLOG_DUMP command, it is very easy too, payload is:</p>
<pre><code>1              COM_BINLOG_DUMP
4              binlog-pos
2              flags
4              server-id
string[EOF]    binlog-filename
</code></pre><p>If the binlog filename is empty, server will use the first known binlog. </p>
<p>You can set flags to 1 to tell server to reply a EOF packet instead of blocking connection if there is no more binlog event. But I prefer blocking so that if there is any new event, server can send to us immediately. </p>
<p>Although MySQL supports COM_BINLOG_DUMP_GTID commands, I still prefer using command binlog filename + position, because it is very easy and for our pseudo slave, we only need to sync binlog, save it in a place and let other applications like MHA use it. </p>
<p>After we send COM_BINLOG_DUMP command, server will response a binlog network stream which contains many binlog events. </p>
<h2 id="BinLog_Event">BinLog Event</h2>
<p>A MySQL binlog event has many versions, but we only care version 4(MySQL 5.0+) and don’t support earlier versions.</p>
<p>A binlog event contains three parts, header, post header and payload, but I like parsing post header and payload at same time. </p>
<p>A event header looks below:</p>
<pre><code>4              timestamp
1              event type
4              server-id
4              event-size
4              log pos
2              flags
</code></pre><p>You can see all binlog event types here. The log pos is position of the next event, we can save this position for resuming syncing later. </p>
<p>The first binlog event in the binlog file is FORMAT_DESCRIPTION_EVENT, this event is very important, we use it to determine table ID size for row based event, and check the last 5 bytes to see whether CRC32 is enabled or not for the following events, etc…</p>
<p>Another event we should care is ROTATE_EVENT, it tells us a new binlog coming. </p>
<p>Parsing a binlog event is very easy, we only need to refer the document and parse it one by one, except row based replication event. </p>
<h2 id="Row_Based_Replication">Row Based Replication</h2>
<p>Parsing row based replication event is very hard if we want to know the real data changes for a column in one row. </p>
<p>First, we must parse TABLE_MAP_EVENT, payload is:</p>
<pre><code>post-header:
    if post_header_len == 6 {
  4              table id
    } else {
  6              table id
    }
  2              flags

payload:
  1              schema name length
  string         schema name
  1              [00]
  1              table name length
  string         table name
  1              [00]
  lenenc-int     column-count
  string.var_len [length=$column-count] column-def
  lenenc-str     column-meta-def
  n              NULL-bitmask, length: (column-count + 8) / 7
</code></pre><p>The post_header_len is saved in FORMAT_DESCRIPTION_EVENT, column count and def tells us how many columns in one row and their data types, like tiny int, short int, float, set, etc. column meta def is tricky, but we must use it to parse following ROWS_EVENT.</p>
<p>A ROWS_EVENT contains WRITE_ROWS_EVENT(insert), DELETE_ROWS_EVENT(delete) and UPDATE_ROWS_EVENT(update), every event contains v0, v1 and v2 three versions, most of time, we only need to care v1 and v2. </p>
<p>A ROWS_EVENT looks below:</p>
<pre><code>header:
  if post_header_len == 6 {
4                    table id
  } else {
6                    table id
  }
2                    flags
  if version == 2 {
2                    extra-data-length
string.var_len       extra-data
  }

body:
lenenc_int           number of columns
string.var_len       columns-present-bitmap1, length: (num of columns+7)/8
  if UPDATE_ROWS_EVENTv1 or v2 {
string.var_len       columns-present-bitmap2, length: (num of columns+7)/8
  }

rows:
string.var_len       nul-bitmap, length (bits set in &#39;columns-present-bitmap1&#39;+7)/8
string.var_len       value of each field as defined in table-map
  if UPDATE_ROWS_EVENTv1 or v2 {
string.var_len       nul-bitmap, length (bits set in &#39;columns-present-bitmap2&#39;+7)/8
string.var_len       value of each field as defined in table-map
  }
  ... repeat rows until event-end
</code></pre><p>I promise that if you don’t dive into the MySQL source, you can not understand how to parse it at all. </p>
<p>First, let’s see columns-present-bitmap, for a column, it will not be saved in the row data if the associated bit in columns-present-bitmap is 0, and we will skip this column when parsing. </p>
<p>For every row data, first we should calculate the null-bitmap, a big pitfall here,  we calculate columns-present-bitmap using (num of columns+7)/8, but we must use (bits set in ‘columns-present-bitmap’+7)/8 for null-bitmap. (You should google bits set if you don’t understand it).</p>
<p>From MySQL 5.6, it supports another two binlog row images: minimal and noblob. For minimal row image update, if we have 16 columns and only the first column data changed, if we use (num of columns+7)/8, we should use 2 bytes to store null bitmap, but if we use (bits set in ‘columns-present-bitmap’+7)/8, we will only use 1 bytes to store null bitmap, saving 1 byte(is it really necessary?). By the way, I sent a pull request for python-mysql-replication to handle minimal and noblob row image paring.</p>
<p>Now we get column-present-bitmap and null-bitmap, for a column, if it’s not set in column-present-bitmap or set in null-bitmap, we will know that this column is null for the current row data.</p>
<p>Then we will parse the rest of none null columns. For some special  columns, like MYSQL_TYPE_LONG or MYSQL_TYPE_FLOAT, we can know the data length directly, e.g, MYSQL_TYPE_LONG is 4 bytes and MYSQL_TYPE_TINY_INT is 1 byte.</p>
<p>But for other columns, we should use column meta in TABLE_MAP_EVENT to help us determine the data length. For example, a MYSQL_TYPE_BLOB column, if meta is 1, the data is tiny blob and the first 1 byte in data is the length for payload, if meta is 2, the data is short blob and the first 2 bytes is the lenght for payload. </p>
<p>Paring the real data is very hard and I can not illustrate here fully and clearly. I hope you can see the source in MySQL or <a href="https://github.com/siddontang/go-mysql" title="A MySQL toolset" target="_blank" rel="external">go-mysql</a> replication pkg if you have some interest. </p>
<h2 id="Semi_Sync_Replication">Semi Sync Replication</h2>
<p>At first, I didn’t support semi sync replication in <a href="https://github.com/siddontang/go-mysql" title="A MySQL toolset" target="_blank" rel="external">go-mysql</a>, but after I develop <a href="https://github.com/siddontang/go-mysql-elasticsearch" title="Sync MySQL into Elasticsearch" target="_blank" rel="external">go-mysql-elasticsearch</a>, I realize that if I want to sync MySQL changes into elasticsearch more quickly and immediately, supporting semi sync replication is a good choice and it’s easy to do like below: </p>
<ul>
<li>Check whether master supports semi sync or not, using “SHOW VARIABLES LIKE ‘rpl_semi_sync_master_enabled’”.</li>
<li>Tell master we support semi sync using “SET @rpl_semi_sync_slave = 1”.</li>
</ul>
<p>If all is ok, server will prepend two byte before every binlog event. The first byte is 0xef indicating semi sync and the second byte is semi sync ACK flag, if 1, we must reply a semi sync ACK packet.</p>
<p>It now seems that this is a wise decision. In facebook, they even develop a semi sync binlog, you can see more here. I develop a similar go-mysqlbinlog supporting semi sync too, but it still needs improvement for production environment. </p>
<h2 id="Summary">Summary</h2>
<p>Learning mysql protocol is a hard but happy journey for me, and I have done some interesting things too, like mixer, a MySQL proxy which the modified version(cm) has been used in production in wandoujia company. <a href="https://github.com/siddontang/mixer" title="A MySQL Proxy" target="_blank" rel="external">go-mysql-elasticsearch</a>, a tool to sync MySQL data into elasticsearch immediately. </p>
<p>Now I have been developing go-mysql, a powerful MySQL toolset. I would be very glad if some people find it can help them for their own MySQL development too.</p>
<p>Later I will also try to use go-mysqlbinlog + MHA to build our own MySQL HA solution, I don’t know perl, but luckily, I can understand MHA code.</p>
<p>Below is my contact, any advice and feedback is very welcome.</p>
<ul>
<li><p>Email: siddontang@gmail.com</p>
</li>
<li><p>Skype: live:siddontang_1</p>
</li>
</ul>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Build up a High Availability Distributed Key-Value Store]]></title>
    <link href="http://siddontang.com/2015/03/15/build-ha-distributed-kv-store/"/>
    <id>http://siddontang.com/2015/03/15/build-ha-distributed-kv-store/</id>
    <published>2015-03-15T00:44:53.000Z</published>
    <updated>2015-03-15T00:46:48.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Preface">Preface</h2>
<p>There are many awesome and powerful distributed NoSQL in the world, like Couchbase, MongoDB, Canssandra, etc. but developing a new one is still a challengeable, interesting and attractive thing for me, why?</p>
<ul>
<li>It can satisfy our special needs for our cloud services.</li>
<li>We just need a key-value store, with some simple additional functionalities, we don’t need a complex solution.</li>
<li>We can control the whole thing, especially for fixing bugs and improvement.</li>
<li>Inventing the wheel is not good, but I can learn much in the process.</li>
</ul>
<p>A key-value store may need below features:</p>
<ul>
<li>Simple protocol.</li>
<li>Simple API.</li>
<li>High performance.</li>
<li>High availability.</li>
<li>Cluster support.</li>
</ul>
<p>I knew this would be a hard journey first. But after a long hard work, I develop ledis-cluster, a key-value store based on <a href="https://github.com/siddontang/ledisdb" title="A Fast NoSQL" target="_blank" rel="external">LedisDB</a> + <a href="https://github.com/siddontang/xcodis" title="A distributed Redis/LedisDB proxy" target="_blank" rel="external">xcodis</a> + <a href="https://github.com/siddontang/redis-failover" title="Automatic redis monitoring and failover" target="_blank" rel="external">redis-failover</a>.</p>
<h2 id="Pre_Solution_Thinking">Pre Solution Thinking</h2>
<p>Before I develop ledis-cluster, I thought some other solutions which are valuable to be recorded here too.</p>
<h3 id="MySQL">MySQL</h3>
<p>Aha, first I just wanted to use MySQL as a key-value store. This thought amazed my colleagues before, and I think now it may surprise many other guys too.</p>
<p>MySQL is a relational database and can be used as a key-value store easily and sufficiently. We can use a simple table to store key value data like below:</p>
<pre><code>CREATE TABLE kv (
    k varbinary(256),
    v blob,
    PRIMARY KEY(k),
) ENGINE=innodb;
</code></pre><p>When I worked in Tencent game infrastructure department, we used this way to serve many Tencent games and it works well.</p>
<p>But I don’t want to use MySQL as a key-value store now, MySQL is a little heavy and needs some experienced operations people, this is impossible for our team.</p>
<h3 id="Redis">Redis</h3>
<p>Redis is an awesome NoSQL, it has an amazing performance, supports many useful data structures (kv, hash, list, set and zset), supplies a simple protocol for client user.</p>
<p>I have read the Redis’s code (it is very simple!) many times, used it for about three years in many productions, and I am absolutely confident of maintaining it.</p>
<p>But Redis has a serious problem: memory limitation. We can not store huge data in one machine. Using redis cluster is a good way to solve memory limitation, and there are many existing solutions, like official redis cluster, twemproxy or codis , but I still think another stuff saving huge data exceeding memory limitation in one machine is needed, so I develop LedisDB.</p>
<h2 id="LedisDB">LedisDB</h2>
<p>LedisDB is a fast NoSQL, similar to Redis. It has some good features below:</p>
<ul>
<li>Uses Redis protocol, most of the Redis clients can use LedisDB directly.</li>
<li>Supports multi data structures(kv, hash, list, set, zset).</li>
<li>Uses rocksdb, leveldb or other fast databases as the backend to store huge data, exceeding memory limitation.</li>
<li>High performance, see benchmark. Although it is a little slower than Redis, it can still be used in production.</li>
</ul>
<p>A simple example:</p>
<pre><code>//start ledis server
ledis-server
//another shell
ledis-cli -p 6380
ledis&gt; set a 1
OK
ledis&gt; get a
“1&quot;
</code></pre><p>As we see, LedisDB is simple, we can switch to it easily if we used Redis before.</p>
<p>LedisDB now supports rocksdb, leveldb, goleveldb, boltdb and lmdb as the backend storage, we can choose the best one for our actual environment. In our company projects, we use rocksdb which has a awesome performance and many configurations to be tuned, and I will also use it for the following example.</p>
<h2 id="Data_Security_Guarantee">Data Security Guarantee</h2>
<p>LedisDB can store huge data in one machine, so the data security needs to be considered cautiously. LedisDB uses below ways to guarantee it.</p>
<h3 id="Backup">Backup</h3>
<p>We can back up LedisDB and then restore later. Redis saving RDB may block service for some time, but LedisDB doesn’t have this problem. Thanks to rocksdb fast generating snapshot technology, backing up LedisDB is very fast and easy.</p>
<h3 id="Binlog">Binlog</h3>
<p>LedisDB will first log write operations in binlog, then commit changes into backend storage, this is similar to MySQL.</p>
<p>Redis also has AOF, but the AOF file may grow largely, then rewriting AOF may also block service for some time. LedisDB will rotate binlog and write to the new one when current binlog is larger than maximum size (1GB).</p>
<h3 id="Replication">Replication</h3>
<p>An old saying goes like this: “don’t put all your eggs in one basket”. Similarly, don’t put all our data in one machine.</p>
<p>LedisDB supports asynchronous or semi-synchronous replication. We can not break CAP(Consistency, Availability, Partition tolerance) theorem, for replication, partition tolerance must exist, so we have to choose between consistency and availability.</p>
<p>If we want to guarantee full data security, we may use semi-synchronous replication, but most of time, asynchronous replication is enough.</p>
<h2 id="Monitor_and_Failover">Monitor and Failover</h2>
<p>In the actual production environment, we use a master LedisDB and one or more slaves to construct the topology. We must monitor them in real time because any machine in the topology may be down at any time.</p>
<p>If a slave is down, we may not care too much, this is not a serious problem. But if the master is down (aha, a terrible accident!), we must resolve it quickly.</p>
<p>Generally, we can not expect the master to re-work quickly and infallibly, so electing a best new master from current slaves and doing failover is a better way when master is down.</p>
<p>Redis uses a sentinel feature to monitor the topology and do failover when the master is down. But this sentinel can not be used in LedisDB, so I develop another sentinel: redis-failover, monitoring and doing failover for Redis/LedisDB.</p>
<p>redis-failover uses <code>ROLE</code> command to check master and get all slaves every second. If the master is down, redis-failover will select the best slave from last <code>ROLE</code> returned slaves. The election algorithm is simple, using <code>INFO</code> command to get “slave_priority” and “slave_repl_offset” value, if a slave has a higher priority or a larger repliction offset with same priority, the slave will be elected as the new master.</p>
<p>redis-failover may have single point problem too, I use zookeeper or raft to support redis-failover cluster. Zookeeper or raft will elect a leader and let it monitor and do failover, if the leader is down, a new leader will be elected quickly.</p>
<h2 id="Cluster">Cluster</h2>
<p>Although LedisDB can store huge data, the growing data may still exceed the capability of the system in the near future.</p>
<p>Splitting data and storing them into multi machines may be the only feasible way(We don’t have money to buy a mainframe), but how to split the data? and how to find the data by a key? I think an easy solution is to define a key routing rule (mapping key to the actual machine).</p>
<p>For example, we have two machines, n0 and n1, and the key routing rule is simple hash like <code>crc32(key) % 2</code>. For key “abc”, the calculation result is 0, so we know that the corresponding data is in n0.</p>
<p>The above solution is easy, but we can not use it in production. If we add another machine, the machine number is 3, all the old data mapping relationship will be broken, and we have to relocate huge amount of data.</p>
<p>Using consistency hash may be better, but I prefer using hash + routing table. We don’t map a key to a machine directly, but to a virtual node named slot, then define a routing table mapping slot to the actual machine.</p>
<p>Continuing with the example, assume we use 1024 slots and 2 machines, the slot and machine mapping is [slot0 — slot511] -&gt; n0, [slot512 — slot1023] -&gt; n1. For a key, first using <code>crc32(key) % 1024</code> to get a slot index, then we can find the machine with this slot from the routing table.</p>
<p>This solution may be complex but have a big advantage for re-sharding. If we add another machine n2, change the routing table that mapping slot0 to n2, and we only need to migrate all slot0 data from n0 to n2. The bigger for slot number, the smaller for split data in a slot, and we only migrate little data for one slot.</p>
<p>xcodis uses above way to support LedisDB cluster. Now the slot number is 256, which is a little small that may increase the probability of mapping some busy keys into a slot.</p>
<p>Because of origin LedisDB db index implementation limitation, xcodis can not use bigger slot number than 256, so a better way is to support customizing a routing table for a busy key later. For example, for a key, xcodis should first try to find the associated slot in the routing table, if not found, then use hash.</p>
<p>Another radical choice is to change LedisDB code and upgrade all data saved before. This is may be a huge work, so I will not consider it unless I have no idea to resolve above problems.</p>
<p>xcodis is a proxy supporting redis/LedisDB cluster, the benefit of proxy is that we can hide all cluster information from client users and users can use it easily like using a single server.</p>
<p>In addition to proxy, there are also some other ways to support cluster too:</p>
<p>Official Redis cluster, but it is still in development and should not be used in production now, and it can not be used in LedisDB.<br>Customizing client SDK, the SDK can know whole cluster information and do the right key routing for the user. But this way is not universal and we must write many SDKs for different languages (c, java, php, go, etc.), a hard work!</p>
<h2 id="Final_Architecture">Final Architecture</h2>
<p>At last, the final architecture may look below:</p>
<p><img src="https://raw.githubusercontent.com/siddontang/blog/master/asserts/kv-architecture.png" alt="kv architecture"></p>
<ul>
<li>Use LedisDB to save huge data in one machine.</li>
<li>Use Master/slave to guarantee data security.</li>
<li>Use redis-failover to monitor the system and do failover.</li>
<li>Use xcodis to support cluster.</li>
</ul>
<p>This architecture may be not perfect, but is simple and enough for us. Now we have only use LedisDB and xcodis in our projects, not the whole architecture, but we have been testing and will try to deploy it in production in the near future.</p>
<h2 id="Summary">Summary</h2>
<p>Building up a key-value store is not a easy work, and I don’t think what I do above can beat other existing awesome NoSQLs, but it’s a valuable attempt, I have learned much and meet many new friends in the progress.</p>
<p>Now, I’am the only person to develop the whole thing and need help, if you have interested in what I do, please contact me, maybe we really can build up an awesome NoSQL. :-)</p>
<p>Mail: <a href="&#x6d;&#x61;&#x69;&#108;&#116;&#x6f;&#58;&#115;&#105;&#100;&#100;&#x6f;&#110;&#x74;&#97;&#x6e;&#103;&#64;&#103;&#x6d;&#x61;&#105;&#x6c;&#46;&#99;&#111;&#x6d;">&#115;&#105;&#100;&#100;&#x6f;&#110;&#x74;&#97;&#x6e;&#103;&#64;&#103;&#x6d;&#x61;&#105;&#x6c;&#46;&#99;&#111;&#x6d;</a></p>
<p>Github: <a href="https://github.com/siddontang" target="_blank" rel="external">github.com/siddontang</a></p>
]]></content>
    
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="xcodis" scheme="http://siddontang.com/tags/xcodis/"/>
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[构建高可用分布式Key-Value存储服务]]></title>
    <link href="http://siddontang.com/2015/03/06/build-ha-distributed-kv-service/"/>
    <id>http://siddontang.com/2015/03/06/build-ha-distributed-kv-service/</id>
    <published>2015-03-06T13:24:18.000Z</published>
    <updated>2015-03-06T13:25:43.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>当我们构建服务端应用的时候，都会面临数据存放的问题。不同的数据类型有不同的存放方式，譬如关系型数据通常使用MySQL来存储，文档型数据则会考虑使用MongoDB，而这里，我们仅仅考虑最简单的kv（key-value）。</p>
<p>kv的使用场景很多，一个很典型的场景就是用户session的存放，key为用户当前的session id，而value则是用户当前会话需要保存的一些信息。因为kv的场景很多，所以选择一个好的kv服务就很重要了。</p>
<p>对于笔者来说，一个不错的kv服务可能仅仅需要满足如下几点就够了：</p>
<ul>
<li>协议简单</li>
<li>高性能</li>
<li>高可用</li>
<li>易扩容</li>
</ul>
<p>市面上已经有很多满足条件kv服务，但笔者秉着no zuo no die的精神，决定使用<a href="https://github.com/siddontang/ledisdb" title="A Fast NoSQL" target="_blank" rel="external">LedisDB</a> + <a href="https://github.com/siddontang/xcodis" title="A distributed Redis/LedisDB proxy" target="_blank" rel="external">xcodis</a> + <a href="https://github.com/siddontang/redis-failover" title="Automatic redis monitoring and failover" target="_blank" rel="external">redis-failover</a>来构建一个高可用分布式kv存储服务。</p>
<h2 id="现有解决方案">现有解决方案</h2>
<p>在继续说明之前，笔者想说说曾经考虑使用或者已经使用的一些解决方案。</p>
<h3 id="MySQL">MySQL</h3>
<p>好吧，别笑，我真的说的是MySQL。MySQL作为一个关系型数据库，用来存储kv性能真心一点都不差。table的结构很简单，可能如下：</p>
<pre><code>CREATE TABLE kv (
    k VARBINARY(256),
    v BLOB,
    PRIMARY KEY(k),
) ENGINE=innodb;
</code></pre><p>当我还在腾讯互动娱乐部门的时候，一些游戏项目就仅仅将MySQL作为kv来使用，譬如用来存放玩家数据，游戏服务器通过玩家id读取对应的数据，修改，然后更新。鉴于腾讯游戏恐怖的用户量，MySQL能撑住直接就能说明将MySQL作为一个kv来用是可行的。</p>
<p>不过不知道现在还有多少游戏项目仍然采用这种做法，毕竟笔者觉得，将MySQL作为一个kv服务，有点杀鸡用牛刀的感觉，MySQL还是有点重了。</p>
<h3 id="Couchbase">Couchbase</h3>
<p><a href="http://www.couchbase.com/" target="_blank" rel="external">Couchbase</a>是一个高性能的分布式NoSQL，它甚至能支持跨data center的备份。笔者研究了很长一段时间，但最终并没有决定采用，主要笔者没信心去搞定它的代码。</p>
<h3 id="Redis">Redis</h3>
<p>Redis是一个高性能NoSQL，它不光支持kv，同时还提供了其他的数据结构如hash，list，set，zset供外部使用。</p>
<p>笔者在三年前就开始使用Redis，加之Redis的代码简单，很容易就能理解掌控。所以一直到现在，笔者都会优先使用Redis来存储很多非关系型数据。自然对于kv，笔者也是采用Redis来存放的。</p>
<p>但Redis也有一些不足，最大的莫过于内存限制，Redis存储的总数据大小最好别超过物理内存，不然性能会有问题。同时，笔者觉得Redis的RDB和AOF机制也比较蛋疼，RDB的时候系统可能会出现卡顿，而AOF在rewrite的时候也可能出现类似的问题。</p>
<p>因为内存的限制，所以Redis不能存储超大量的数据，为了解决这个问题，我们只能采用cluster的方案，但是Redis官方的cluster仍然处于开发阶段，并不能真正在生产环境中使用。所以笔者开发了<a href="https://github.com/siddontang/ledisdb" title="A Fast NoSQL" target="_blank" rel="external">LedisDB</a>。</p>
<h2 id="LedisDB">LedisDB</h2>
<p>开发<a href="https://github.com/siddontang/ledisdb" title="A Fast NoSQL" target="_blank" rel="external">LedisDB</a>，主要就是为了解决Redis内存限制问题，它主要有如下特性：</p>
<ul>
<li>采用Redis协议，大部分Redis的client都能直接使用。</li>
<li>提供类似Redis的API，支持kv，hash，list，set，zset。</li>
<li>底层采用多种db存储实际数据，支持rocksdb（推荐），leveldb，goleveldb，boltdb，lmdb，没有Redis内存限制问题，因为将数据放到硬盘里面了。</li>
<li>高性能，参考<a href="https://github.com/siddontang/ledisdb/wiki/Benchmark" target="_blank" rel="external">benchmark</a>，虽然比Redis略慢，但完全可用于生产环境。</li>
</ul>
<p>一个简单地例子：</p>
<pre><code>//start ledis server
ledis-server 

//another shell
ledis-cli -p 6380

ledis&gt; set a 1
OK
ledis&gt; get a
&quot;1&quot;
</code></pre><p>可以看到，LedisDB非常类似Redis，所以用户能很方便的从Redis迁移到LedisDB上面。在实际生产环境中，笔者建议底层选择rocksdb作为其存储模块，它不光性能高，同时提供了很多配置方便用户根据特定情况进行调优（当然，理解这一堆配置可是一件很蛋疼的事情）。后续，笔者对于LedisDB的使用说明都会是基于rocksdb的。</p>
<h2 id="数据安全">数据安全</h2>
<p>虽然LedisDB能存储大量数据，并且易于使用，但是作为一个数据存储服务，数据的安全性是一个非常需要考虑的问题。</p>
<ul>
<li>LedisDB提供了dump和load工具，我们可以很方便的对其备份。在dump的时候，我们仅仅使用的是rocksdb的snapshot机制，非常快速，同时不会阻塞当前服务。这点可能是相对于Redis RDB的优势。虽然Redis的RDB在save的时候也是fork一个子进程进行处理，但如果Redis的数据量巨大，仍然可能造成Redis的卡顿。</li>
<li>LedisDB提供类似MySQL的binlog支持，任何操作都是写入binlog之后再最终提交到底层db的。如果服务崩溃，我们能通过binlog进行数据恢复。binlog文件有大小限制，当超过阀值之后，LedisDB会写入一个新的binlog中，而不是像Redis的AOF一样进行rewrite处理。</li>
<li>LedisDB支持同步或者异步replication，同步复制能保证数据的强一致，但是会牺牲系统的性能，而异步复制虽然高效，但可能会面对数据丢失问题。这其实就是一个CAP选择问题，在P（partition tolerance）铁定存在的情况下，选择C（consistency）还是选择A（availability）？通常情况下，笔者会选择A。</li>
</ul>
<h2 id="故障转移">故障转移</h2>
<p>在生产环境中，为了保证数据安全，一个master我们会通常配备一个或者多个slave（笔者喜欢将其称为replication topology），当master当掉的时候，监控系统会选择一个最优的slave（也就是拥有master数据最多的那个），将其提升为新的master，并且将其他slave指向该new master。这套流程也就是我们通常说的failover。</p>
<p>Redis提供了sentinel机制来实现整个replication topology的failover。但sentinel是跟redis绑定的，所以不能直接在LedisDB上面使用，所以笔者开发了<a href="https://github.com/siddontang/redis-failover" title="Automatic redis monitoring and failover" target="_blank" rel="external">redis-failover</a>，一个能支持redis，或者LedisDB failover的sentinel。</p>
<p>redis-failover通过定期向master发送<code>role</code>命令来获知当前replication topology，主要是slaves的信息。当master当掉之后，redis-failover就会从先前获取的slaves里面选择一个最优的slave，提升为master，选择最优的算法很简单，通过<code>info</code>命令得到”slave_priority”和”slave_repl_offset”，如果哪个slave的priority最大，就选择那个，如果priority都一样，则选择replication offset最大的那个。</p>
<p>redis-failover会存在单点问题，所以redis-failover自身需要支持cluster。redis-failover的cluster在内部选举一个leader用来进行实际的monitor以及failover处理，当leader当掉之后，则进行重新选举。</p>
<p>现阶段，redis-failover可以通过外部的zookeeper进行leader选举，同时也支持内部自身通过raft算法进行leader选举。</p>
<h2 id="分布式集群">分布式集群</h2>
<p>随着数据量的持续增大，单台机器最终无法存储所有数据，我们不得不考虑通过cluster的方式来解决，也就是将数据放到不同的机器上面去。</p>
<p>要构建LedisDB的cluster，笔者考虑了如下三种方案，这里，我们不说啥hash取模或者consistency hash了，如果cluster真能通过这两种技术简单搞定，那还要这么费力干啥。</p>
<ul>
<li><p>Redis cluster。</p>
<p>  redis cluster是redis官方提供的cluster解决方案，性能高，并且能支持resharding。可是直到现在，redis cluster仍处于开发阶段，至少笔者是不敢将其用于生产环境中。另外，笔者觉得它真的很复杂，还是别浪费脑细胞去搞定这套架构了。</p>
</li>
<li><p>定制client。</p>
<p>  通过定制client，我们可以知道不同key的路由规则，自然就能找到实际的数据了。这方面的工作我的一位盆友正在进行，但定制client有一个很严重的问题在于所有的client都必须自己实现，其实不算是一个通用的解决方案。</p>
</li>
<li><p>Proxy</p>
<p>  记得有人说过，计算机科学领域的任何问题, 都可以通过添加一个中间层来解决。而proxy则是用来解决cluster问题的一个中间层。</p>
<p>  <a href="https://github.com/twitter/twemproxy" target="_blank" rel="external">Twemproxy</a>是一个很不错的选择，但是它不能支持resharding，而且貌似twitter内部也没在使用了，所以笔者并不考虑使用。</p>
<p>  本来笔者打算自己写一个proxy，但这时候，<a href="https://github.com/wandoulabs/codis" target="_blank" rel="external">codis</a>横空出世，它是一个分布式的proxy，同时支持resharding，并且在豌豆荚的生产环境中得到验证，笔者立刻就决定使用codis了。</p>
<p>  但codis并不支持LedisDB，同时为了满足他们自身的需求，使用的也是一个修改版的redis，鉴于此，笔者实现了<a href="https://github.com/siddontang/xcodis" title="A distributed Redis/LedisDB proxy" target="_blank" rel="external">xcodis</a>，一个基于codis的，支持LedisDB以及原生redis的proxy。</p>
</li>
</ul>
<h2 id="架构">架构</h2>
<p>最终的架构如下。</p>
<p><img src="https://raw.githubusercontent.com/siddontang/blog/master/asserts/kv-architecture.png" alt="kv architecture"></p>
<p>我们通过使用LedisDB来解决了Redis单机数据容量问题，通过replication机制保证数据安全性，通过redis-failover用来进行failover处理，最后通过xcodis进行集群管理。</p>
<p>当前，这套架构并没有在生产环境中得到验证，但我们一直在内部不断测试，而且国外也有用户在帮助笔者验证这套架构，所以笔者对其还是很有信心的，希望能早日上线。如果有哪位童鞋也对这套架构感兴趣，想吃螃蟹的，笔者非常愿意提供支持。</p>
]]></content>
    
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="xcodis" scheme="http://siddontang.com/tags/xcodis/"/>
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习redis sort命令]]></title>
    <link href="http://siddontang.com/2015/02/10/learn-redis-sort/"/>
    <id>http://siddontang.com/2015/02/10/learn-redis-sort/</id>
    <published>2015-02-10T13:39:57.000Z</published>
    <updated>2015-02-10T13:41:32.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/siddontang/ledisdb" title="A fast NoSQL" target="_blank" rel="external">LedisDB</a>本来是没有sort命令的，而且实际我们也没有使用过该命令，但一位用户给我反应他迫切需要这个功能，我决定首先考察一下redis相关的实现，再看是否提供。</p>
<p>然后我一看redis的sort命令，真的是震惊了，这可能算得上redis里面最复杂的一个命令了，命令原型如下：</p>
<pre><code>SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC|DESC] [ALPHA] [STORE destination]
</code></pre><p>如果不仔细看文档，或者看源码，一下子真的不知道这个命令怎么用。首先我们可以去掉LIMIT offset count这个选项，这个很容易理解，就是排好序之后取偏移数据。ASC和DESC这个也比较容易，就是正向和逆向排序。STORE destination这个其实就是将排好序的数据放到destination这个list里面，也比较容易理解。好了，去掉这些，那么sort的原型就是这个样子了:</p>
<pre><code>SORT key [BY pattern] [GET pattern [GET pattern ...]] [ALPHA]
</code></pre><p>key里面存储的就是需要排序的东西，所以key只能是list，set或者zset类型，我们以list为例。假设做如下操作：</p>
<pre><code>redis&gt; lpush a 1 2 3
redis&gt; lrange a 0 -1
1) &quot;3&quot;
2) &quot;2&quot;
3) &quot;1&quot;
</code></pre><p>如果使用sort，则排序结果如下:</p>
<pre><code>redis&gt; sort a
1) &quot;1&quot;
2) &quot;2&quot;
3) &quot;3&quot;
</code></pre><p>呢么ALPHA是什么意思呢？我们可以做如下操作解释：</p>
<pre><code>redis&gt; lpush b a1 a2 a3
redis&gt; sort b
(error) ERR One or more scores can&#39;t be converted into double
redis&gt; sort b alpha
1) &quot;a1&quot;
2) &quot;a2&quot;
3) &quot;a3&quot;
</code></pre><p>我们在b里面压入的是字符串，所以不能直接sort，必须指定alpha方式。所以alpha就是明确告知sort使用字节序排序，不然sort就会尝试将需要排序的数据转成double类型。</p>
<p>理解了alpha，我们再来看看by的含义，如下例子：</p>
<pre><code>redis&gt; set w_1 3
redis&gt; set w_1 30
redis&gt; set w_2 20
redis&gt; set w_3 10
redis&gt; sort a by w_*
1) &quot;3&quot;
2) &quot;2&quot;
3) &quot;1&quot;
127.0.0.1:6379&gt;
</code></pre><p>如果有by了，sort就会首先取出对应的数据，也就是1，2，3，然后跟by的pattern进行组合，变成w_1，w_2，w_3，然后以这个作为key去获取对应的值，也就是30，20，10，在按照这些值进行排序。上面这个例子，1对应的by值最大，为30，所以升序排列的时候在最后。</p>
<p>说完了by，我们再来说说get，get是不参与排序的，只是在拍完序之后，将排好序的值依次跟get的pattern组合，获取对应的数据，进行返回，如下例子：</p>
<pre><code>redis&gt; set o_1 10
redis&gt; set o_2 20
redis&gt; set o_3 30
redis&gt; sort a get o_*
1) &quot;10&quot;
2) &quot;20&quot;
3) &quot;30&quot;
</code></pre><p>再来一个多个get的例子：</p>
<pre><code>redis&gt; set oo_1 100
redis&gt; set oo_2 200
redis&gt; set oo_3 300
redis&gt; sort a get o_* get oo_*
1) &quot;10&quot;
2) &quot;100&quot;
3) &quot;20&quot;
4) &quot;200&quot;
5) &quot;30&quot;
6) &quot;300&quot;
</code></pre><p>从上面可以看到，如果有多个get，那么sort的做法是对于排好序的一个值，依次通过get获取值，放到结果中，然后在处理下一个值。</p>
<p>如果有get，我们就能获取到相关的值，但这时候我们还需要返回原有的值怎么办？只需要<code>get #</code>就成了，如下：</p>
<pre><code>redis&gt; sort a get o_* get #
1) &quot;10&quot;
2) &quot;1&quot;
3) &quot;20&quot;
4) &quot;2&quot;
5) &quot;30&quot;
6) &quot;3&quot;
</code></pre><p>好了，折腾了这么久，我算是终于理解了sort的原理，然后在看完sort的实现，依葫芦画瓢在<a href="https://github.com/siddontang/ledisdb" title="A fast NoSQL" target="_blank" rel="external">LedisDB</a>里面支持了sort。当然在一些底层细节上面还是稍微跟redis不一样的。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[深入解析MySQL replication协议]]></title>
    <link href="http://siddontang.com/2015/02/02/mysql-replication-protocol/"/>
    <id>http://siddontang.com/2015/02/02/mysql-replication-protocol/</id>
    <published>2015-02-02T13:10:03.000Z</published>
    <updated>2015-02-02T13:10:53.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Why">Why</h2>
<p>最开始的时候，<a href="https://github.com/siddontang/go-mysql" title="A MySQL toolset" target="_blank" rel="external">go-mysql</a>只是简单的抽象<a href="https://github.com/siddontang/mixer" title="A MySQL Proxy" target="_blank" rel="external">mixer</a>的代码，提供一个基本的mysql driver以及proxy framework，但做到后面，笔者突然觉得，既然研究了这么久mysql client/server protocol，干脆顺带把replication protocol也给弄明白算了。现在想想，幸好当初决定实现了replication的支持，不然后续<a href="https://github.com/siddontang/go-mysql-elasticsearch" title="Sync MySQL into Elasticsearch" target="_blank" rel="external">go-mysql-elasticsearch</a>这个自动同步MySQL到Elasticsearch的工具就不可能在短时间完成。</p>
<p>其实MySQL replication protocol很简单，client向server发送一个MySQL binlog dump的命令，server就会源源不断的给client发送一个接一个的binlog event了。</p>
<h2 id="Register">Register</h2>
<p>首先，我们需要伪造一个slave，向master注册，这样master才会发送binlog event。注册很简单，就是向master发送COM_REGISTER_SLAVE命令，带上slave相关信息。这里需要注意，因为在MySQL的replication topology中，都需要使用一个唯一的server id来区别标示不同的server实例，所以这里我们伪造的slave也需要一个唯一的server id。</p>
<h2 id="Binlog_dump">Binlog dump</h2>
<p>最开始的时候，MySQL只支持一种binlog dump方式，也就是指定binlog filename + position，向master发送COM_BINLOG_DUMP命令。在发送dump命令的时候，我们可以指定flag为BINLOG_DUMP_NON_BLOCK，这样master在没有可发送的binlog event之后，就会返回一个EOF package。不过通常对于slave来说，一直把连接挂着可能更好，这样能更及时收到新产生的binlog event。</p>
<p>在MySQL 5.6之后，支持了另一种dump方式，也就是GTID dump，通过发送COM_BINLOG_DUMP_GTID命令实现，需要带上的是相应的GTID信息，不过笔者觉得，如果只是单纯的实现一个能同步binlog的工具，使用最原始的binlog filename + position就够了，毕竟我们不是MySQL，解析GTID还是稍显麻烦的。这里，顺带吐槽一下MySQL internal文档，里面关于GTID encode的格式说明竟然是错误的，文档格式如下:</p>
<pre><code>4                n_sids
  for n_sids {
string[16]       SID
8                n_intervals
    for n_intervals {
8                start (signed)
8                end (signed)
    }
</code></pre><p>但实际坑爹的是n_sids的长度是8个字节。这个错误可以算是血的教训，笔者当时debug了很久都没发现为啥GTID dump一直出错，直到笔者查看了MySQL的源码。</p>
<p>MariaDB虽然也引入了GTID，但是并没有提供一个类似MySQL的GTID dump命令，仍是使用的COM_BINLOG_DUMP命令，不过稍微需要额外设置一些session variable，譬如要设置slave_connect_state为当前已经完成的GTID，这样master就能知道下一个event从哪里发送了。</p>
<h2 id="Binlog_Event">Binlog Event</h2>
<p>对于一个binlog event来说，它分为三个部分，header，post-header以及payload。但实际笔者在处理event的时候，把post-header和payload当成了一个整体body。</p>
<p>MySQL的binlog event有很多版本，但这里笔者只关心version 4的，也就是从MySQL 5.1.x之后支持的版本。而且笔者也只支持这个版本的event解析，首先是不想写过多的兼容代码，另一个更主要的原因就在于现在几乎都没有人使用低版本的MySQL了。</p>
<p>Binlog event的header格式如下：</p>
<pre><code>4              timestamp
1              event type
4              server-id
4              event-size
4              log pos
2              flags
</code></pre><p>header的长度固定为19，event type用来标识这个event的类型，event size则是该event包括header的整体长度，而log pos则是下一个event所在的位置。</p>
<p>在v4版本的binlog文件中，第一个event就是FORMAT_DESCRIPTION_EVENT，格式为:</p>
<pre><code>2                binlog-version
string[50]       mysql-server version
4                create timestamp
1                event header length
string[p]        event type header lengths
</code></pre><p>我们需要关注的就是event type header length这个字段，它保存了不同event的post-header长度，通常我们都不需要关注这个值，但是在解析后面非常重要的ROWS_EVENT的时候，就需要它来判断TableID的长度了。这个后续在说明。</p>
<p>而binlog文件的结尾，通常（只要master不当机）就是ROTATE_EVENT或者STOP_EVENT。这里我们重点关注ROTATE_EVENT，格式如下:</p>
<pre><code>Post-header
8              position
Payload
string[p]      name of the next binlog
</code></pre><p>它里面其实就是标明下一个event所在的binlog filename和position。这里需要注意，当slave发送binlog dump之后，master首先会发送一个ROTATE_EVENT，用来告知slave下一个event所在位置，然后才跟着FORMAT_DESCRIPTION_EVENT。</p>
<p>其实我们可以看到，binlog event的格式很简单，文档都有着详细的说明。通常来说，我们仅仅需要关注几种特定类型的event，所以只需要写出这几种event的解析代码就可以了，剩下的完全可以跳过。</p>
<h2 id="Row_Based_Replication">Row Based Replication</h2>
<p>如果真要说处理binlog event有啥复杂的，那铁定属于row based replication相关的ROWS_EVENT了，对于一个ROWS_EVENT来说，它记录了每一行数据的变化情况，而对于外部来说，是需要准确的知道这一行数据到底如何变化的，所以我们需要获取到该行每一列的值。而如何解析相关的数据，是非常复杂的。笔者也是看了很久MySQL，MariaDB源码，以及<a href="https://github.com/noplay/python-mysql-replication" title="Pure python for MySQL replication protocol" target="_blank" rel="external">mysql-python-replication</a>的实现，才最终搞定了这个个人觉得最困难的部分。</p>
<p>在详细说明ROWS_EVENT之前，我们先来看看TABLE_MAP_EVENT，该event记录的是某个table一些相关信息，格式如下:</p>
<pre><code>post-header:
    if post_header_len == 6 {
  4              table id
    } else {
  6              table id
    }
  2              flags

payload:
  1              schema name length
  string         schema name
  1              [00]
  1              table name length
  string         table name
  1              [00]
  lenenc-int     column-count
  string.var_len [length=$column-count] column-def
  lenenc-str     column-meta-def
  n              NULL-bitmask, length: (column-count + 8) / 7
</code></pre><p>table id需要根据post_header_len来判断字节长度，而post_header_len就是存放到FORMAT_DESCRIPTION_EVENT里面的。这里需要注意，虽然我们可以用table id来代表一个特定的table，但是因为alter table或者rotate binlog event等原因，master会改变某个table的table id，所以我们在外部不能使用这个table id来索引某个table。</p>
<p>TABLE_MAP_EVENT最需要关注的就是里面的column meta信息，后续我们解析ROWS_EVENT的时候会根据这个来处理不同数据类型的数据。column def则定义了每个列的类型。</p>
<p>ROWS_EVENT包含了insert，update以及delete三种event，并且有v0，v1以及v2三个版本。</p>
<p>ROWS_EVENT的格式很复杂，如下：</p>
<pre><code>header:
  if post_header_len == 6 {
4                    table id
  } else {
6                    table id
  }
2                    flags
  if version == 2 {
2                    extra-data-length
string.var_len       extra-data
  }

body:
lenenc_int           number of columns
string.var_len       columns-present-bitmap1, length: (num of columns+7)/8
  if UPDATE_ROWS_EVENTv1 or v2 {
string.var_len       columns-present-bitmap2, length: (num of columns+7)/8
  }

rows:
string.var_len       nul-bitmap, length (bits set in &#39;columns-present-bitmap1&#39;+7)/8
string.var_len       value of each field as defined in table-map
  if UPDATE_ROWS_EVENTv1 or v2 {
string.var_len       nul-bitmap, length (bits set in &#39;columns-present-bitmap2&#39;+7)/8
string.var_len       value of each field as defined in table-map
  }
  ... repeat rows until event-end
</code></pre><p>ROWS_EVENT的table id跟TABLE_MAP_EVENT一样，虽然table id可能变化，但是ROWS_EVENT和TABLE_MAP_EVENT的table id是能保证一致的，所以我们也是通过这个来找到对应的TABLE_MAP_EVENT。</p>
<p>为了节省空间，ROWS_EVENT里面对于各列状态都是采用bitmap的方式来处理的。</p>
<p>首先我们需要得到columns present bitmap的数据，这个值用来表示当前列的一些状态，如果没有设置，也就是某列对应的bit为0，表明该ROWS_EVENT里面没有该列的数据，外部直接使用null代替就成了。</p>
<p>然后就是null bitmap，这个用来表明一行实际的数据里面有哪些列是null的，这里最坑爹的是null bitmap的计算方式并不是<code>(num of columns+7)/8</code>，也就是MySQL计算bitmap最通用的方式，而是通过columns present bitmap的bits set个数来计算的，这个坑真的很大，为啥要这么设计，最主要的原因就在于MySQL 5.6之后binlog row image的格式增加了minimal和noblob，尤其是minimal，update的时候只会记录相应更改字段的数据，譬如我一行有16列，那么用2个byte就能搞定null bitmap了，但是如果这时候只有第一列更新了数据，其实我们只需要使用1个byte就能记录了，因为后面的铁定全为0，就不需要额外空间存放了，不过话说真有必要这么省空间吗？</p>
<p>null bitmap的计算需要通过columns present bitmap的bits set计算，bits set其实也很好理解，就是一个byte按照二进制展示的时候1的个数，譬如1的bits set就是1，而3的bits set就是2，而255的bits set就是8了。</p>
<p>好了，得到了present bitmap以及null bitmap之后，我们就能实际解析这行对应的列数据了，对于每一列，首先判断是否present bitmap标记了，如果为0，则跳过用null表示，然后在看是否在null bitmap里面标记了，如果为1，表明值为null，最后我们就开始解析真有有数据的列了。</p>
<p>但是，因为我们得到的是一行数据的二进制流，我们怎么知道一列数据如何解析？这里，就要靠TABLE_MAP_EVENT里面的column def以及meta了。</p>
<p>column def定义了该列的数据类型，对于一些特定的类型，譬如MYSQL_TYPE_LONG, MYSQL_TYPE_TINY等，长度都是固定的，所以我们可以直接读取对应的长度数据得到实际的值。但是对于一些类型，则没有这么简单了。这时候就需要通过meta来辅助计算了。</p>
<p>譬如对于MYSQL_TYPE_BLOB类型，meta为1表明是tiny blob，第一个字节就是blob的长度，2表明的是short blob，前两个字节为blob的长度等，而对于MYSQL_TYPE_VARCHAR类型，meta则存储的是string长度。这里，笔者并没有列出MYSQL_TYPE_NEWDECIMAL，MYSQL_TYPE_TIME2等，因为它们的实现实在是过于复杂，笔者几乎对照着MySQL的源码实现的。</p>
<p>搞定了这些，我们终于可以完整的解析一个ROWS_EVENT了，顺带说一下，<a href="https://github.com/noplay/python-mysql-replication" title="Pure python for MySQL replication protocol" target="_blank" rel="external">python-mysql-replication</a>里面minimal/noblob row image的支持，也是笔者提交的<a href="https://github.com/noplay/python-mysql-replication/pull/103" title="Add binlog row minimal and noblob image support" target="_blank" rel="external">pull request</a>，貌似是笔者第一次给其他开源项目做贡献。</p>
<h2 id="总结">总结</h2>
<p>实现MySQL replication protocol的解析真心是一件很有挑战的事情，虽然辛苦，但是让笔者更加深入的学习了MySQL的源码，为后续笔者改进<a href="https://github.com/noplay/python-mysql-replication/pull/103" title="Add binlog row minimal and noblob image support" target="_blank" rel="external">LedisDB</a>的replication以及更深入的了解MySQL的replication打下了坚实的基础。</p>
<p>话说，现在成果已经显现，不然<a href="https://github.com/siddontang/go-mysql-elasticsearch" title="Sync MySQL into Elasticsearch" target="_blank" rel="external">go-mysql-elasticsearch</a>不可能如此快速实现，后续笔者准备基于此做一个更新cache的服务，这样我们的代码里面就不会到处出现更新cache的代码了。</p>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[MySQL问题两则]]></title>
    <link href="http://siddontang.com/2015/01/25/two-mysql-problems/"/>
    <id>http://siddontang.com/2015/01/25/two-mysql-problems/</id>
    <published>2015-01-25T13:06:26.000Z</published>
    <updated>2015-01-25T13:07:08.000Z</updated>
    <content type="html"><![CDATA[<p>这段时间处理了两个比较有意思的MySQL问题，一个死锁的，一个优化的，陡然发现其实自己对MySQL的理解还不深入，很多运行机制也是知其然但不知其所以然，后续还需要好好恶补一下底层知识。</p>
<h2 id="一次不可思议的死锁">一次不可思议的死锁</h2>
<p>假设有如下表结构：</p>
<pre><code>mysql&gt; show create table tt \G;
*************************** 1. row ***************************
       Table: tt
Create Table: CREATE TABLE `tt` (
  `id` int(11) NOT NULL DEFAULT &#39;0&#39;,
  `fileid` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `fileid` (`fileid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
1 row in set (0.00 sec)
</code></pre><p>启动三个shell，连接MySQL，然后<code>begin</code>开启一个事务，各个shell分别执行对应的更新语句，</p>
<p>shell 1：</p>
<pre><code>shell 1&gt; update tt set id = 2 where fileid = 1;
</code></pre><p>shell 2：</p>
<pre><code>shell 2&gt; update tt set id = 3 where fileid = 1;
</code></pre><p>shell 3：</p>
<pre><code>shell 3&gt; update tt set id = 4 where fileid = 1;
</code></pre><p>假设shell 1先执行，这时候2和3会block，然后shell 1 commit提交，我们发现shell 2执行成功，但是3出现死锁错误，通过<code>show engine innodb status</code>我们得到如下死锁信息:</p>
<pre><code>------------------------
LATEST DETECTED DEADLOCK
------------------------
2015-01-23 14:24:16 10ceed000
*** (1) TRANSACTION:
TRANSACTION 24897, ACTIVE 3 sec starting index read
mysql tables in use 1, locked 1
LOCK WAIT 2 lock struct(s), heap size 360, 1 row lock(s)
MySQL thread id 8, OS thread handle 0x10cea5000, query id 138 127.0.0.1 root updating
update tt set id = 4 where fileid = 1
*** (1) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24897 lock_mode X locks rec but not gap waiting
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** (2) TRANSACTION:
TRANSACTION 24896, ACTIVE 8 sec updating or deleting
mysql tables in use 1, locked 1
4 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 2
MySQL thread id 7, OS thread handle 0x10ceed000, query id 136 127.0.0.1 root updating
update tt set id = 3 where fileid = 1
*** (2) HOLDS THE LOCK(S):
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24896 lock_mode X locks rec but not gap
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** (2) WAITING FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 495 page no 4 n bits 72 index `fileid` of table `test`.`tt` trx id 24896 lock mode S waiting
Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
 0: len 4; hex 80000001; asc     ;;
 1: len 4; hex 80000002; asc     ;;

*** WE ROLL BACK TRANSACTION (1)
------------
</code></pre><p>刚开始碰到这个死锁问题，真心觉得很奇怪，每个事务一条语句，通过一个唯一索引去更新同一条记录，正常来说完全不可能发生死锁，但确确实实发生了。笔者百思不得其解，幸好有google，然后搜到了这篇，<a href="http://hedengcheng.com/?p=844" target="_blank" rel="external">一个最不可思议的MySQL死锁分析</a>，虽然触发情况不一样，但是死锁原理都应该类似的，后续如果有精力，笔者将好好深入研究一下。</p>
<p>顺带再说一下，<a href="http://hedengcheng.com/?p=771" target="_blank" rel="external">MySQL 加锁处理分析</a>这篇文章也是干活满满，这两篇加起来深入理解了，对MySQL的deadlock就会有一个很全面的认识了。</p>
<h2 id="一次坑爹的优化">一次坑爹的优化</h2>
<p>我们需要在一张表里面删除某种类型的数据，大概的表结构类似这样:</p>
<pre><code>CREATE TABLE t (
    id INT,
    tp ENUM (&quot;t1&quot;, &quot;t2&quot;),
    PRIMARY KEY(id)
) ENGINE=INNODB;
</code></pre><p>假设我们需要删除类型为t2的数据，语句可能是这样<code>delete from t where tp = &quot;t2&quot;</code>，这样没啥问题，但我们这张表有5亿数据，好吧，真的是5亿，所以以后别再跟我说MySQL表存储百万级别数据就要分表了，百万太小case了。</p>
<p>这事情我交给了一个小盆友去帮我搞定，他最开始写出了如下的语句<code>delete from t where tp = &quot;t2&quot; limit 1000</code>，使用limit来限制一次删除的个数，可以了，不过这有个很严重的问题，就是越往后，随着t2类型的减少，我们几乎都是全表遍历来删除，所以总的应该是O(n*n)的开销。</p>
<p>于是我让他考虑主键，每次操作的时候，记录当前最大的主键，这样下次就可以从这个主键之后开始删除了，首先 <code>select id from t where id &gt; last_max_select_id and tp = &quot;t2&quot; limit 1000</code>，然后<code>delete from t where id in (ids)</code>，虽然这次优化采用了两条语句，但是通过主键，我们只需要遍历一次表就可以了，总的来说，性能要快的。</p>
<p>但是，实际测试的时候，我们却发现，select这条语句耗时将近30s，太慢了。虽然我们使用了主键，但是MySQL仍然需要不停的读取数据判断条件，加之t2类型的数据在表里面比较少量，所以为了limit 1000这个条件，MySQL需要持续的进行IO读取操作，结果自然是太慢了。</p>
<p>想清楚了这个，其实就好优化了，我们只需要让条件判断在应用层做，MySQL只查询数据返回，语句就是 <code>select id, tp from t where id &gt; last_max_select_id limit 1000</code>，得到结果集之后，自行判断需要删除的id，然后delete。看似我们需要额外处理逻辑，并且网络开销也增大了，但MySQL只是简单的IO读取，非常快，总的来说，性能提升很显著。当然笔者后续还需要更深入的分析。</p>
<p>最后执行，很happy的是，非常快速的就删完了相关数据，而select的查询时间消耗几乎忽略不计。</p>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Go中优雅的关闭HTTP服务]]></title>
    <link href="http://siddontang.com/2015/01/25/stop-server-gracefully/"/>
    <id>http://siddontang.com/2015/01/25/stop-server-gracefully/</id>
    <published>2015-01-25T13:05:04.000Z</published>
    <updated>2015-01-25T13:07:01.000Z</updated>
    <content type="html"><![CDATA[<p>虽然写出7x24小时不间断运行的服务是一件很酷的事情，但是我们仍然在某些时候，譬如服务升级，配置更新等，得考虑如何优雅的结束这个服务。</p>
<p>当然，最暴力的做法直接就是<code>kill -9</code>，但这样直接导致的后果就是可能干掉了很多运行到一半的任务，最终导致数据不一致，这个苦果只有遇到过的人才能深深地体会，数据的修复真的挺蛋疼，有时候还得给用户赔钱啦。</p>
<p>所以，通常我们都是给服务发送一个信号，SIGTERM也行，SIGINTERRUPT也成，反正要让服务知道该结束了。而服务收到结束信号之后，首先会拒绝掉所有外部新的请求，然后等待当前所有正在执行的请求完成之后，在结束。当然很有可能当前在执行一个很耗时间的任务，导致服务长时间不能结束，这时候就得决定是否强制结束了。</p>
<p>具体到go的HTTP Server里面，如何优雅的结束一个HTTP Server呢？</p>
<p>首先，我们需要显示的创建一个listener，让其循环不断的accept新的连接供server处理，为啥不用默认的http.ListenAndServe，主要就在于我们可以在结束的时候通过关闭这个listener来主动的拒绝掉外部新的连接请求。代码如下:</p>
<pre><code>l, _ := net.Listen(&quot;tcp&quot;, address)
svr := http.Server{Handler: handler}
svr.Serve(l)
</code></pre><p>Serve这个函数是个死循环，我们可以在外部通过close对应的listener来结束。</p>
<p>当listener accept到新的请求之后，会开启一个新的goroutine来执行，那么在server结束的时候，我们怎么知道这个goroutine是否完成了呢？</p>
<p>在很早之前，大概go1.2的时候，笔者通过在handler入口处使用sync WaitGroup来实现，因为我们有统一的一个入口handler，所以很容易就可以通过如下方式知道请求是否完成，譬如：</p>
<pre><code>func (h *Handler) ServeHTTP(w ResponseWriter, r *Request) {
    h.svr.wg.Add(1)
    defer h.svr.wg.Done()

    ......
}
</code></pre><p>但这样其实只是用来判断请求是否结束了，我们知道在HTTP 1.1中，connection是能够keepalived的，也就是请求处理完成了，但是connection仍是可用的，我们没有一个好的办法close掉这个connection。不过话说回来，我们只要保证当前请求能正常结束，connection能不能正常close真心无所谓，毕竟服务都结束了，connection自动就close了。但谁叫笔者是典型的处女座呢。</p>
<p>在go1.3之后，提供了一个ConnState的hook，我们能通过这个来获取到对应的connection，这样在服务结束的时候我们就能够close掉这个connection了。该hook会在如下几种ConnState状态的时候调用。</p>
<ul>
<li>StateNew：新的连接，并且马上准备发送请求了</li>
<li>StateActive：表明一个connection已经接收到一个或者多个字节的请求数据，在server调用实际的handler之前调用hook。</li>
<li>StateIdle：表明一个connection已经处理完成一次请求，但因为是keepalived的，所以不会close，继续等待下一次请求。</li>
<li>StateHijacked：表明外部调用了hijack，最终状态。</li>
<li>StateClosed：表明connection已经结束掉了，最终状态。</li>
</ul>
<p>通常，我们不会进入hijacked的状态（如果是websocket就得考虑了），所以一个可能的hook函数如下，参考<a href="http://rcrowley.org/talks/gophercon-2014.html" target="_blank" rel="external">http://rcrowley.org/talks/gophercon-2014.html</a></p>
<pre><code>s.ConnState = func(conn net.Conn, state http.ConnState) {
    switch state {
    case http.StateNew:
        // 新的连接，计数加1
        s.wg.Add(1)
    case http.StateActive:
        // 有新的请求，从idle conn pool中移除
        s.mu.Lock()
        delete(s.conns, conn.LocalAddr().String())
        s.mu.Unlock()
    case http.StateIdle:
        select {
        case &lt;-s.quit:
            // 如果要关闭了，直接Close，否则加入idle conn pool中。
            conn.Close()
        default:
            s.mu.Lock()
            s.conns[conn.LocalAddr().String()] = conn
            s.mu.Unlock()
        }
    case http.StateHijacked, http.StateClosed:
        // conn已经closed了，计数减一
        s.wg.Done()
    }
</code></pre><p>当结束的时候，会走如下流程：</p>
<pre><code>func (s *Server) Close() error {
    // close quit channel, 广播我要结束啦
    close(s.quit)

    // 关闭keepalived，请求返回的时候会带上Close header。客户端就知道要close掉connection了。
    s.SetKeepAlivesEnabled(false)
    s.mu.Lock()

    // close listenser
    if err := s.l.Close(); err != nil {
        return err 
    }

    //将当前idle的connections设置read timeout，便于后续关闭。
    t := time.Now().Add(100 * time.Millisecond)
    for _, c := range s.conns {
        c.SetReadDeadline(t)
    }
    s.conns = make(map[string]net.Conn)
    s.mu.Unlock()

    // 等待所有连接结束
    s.wg.Wait()
    return nil
}
</code></pre><p>好了，通过以上方法，我们终于能从容的关闭server了。但这里仅仅是针对跟客户端的连接，实际还有MySQL连接，Redis连接，打开的文件句柄，等等，总之，要实现优雅的服务关闭，真心不是一件很简单的事情。</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Elasticsearch学习笔记]]></title>
    <link href="http://siddontang.com/2015/01/18/elasticsearch-note/"/>
    <id>http://siddontang.com/2015/01/18/elasticsearch-note/</id>
    <published>2015-01-18T06:58:47.000Z</published>
    <updated>2015-01-18T10:05:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Why_Elasticsearch？">Why Elasticsearch？</h2>
<p>由于需要提升项目的搜索质量，最近研究了一下Elasticsearch，一款非常优秀的分布式搜索程序。最开始的一些笔记放到<a href="https://github.com/siddontang/elasticsearch-note" target="_blank" rel="external">github</a>，这里只是归纳总结一下。</p>
<p>首先，为什么要使用Elasticsearch？最开始的时候，我们的项目仅仅使用MySQL进行简单的搜索，然后一个不能索引的like语句，直接拉低MySQL的性能。后来，我们曾考虑过sphinx，并且sphinx也在之前的项目中成功实施过，但想想现在的数据量级，多台MySQL，以及搜索服务本身HA，还有后续扩容的问题，我们觉得sphinx并不是一个最优的选择。于是自然将目光放到了Elasticsearch上面。</p>
<p>根据官网自己的介绍，Elasticsearch是一个分布式搜索服务，提供Restful API，底层基于Lucene，采用多shard的方式保证数据安全，并且提供自动resharding的功能，加之github等大型的站点也采用Elasticsearch作为其搜索服务，我们决定在项目中使用Elasticsearch。</p>
<p>对于Elasticsearch，如果要在项目中使用，需要解决如下问题：</p>
<ol>
<li>索引，对于需要搜索的数据，如何建立合适的索引，还需要根据特定的语言使用不同的analyzer等。</li>
<li>搜索，Elasticsearch提供了非常强大的搜索功能，如何写出高效的搜索语句？</li>
<li>数据源，我们所有的数据是存放到MySQL的，MySQL是唯一数据源，如何将MySQL的数据导入到Elasticsearch？</li>
</ol>
<p>对于1和2，因为我们的数据都是从MySQL生成，index的field是固定的，主要做的工作就是根据业务场景设计好对应的mapping以及search语句就可以了，当然实际不可能这么简单，需要我们不断的调优。</p>
<p>而对于3，则是需要一个工具将MySQL的数据导入Elasticsearch，因为我们对搜索实时性要求很高，所以需要将MySQL的增量数据实时导入，笔者唯一能想到的就是通过row based binlog来完成。而近段时间的工作，也就是实现一个MySQL增量同步到Elasticsearch的服务。</p>
<h2 id="Lucene">Lucene</h2>
<p>Elasticsearch底层是基于Lucene的，Lucene是一款优秀的搜索lib，当然，笔者以前仍然没有接触使用过。:-)</p>
<p>Lucene关键概念：</p>
<ul>
<li>Document：用来索引和搜索的主要数据源，包含一个或者多个Field，而这些Field则包含我们跟Lucene交互的数据。</li>
<li>Field：Document的一个组成部分，有两个部分组成，name和value。</li>
<li>Term：不可分割的单词，搜索最小单元。</li>
<li>Token：一个Term呈现方式，包含这个Term的内容，在文档中的起始位置，以及类型。</li>
</ul>
<p>Lucene使用<a href="http://en.wikipedia.org/wiki/Inverted_index" target="_blank" rel="external">Inverted index</a>来存储term在document中位置的映射关系。<br>譬如如下文档：</p>
<ul>
<li>Elasticsearch Server 1.0 （document 1）</li>
<li>Mastring Elasticsearch （document 2）</li>
<li>Apache Solr 4 Cookbook （document 3）</li>
</ul>
<p>使用inverted index存储，一个简单地映射关系：</p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Count</th>
<th>Docuemnt</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>1</td>
<td><1></1></td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Apache</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Cookbook</td>
<td>1</td>
<td><3></3></td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>2</td>
<td><1>.<2></2></1></td>
</tr>
<tr>
<td>Mastering</td>
<td>1</td>
<td><2></2></td>
</tr>
<tr>
<td>Server</td>
<td>1</td>
<td><1></1></td>
</tr>
<tr>
<td>Solr</td>
<td>1</td>
<td><3></3></td>
</tr>
</tbody>
</table>
<p>对于上面例子，我们首先通过分词算法将一个文档切分成一个一个的token，再得到该token与document的映射关系，并记录token出现的总次数。这样就得到了一个简单的inverted index。</p>
<h2 id="Elasticsearch关键概念">Elasticsearch关键概念</h2>
<p>要使用Elasticsearch，笔者认为，只需要理解几个基本概念就可以了。</p>
<p>在数据层面，主要有：</p>
<ul>
<li>Index：Elasticsearch用来存储数据的逻辑区域，它类似于关系型数据库中的table概念。一个index可以在一个或者多个shard上面，同时一个shard也可能会有多个replicas。</li>
<li>Document：Elasticsearch里面存储的实体数据，类似于关系数据中一个table里面的一行数据。<br>document由多个field组成，不同的document里面同名的field一定具有相同的类型。document里面field可以重复出现，也就是一个field会有多个值，即multivalued。</li>
<li>Document type：为了查询需要，一个index可能会有多种document，也就是document type，但需要注意，不同document里面同名的field一定要是相同类型的。</li>
<li>Mapping：存储field的相关映射信息，不同document type会有不同的mapping。</li>
</ul>
<p>对于熟悉MySQL的童鞋，我们只需要大概认为Index就是一个table，document就是一行数据，field就是table的column，mapping就是table的定义就可以了。</p>
<p>Document type这个概念其实最开始也把笔者给弄糊涂了，其实它就是为了更好的查询，举个简单的例子，一个index，可能一部分数据我们想使用一种查询方式，而另一部分数据我们想使用另一种查询方式，于是就有了两种type了。不过这种情况应该在我们的项目中不会出现，所以通常一个index下面仅会有一个type。</p>
<p>在服务层面，主要有：</p>
<ul>
<li>Node: 一个server实例。</li>
<li>Cluster：多个node组成cluster。</li>
<li>Shard：数据分片，一个index可能会存在于多个shards，不同shards可能在不同nodes。</li>
<li>Replica：shard的备份，有一个primary shard，其余的叫做replica shards。</li>
</ul>
<p>Elasticsearch之所以能动态resharding，主要在于它最开始就预先分配了多个shards（貌似是1024），然后以shard为单位进行数据迁移。这个做法其实在分布式领域非常的普遍，<a href="github.com/wandoulabs/codis">codis</a>就是使用了1024个slot来进行数据迁移。</p>
<p>因为任意一个index都可配置多个replica，通过冗余备份的方式保证了数据的安全性，同时replica也能分担读压力，类似于MySQL中的slave。</p>
<h2 id="Restful_API">Restful API</h2>
<p>Elasticsearch提供了Restful API，使用json格式，这使得它非常利于与外部交互，虽然Elasticsearch的客户端很多，但笔者仍然很容易的就写出了一个简易客户端用于项目中，再次印证了Elasticsearch的使用真心很容易。</p>
<p>Restful的接口很简单，一个url表示一个特定的资源，譬如<code>/blog/article/1</code>，就表示一个index为blog，type为aritcle，id为1的document。</p>
<p>而我们使用http标准method来操作这些资源，POST新增，PUT更新，GET获取，DELETE删除，HEAD判断是否存在。</p>
<p>这里，友情推荐<a href="https://github.com/jakubroztocil/httpie" target="_blank" rel="external">httpie</a>，一个非常强大的http工具，个人感觉比curl还用，几乎是命令行调试Elasticsearch的绝配。</p>
<p>一些使用httpie的例子:</p>
<pre><code># create
http POST :9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&#39;[&quot;elasticsearch&quot;]&#39;

# get
http GET :9200/blog/article/1

# update
http PUT :9200/blog/article/1 title=&quot;hello elasticsearch&quot; tags:=&#39;[&quot;elasticsearch&quot;, &quot;hello&quot;]&#39;

# delete
http DELETE :9200/blog/article/1

# exists
http HEAD :9200/blog/article/1
</code></pre><h2 id="索引和搜索">索引和搜索</h2>
<p>虽然Elasticsearch能自动判断field类型并建立合适的索引，但笔者仍然推荐自己设置相关索引规则，这样才能更好为后续的搜索服务。</p>
<p>我们通过定制mapping的方式来设置不同field的索引规则。</p>
<p>而对于搜索，Elasticsearch提供了太多的搜索选项，就不一一概述了。</p>
<p>索引和搜索是Elasticsearch非常重要的两个方面，直接关系到产品的搜索体验，但笔者现阶段也仅仅是大概了解了一点，后续在详细介绍。</p>
<h2 id="同步MySQL数据">同步MySQL数据</h2>
<p>Elasticsearch是很强大，但要建立在有足量数据情况下面。我们的数据都在MySQL上面，所以如何将MySQL的数据导入Elasticsearch就是笔者最近研究的东西了。</p>
<p>虽然现在有一些实现，譬如<a href="https://github.com/jprante/elasticsearch-river-jdbc" target="_blank" rel="external">elasticsearch-river-jdbc</a>，或者<a href="https://github.com/scharron/elasticsearch-river-mysql" target="_blank" rel="external">elasticsearch-river-mysql</a>，但笔者并不打算使用。</p>
<p>elasticsearch-river-jdbc的功能是很强大，但并没有很好的支持增量数据更新的问题，它需要对应的表只增不减，而这个几乎在项目中是不可能办到的。</p>
<p>elasticsearch-river-mysql倒是做的很不错，采用了<a href="https://github.com/noplay/python-mysql-replication" target="_blank" rel="external">python-mysql-replication</a>来通过binlog获取变更的数据，进行增量更新，但它貌似处理MySQL dump数据导入的问题，不过这个笔者真的好好确认一下？话说，python-mysql-replication笔者还提交过<a href="https://github.com/noplay/python-mysql-replication/pull/103" target="_blank" rel="external">pull</a>解决了minimal row image的问题，所以对elasticsearch-river-mysql这个项目很有好感。只是笔者决定自己写一个出来。</p>
<p>为什么笔者决定自己写一个，不是因为笔者喜欢造轮子，主要原因在于对于这种MySQL syncer服务（增量获取MySQL数据更新到相关系统），我们不光可以用到Elasticsearch上面，而且还能用到其他服务，譬如cache上面。所以笔者其实想实现的是一个通用MySQL syncer组件，只是现在主要关注Elasticsearch罢了。</p>
<p>项目代码在这里<a href="https://github.com/siddontang/go-mysql-elasticsearch" target="_blank" rel="external">go-mysql-elasticsearch</a>，仍然处于开发状态，预计下周能基本完成。</p>
<p>go-mysql-elasticsearch的原理很简单，首先使用mysqldump获取当前MySQL的数据，然后在通过此时binlog的name和position获取增量数据。</p>
<p>一些限制：</p>
<ul>
<li>binlog一定要变成row-based format格式，其实我们并不需要担心这种格式的binlog占用太多的硬盘空间，MySQL 5.6之后GTID模式都推荐使用row-based format了，而且通常我们都会把控SQL语句质量，不允许一次性更改过多行数据的。</li>
<li>需要同步的table最好是innodb引擎，这样mysqldump的时候才不会阻碍写操作。</li>
<li>需要同步的table一定要有主键，好吧，如果一个table没有主键，笔者真心会怀疑设计这个table的同学编程水平了。多列主键也是不推荐的，笔者现阶段不打算支持。</li>
<li>一定别动态更改需要同步的table结构，Elasticsearch只能支持动态增加field，并不支持动态删除和更改field。通常来说，如果涉及到alter table，很多时候已经证明前期设计的不合理以及对于未来扩展的预估不足了。</li>
</ul>
<p>更详细的说明，等到笔者完成了go-mysql-elasticsearch的开发，再进行补充。</p>
<h2 id="总结">总结</h2>
<p>最近一周，笔者花了不少时间在Elasticsearch上面，现在算是基本入门了。其实笔者觉得，对于一门不懂的技术，找一份靠谱的资料（官方文档或者入门书籍），蛋疼的对着资料敲一遍代码，不懂的再问google，最后在将其用到实际项目，这门技术就算是初步掌握了，当然精通还得在下点功夫。</p>
<p>现在笔者只是觉得Elasticsearch很美好，上线之后铁定会有坑的，那时候只能慢慢填了。话说，笔者是不是要学习下java了，省的到时候看不懂代码就惨了。:-)</p>
]]></content>
    
    
      <category term="elasticsearch" scheme="http://siddontang.com/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker实践]]></title>
    <link href="http://siddontang.com/2015/01/09/docker-practise/"/>
    <id>http://siddontang.com/2015/01/09/docker-practise/</id>
    <published>2015-01-09T14:48:10.000Z</published>
    <updated>2015-01-09T14:49:18.000Z</updated>
    <content type="html"><![CDATA[<h2 id="起因">起因</h2>
<p>Docker算是现在非常火的一个项目，但笔者对其一直不怎么感冒，毕竟没啥使用场景。只是最近，笔者需要在自己的mac电脑上面安装项目的开发环境，发现需要安装MySQL，LedisDB，xcodis，Redis，Zookeeper等一堆东西，而同样的流程仍然要在Windows的机器上面再来一遍，陡然觉得必须得有一个更好的方式来管理整个项目的开发环境了。自然，笔者将目光放到了Docker上面。</p>
<p>根据官方自己的介绍，Docker其实是一个为开发和运维人员提供构建，分发以及运行分布式应用的开源平台（野心真的不小，难怪CoreOS要新弄一个Rocket来跟他竞争的）。</p>
<p>Docker主要包括Docker Engine，一个轻量级的运行和包管理工具，Docker Hub，一个用来共享和自动化工作流的云服务。实际在使用Docker的工程中，我们通常都是会在Docker Hub上面找到一个base image，编写Dockerfile，构建我们自己的image。所以很多时候，学习使用Docker，我们仅需要了解Docker Engine的东西就可以了。</p>
<p>至于为啥选用Docker，原因还是很明确的，轻量简单，相比于使用VM，Docker实在是太轻量了，笔者在自己的mac air上面同时可以运行多个Docker container进行开发工作，而这个对VM来说是不敢想象的。</p>
<p>后面，笔者将结合自己的经验，来说说如何构建一个MySQL Docker，以及当中踩过的坑。</p>
<h2 id="MySQL_Docker">MySQL Docker</h2>
<p>笔者一直从事MySQL相关工具的开发，对于MySQL的依赖很深，但每次安装MySQL其实是让笔者非常头疼的一件事情，不同平台安装方式不一样，加上一堆设置，很容易就把人搞晕了。所以自然，我的Docker第一次尝试就放到了MySQL上面。</p>
<p>对于mac用户，首先需要安装boot2docker这个工具才能使用Docker，这个工具是挺方便的，但也有点坑，后续会说明。</p>
<p>笔者前面说了，通常使用Docker的方式是在Hub上面找一个base image，虽然Hub上面有很多MySQL的image，但笔者因为开发<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>，需要在MySQL启动的时候传入特定的参数，所以决定自行编写Dockerfile来构建。</p>
<p>首先，笔者使用的base image为ubuntu:14.04，Dockerfile文件很简单，如下:</p>
<pre><code>FROM ubuntu:14.04

# 安装MySQL 5.6，因为笔者需要使用GTID
RUN apt-get update \
    &amp;&amp; apt-get install -y mysql-server-5.6

# 清空apt-get的cache以及MySQL datadir
RUN apt-get clean
RUN rm -rf /var/lib/apt/lists/* /var/lib/mysql

# 使用精简配置，主要是为了省内存，笔者机器至少要跑6个MySQL
ADD my.cnf /etc/mysql/my.cnf

# 这里主要是给mysql_install_db脚本使用
ADD my-default.cnf /usr/share/mysql/my-default.cnf

# 增加启动脚本
ADD start.sh /start.sh
RUN chmod +x /start.sh

# 将MySQL datadir设置成可外部挂载
VOLUME [&quot;/var/lib/mysql&quot;]

# 导出3306端口
EXPOSE 3306

# 启动执行start.sh脚本
CMD [&quot;/start.sh&quot;]
</code></pre><p>我们需要注意，对于MySQL这种需要存储数据的服务来说，一定需要给datadir设置VOLUMN，这样你才能存储数据。笔者当初就忘记设置VOLUMN，结果启动6个MySQL Docker container之后，突然发现这几个MySQL使用的是同一份数据。</p>
<p>如果有VOLUMN, 我们可以在<code>docker run</code>的时候指定对应的外部挂载点，如果没有指定，Docker会在自己的vm目录下面生成一个唯一的挂载点，我们可以通过<code>docker inspect</code>命令详细了解每个container的情况。</p>
<p>对于<code>start.sh</code>，比较简单：</p>
<ul>
<li>判断MySQL datadir下面有没有数据，如果没有，调用<code>mysql_install_db</code>初始化。</li>
<li>允许任意ip都能使用root账号访问，<code>mysql -uroot -e &quot;GRANT ALL ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39; WITH GRANT OPTION;&quot;</code>，否则我们在外部无法连接MySQL。</li>
<li>启动mysql</li>
</ul>
<p>构建好了MySQL Docker image，我们就能使用<code>docker run</code>来运行了，很简单</p>
<pre><code>docker run -d -p 3306:3306 --name=mysql siddontang/mysql:latest
</code></pre><p>这里，我们基于siddontang/mysql这个image创建了一个名叫mysql的container并运行，它会调用<code>start.sh</code>脚本来启动MySQL。</p>
<p>而我们通过<code>docker stop mysql</code>就可以停止mysql container了。</p>
<p>如果笔者需要运行多个MySQL，仅仅需要多新建几个container并运行就可以了，当然得指定对应的端口。可以看到，这种方式非常的简单，虽然使用<code>mysqld_multi</code>也能达到同样的效果，但是如果我需要在新增一个MySQL实例，<code>mysqld_mutli</code>还需要去更改配置文件，以及在对应的MySQL里面设置允许<code>mysqld_multi stop</code>的权限，其实算是比较麻烦的。而这些，在Docker里面，一个<code>docker run</code>就搞定了。</p>
<p>完整的构建代码在这里，<a href="https://github.com/siddontang/mysql-docker" target="_blank" rel="external">mysql-docker</a>，你也可以pull笔者提交到Hub的image <code>siddontang/mysql</code>来直接使用<code>docker pull siddontang/mysql:latest</code>。</p>
<h2 id="Boot2Docker_Pitfall">Boot2Docker Pitfall</h2>
<p>从前面可以看到，Docker的使用是非常方便的，但笔者在使用的时候仍然碰到了一点坑，这里记录一下。</p>
<h3 id="IP">IP</h3>
<p>最开始碰到的就是ip问题，笔者在run的时候做了端口映射，但是外部使用MySQL客户端死活连接不上，而这个只在笔者mac上面出现，linux上面正常，后来发现是boot2docker的问题，我们需要使用<code>boot2docker ip</code>返回的ip来访问container，在笔者的机器上面，这个ip为192.168.59.103。</p>
<h3 id="Volumn">Volumn</h3>
<p>仍然是boot2docker的问题，笔者在<code>docker run</code>的时候，使用<code>-v</code>来将外部的目录绑定到datadir这个VOLUMN上面，这个在linux上面是成功的，可是在mac上面，笔者发现<code>mysql_install_db</code>死活没有权限写入磁盘。后来才知道，boot2docker只允许对自己VM下面的路径进行绑定。鉴于在mac下面仅仅是调试，数据不许持久化保存，这个问题也懒得管了。反正只要不删除掉container，数据还是会在的。</p>
<h2 id="Flatten_Image">Flatten Image</h2>
<p>在使用Dockerfile构建自己的image的时候，对于Dockerfile里面的每一步，Docker都会生成一个layer来对应，也就是每一步都是一次提交，到最后你会发现，生成的image非常的庞大，而当你push这个image到Hub上面的时候，你的所有layer都会提交上去，加之我们国家的网速水平，会让人崩溃的。</p>
<p>所以我们需要精简生成的image大小，也就是flatten，这个Docker官方还没有支持，但至少我们还是有办法的：</p>
<ul>
<li><code>docker export</code> and <code>docker import</code>，通过对特定container的export和import操作，我们可以生成一个无历史的新container，详见<a href="http://tuhrig.de/flatten-a-docker-container-or-image/" target="_blank" rel="external">这里</a>。</li>
<li><a href="https://github.com/jwilder/docker-squash" target="_blank" rel="external">docker-squash</a>，很方便的一个工具，笔者就使用这个进行image的flatten处理。</li>
</ul>
<h2 id="后记">后记</h2>
<p>总的来说，Docker还是很容易上手的，只要我们熟悉了它的命令，Dockerfile的编写以及相应的运行机制，就能很方便的用Docker来进行团队的持续集成开发。而在生产环境中使用Docker，笔者还没有相关的经验，没准后续私有云会采用Docker进行部署。</p>
<p>后续，对于多个Container的交互，以及服务发现，扩容等，笔者也还需要好好研究，CoreOS没准是一个方向，或者研究下<a href="https://github.com/coreos/rocket" target="_blank" rel="external">rocket</a> :-)</p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="docker" scheme="http://siddontang.com/tags/docker/"/>
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[MySQL高可用浅析]]></title>
    <link href="http://siddontang.com/2015/01/03/mysql-ha/"/>
    <id>http://siddontang.com/2015/01/03/mysql-ha/</id>
    <published>2015-01-03T13:20:27.000Z</published>
    <updated>2015-01-03T13:25:21.000Z</updated>
    <content type="html"><![CDATA[<p>对于多数应用来说，MySQL都是作为最关键的数据存储中心的，所以，如何让MySQL提供HA服务，是我们不得不面对的一个问题。当master当机的时候，我们如何保证数据尽可能的不丢失，如何保证快速的获知master当机并进行相应的故障转移处理，都是需要我们好好思考的。这里，笔者将结合这段时间做的MySQL proxy以及toolsets相关工作，说说我们现阶段以及后续会在项目中采用的MySQL HA方案。</p>
<h2 id="Replication">Replication</h2>
<p>要保证MySQL数据不丢失，replication是一个很好的解决方案，而MySQL也提供了一套强大的replication机制。只是我们需要知道，为了性能考量，replication是采用的asynchronous模式，也就是写入的数据并不会同步更新到slave上面，如果这时候master当机，我们仍然可能会面临数据丢失的风险。</p>
<p>为了解决这个问题，我们可以使用semi-synchronous replication，semi-synchronous replication的原理很简单，当master处理完一个事务，它会等待至少一个支持semi-synchronous的slave确认收到了该事件并将其写入relay-log之后，才会返回。这样即使master当机，最少也有一个slave获取到了完整的数据。</p>
<p>但是，semi-synchronous并不是100%的保证数据不会丢失，如果master在完成事务并将其发送给slave的时候崩溃，仍然可能造成数据丢失。只是相比于传统的异步复制，semi-synchronous replication能极大地提升数据安全。更为重要的是，它并不慢，MHA的作者都说他们在facebook的生产环境中使用了semi-synchronous（<a href="http://yoshinorimatsunobu.blogspot.ca/2014/04/semi-synchronous-replication-at-facebook.html" target="_blank" rel="external">这里</a>），所以我觉得真心没必要担心它的性能问题，除非你的业务量级已经完全超越了facebook或者google。</p>
<p>如果真的想完全保证数据不会丢失，现阶段一个比较好的办法就是使用<a href="http://galeracluster.com/" target="_blank" rel="external">gelera</a>，一个MySQL集群解决方案，它通过同时写三份的策略来保证数据不会丢失。笔者没有任何使用gelera的经验，只是知道业界已经有公司将其用于生产环境中，性能应该也不是问题。但gelera对MySQL代码侵入性较强，可能对某些有代码洁癖的同学来说不合适了:-)</p>
<p>我们还可以使用<a href="http://drbd.linbit.com/" target="_blank" rel="external">drbd</a>来实现MySQL数据复制，MySQL官方文档有一篇文档有详细<a href="http://dev.mysql.com/doc/refman/5.6/en/ha-drbd.html" target="_blank" rel="external">介绍</a>，但笔者并未采用这套方案，MHA的作者写了一些采用drdb的问题，<a href="https://code.google.com/p/mysql-master-ha/wiki/Other_HA_Solutions#Pacemaker_+_DRBD" target="_blank" rel="external">在这里</a>，仅供参考。</p>
<p>在后续的项目中，笔者会优先使用semi-synchronous replication的解决方案，如果数据真的非常重要，则会考虑使用gelera。</p>
<h2 id="Monitor">Monitor</h2>
<p>前面我们说了使用replication机制来保证master当机之后尽可能的数据不丢失，但是我们不能等到master当了几分钟才知道出现问题了。所以一套好的监控工具是必不可少的。</p>
<p>当master当掉之后，monitor能快速的检测到并做后续处理，譬如邮件通知管理员，或者通知守护程序快速进行failover。</p>
<p>通常，对于一个服务的监控，我们采用keepalived或者heartbeat的方式，这样当master当机之后，我们能很方便的切换到备机上面。但他们仍然不能很即时的检测到服务不可用。笔者的公司现阶段使用的是keepalived的方式，但后续笔者更倾向于使用zookeeper来解决整个MySQL集群的monitor以及failover。</p>
<p>对于任何一个MySQL实例，我们都有一个对应的agent程序，agent跟该MySQL实例放到同一台机器上面，并且定时的对MySQL实例发送ping命令检测其可用性，同时该agent通过ephemeral的方式挂载到zookeeper上面。这样，我们可以就能知道MySQL是否当机，主要有以下几种情况：</p>
<ol>
<li>机器当机，这样MySQL以及agent都会当掉，agent与zookeeper连接自然断开</li>
<li>MySQL当掉，agent发现ping不通，主动断开与zookeeper的连接</li>
<li>Agent当掉，但MySQL未当</li>
</ol>
<p>上面三种情况，我们都可以认为MySQL机器出现了问题，并且zookeeper能够立即感知。agent与zookeeper断开了连接，zookeeper触发相应的children changed事件，监控到该事件的管控服务就可以做相应的处理。譬如如果是上面前两种情况，管控服务就能自动进行failover，但如果是第三种，则可能不做处理，等待机器上面crontab或者supersivord等相关服务自动重启agent。</p>
<p>使用zookeeper的好处在于它能很方便的对整个集群进行监控，并能即时的获取整个集群的变化信息并触发相应的事件通知感兴趣的服务，同时协调多个服务进行相关处理。而这些是keepalived或者heartbeat做不到或者做起来太麻烦的。</p>
<p>使用zookeeper的问题在于部署起来较为复杂，同时如果进行了failover，如何让应用程序获取到最新的数据库地址也是一个比较麻烦的问题。</p>
<p>对于部署问题，我们要保证一个MySQL搭配一个agent，幸好这年头有了docker，所以真心很简单。而对于第二个数据库地址更改的问题，其实并不是使用了zookeeper才会有的，我们可以通知应用动态更新配置信息，或者使用proxy来解决。</p>
<p>虽然zookeeper的好处很多，但如果你的业务不复杂，譬如只有一个master，一个slave，zookeeper可能并不是最好的选择，没准keepalived就够了。</p>
<h2 id="Failover">Failover</h2>
<p>通过monitor，我们可以很方便的进行MySQL监控，同时在MySQL当机之后通知相应的服务做failover处理，假设现在有这样的一个MySQL集群，a为master，b，c为其slave，当a当掉之后，我们需要做failover，那么我们选择b，c中的哪一个作为新的master呢？</p>
<p>原则很简单，哪一个slave拥有最近最多的原master数据，就选哪一个作为新的master。我们可以通过<code>show slave status</code>这个命令来获知哪一个slave拥有最新的数据。我们只需要比较两个关键字段<code>Master_Log_File</code>以及<code>Read_Master_Log_Pos</code>，这两个值代表了slave读取到master哪一个binlog文件的哪一个位置，binlog的索引值越大，同时pos越大，则那一个slave就是能被提升为master。这里我们不讨论多个slave可能会被提升为master的情况。</p>
<p>在前面的例子中，假设b被提升为master了，我们需要将c重新指向新的master b来开始复制。我们通过<code>CHANGE MASTER TO</code>来重新设置c的master，但是我们怎么知道要从b的binlog的哪一个文件，哪一个position开始复制呢？</p>
<h3 id="GTID">GTID</h3>
<p>为了解决这一个问题，MySQL 5.6之后引入了GTID的概念，即uuid:gid，uuid为MySQL server的uuid，是全局唯一的，而gid则是一个递增的事务id，通过这两个东西，我们就能唯一标示一个记录到binlog中的事务。使用GTID，我们就能非常方便的进行failover的处理。</p>
<p>仍然是前面的例子，假设b此时读取到的a最后一个GTID为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:23</code>，而c的为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:15</code>，当c指向新的master b的时候，我们通过GTID就可以知道，只要在b中的binlog中找到GTID为<code>3E11FA47-71CA-11E1-9E33-C80AA9429562:15</code>这个event，那么c就可以从它的下一个event的位置开始复制了。虽然查找binlog的方式仍然是顺序查找，稍显低效暴力，但比起我们自己去猜测哪一个filename和position，要方便太多了。</p>
<p>google很早也有了一个<a href="https://code.google.com/p/google-mysql-tools/wiki/GlobalTransactionIds" target="_blank" rel="external">Global Transaction ID</a>的补丁，不过只是使用的一个递增的整形，<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">LedisDB</a>就借鉴了它的思路来实现failover，只不过google貌似现在也开始逐步迁移到MariaDB上面去了。</p>
<p>MariaDB的GTID实现跟MySQL 5.6是不一样的，这点其实比较麻烦，对于我的MySQL工具集<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>来说，意味着要写两套不同的代码来处理GTID的情况了。后续是否支持MariaDB再看情况吧。</p>
<h3 id="Pseudo_GTID">Pseudo GTID</h3>
<p>GTID虽然是一个好东西，但是仅限于MySQL 5.6+，当前仍然有大部分的业务使用的是5.6之前的版本，笔者的公司就是5.5的，而这些数据库至少长时间也不会升级到5.6的。所以我们仍然需要一套好的机制来选择master binlog的filename以及position。</p>
<p>最初，笔者打算研究<a href="https://code.google.com/p/mysql-master-ha/" target="_blank" rel="external">MHA</a>的实现，它采用的是首先复制relay log来补足缺失的event的方式，但笔者不怎么信任relay log，同时加之MHA采用的是perl，一个让我完全看不懂的语言，所以放弃了继续研究。</p>
<p>幸运的是，笔者遇到了<a href="https://github.com/outbrain/orchestrator" target="_blank" rel="external">orchestrator</a>这个项目，这真的是一个非常神奇的项目，它采用了一种<a href="http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid" target="_blank" rel="external">Pseudo GTID</a>的方式，核心代码就是这个</p>
<pre><code>create database if not exists meta;

drop event if exists meta.create_pseudo_gtid_view_event;

delimiter ;;
create event if not exists
  meta.create_pseudo_gtid_view_event
  on schedule every 10 second starts current_timestamp
  on completion preserve
  enable
  do
    begin
      set @pseudo_gtid := uuid();
      set @_create_statement := concat(&#39;create or replace view meta.pseudo_gtid_view as select \&#39;&#39;, @pseudo_gtid, &#39;\&#39; as pseudo_gtid_unique_val from dual&#39;);
      PREPARE st FROM @_create_statement;
      EXECUTE st;
      DEALLOCATE PREPARE st;
    end
;;

delimiter ;

set global event_scheduler := 1;
</code></pre><p>它在MySQL上面创建了一个事件，每隔1s，就将一个uuid写入到一个view里面，而这个是会记录到binlog中的，虽然我们仍然不能像GTID那样直接定位到一个event，但也能定位到一个1s的区间了，这样我们就能在很小的一个区间里面对比两个MySQL的binlog了。</p>
<p>继续上面的例子，假设c最后一次出现uuid的位置为s1，我们在b里面找到该uuid，位置为s2，然后依次对比后续的event，如果不一致，则可能出现了问题，停止复制。当遍历到c最后一个binlog event之后，我们就能得到此时b下一个event对应的filename以及position了，然后让c指向这个位置开始复制。</p>
<p>使用Pseudo GTID需要slave打开<code>log-slave-update</code>的选项，考虑到GTID也必须打开该选项，所以个人感觉完全可以接受。</p>
<p>后续，笔者自己实现的failover工具，将会采用这种Pseudo GTID的方式实现。</p>
<p>在《MySQL High Availability》这本书中，作者使用了另一种GTID的做法，每次commit的时候，需要在一个表里面记录gtid，然后就通过这个gtid来找到对应的位置信息，只是这种方式需要业务MySQL客户端的支持，笔者不很喜欢，就不采用了。</p>
<h2 id="后记">后记</h2>
<p>MySQL HA一直是一个水比较深的领域，笔者仅仅列出了一些最近研究的东西，有些相关工具会尽量在<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>中实现。</p>
]]></content>
    
    
      <category term="mysql" scheme="http://siddontang.com/tags/mysql/"/>
    
      <category term="mysql" scheme="http://siddontang.com/categories/mysql/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的2014]]></title>
    <link href="http://siddontang.com/2014/12/27/my-2014/"/>
    <id>http://siddontang.com/2014/12/27/my-2014/</id>
    <published>2014-12-27T05:21:03.000Z</published>
    <updated>2014-12-27T05:21:38.000Z</updated>
    <content type="html"><![CDATA[<p>每到年末，都要进行一次总结了，看看今年都做了哪些事情，有啥提高，明年目标是什么样的，大概计划怎样。</p>
<h2 id="家人">家人</h2>
<p>今年对我来说最大的两件事情就是我奶奶的离世以及我女儿的出生。我奶奶是在正月初一走的，可惜我没在旁边，因为要照顾快分娩的老婆。得到消息之后，立刻定了第二天的机票赶回老家送了奶奶最后一程。我爷爷是在五十年前的大年三十去世的，整整五十年零一天，真的像是冥冥中自有天注定似的。家里面突然少了一个人，陡然觉得心里空荡荡了许多。</p>
<p>在奶奶去世之后的二十五天，我的女儿出生了，家里面又多了一个人，当了爸爸，责任重了，压力大了。要考虑多多赚钱给她更好的生活了。</p>
<p>养育孩子真的是一件挺辛苦的事情，这里真的要感谢一下我的老婆，真的辛苦了。</p>
<p>等我女儿大了懂事了，我也会跟她说说她太奶奶的事情，虽然离她很遥远，但这是我宝贵的回忆。</p>
<h2 id="技术">技术</h2>
<p>今年我仍然继续着我的开源之路，比较欣慰的是弄的<a href="https://github.com/siddontang/ledisdb" target="_blank" rel="external">LedisDB</a>终于算是小有成功，被用在生产环境中，而且还有一些国外用户使用。不过比较让我郁闷的是这个东西在国内收到很多嘲讽，谩骂。后来跟一些中国其他开源作者交流了，发现几乎也都是这样。想想我们的心态也还真是好。</p>
<p>除了NoSQL，我还尝试在MySQL领域耕耘，于是就有了一个MySQL proxy，<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">mixer</a>，这个项目其实我很看好前景的，并且也有了很多关注，只是因为一些其他方面的原因我没有继续开发了。不过后来一些用户告诉我他们正在使用mixer里面的一些代码进行自己的proxy开发，这让我感到很欣慰。弄开源，最想的其实就是有人使用，得到认可。</p>
<p>当然今年还做了很多一些小的开源组件，其实也都是一直围绕着NoSQL，MySQL上面来做的。</p>
<p>今年在技术上面最大的感触就是要走出去，跟人多交流，站在别人的肩膀上面。自己造轮子固然好，但是有时候基于别人的好的东西再搭积木，没准更好。一个最好的例子就是<a href="https://github.com/siddontang/xcodis" target="_blank" rel="external">xcodis</a>，我一直想让LedisDB支持集群，然后也想到使用proxy的方式，只是一直没时间去弄，这时候豌豆荚开源了codis，而这个就是我需要的东西，于是我在codis的基础上面直接弄了一个支持LedisDB的集群proxy出来。</p>
<p>今年为了提升自己的算法水平，蛋疼的把leetcode上面的题目全做了一遍，虽然一些是google出来的，但也至少让自己学到了很多。现在也正在写一本<a href="https://www.gitbook.io/book/siddontang/leetcode-solution/" target="_blank" rel="external">Leetcode题解</a>的书，希望明年能搞定,当然不是为了出版，只是为了更好的提升自己的算法水平。</p>
<h2 id="分享">分享</h2>
<p>今年秉着走出去的原则，参加了两场分享会，一场是在腾讯的技术沙龙讲LedisDB，不过自己演讲水平太挫，效果特差。另一场是在珠海北理工讲web service的开发，忽悠了半天，后来发现，现在的学生盆友重点关注的是iOS，android这些的东西。</p>
<p>这两场分享让我锻炼了一下口才，其实还蛮不错的，如果明年有机会，也争取参加一下。</p>
<h2 id="工作">工作</h2>
<p>今年的工作主要集中在推送服务器以及go服务重构上面，当时写push，设想的是能给整个公司提供推送服务，只是计划没有变化快，最后也成了一个鸡肋产品。</p>
<p>以前我们的服务是用openresty + python，随着系统越来越复杂，这套架构也有了很多弊端，所以我们决定使用go完全重构，现在仍在进行中。</p>
<p>今年整个公司变动挺大的，我的老大，一个11年的员工也出去创业了。想想比较唏嘘，当初面试我的两个老大，把我拉进来的都走了。</p>
<p>明年我的担子比较重，因为go的重构是我负责，明年铁定要上的，到时候很多问题都需要我来协调处理了。</p>
<h2 id="展望">展望</h2>
<p>扯了这么多，其实发现2014年也没做什么，但就这么快速的过去了，那2015年了，干点啥呢：</p>
<ul>
<li>育儿，这个发现真挺重要的，孩子马上一岁了，逐渐懂事了，所以铁定要多花时间陪孩子了。</li>
<li>继续开源，仍然用go开发，还是NoSQL和MySQL方向，没准MQ也会有。重点完成mysql proxy的后续开发。</li>
<li>练习英语，现在跟外国盆友交流只敢打字，说话什么的那是不可能的。所以这方面一定要加强。奶爸推荐的一些学习方法还是挺不错的，要强迫自己学习。另外，争取看完全英文的哈利波特，或者其他入门级的英文读物。</li>
<li>完成LeetCode题解这本书，前面已经说了，这是对我自己一段时间算法学习的总结。</li>
<li>争取每周一篇文章，技术的，人文的都可以，今年零零散散写了一点，但还是不够。明年争取能在medium上面用英文多写几篇文章，今年只写了可怜的两篇。</li>
<li>docker，这玩意现在太火了，我也正在推进开发中使用docker，不过没准能在生产环境中也用到，需要好好研究。</li>
<li>深入学习网络以及Linux底层知识，这方面要加强，现在只是知道大概，稍微深入一点就不懂了，做高性能服务器开发，得掌握。</li>
<li>读书，不是技术书籍，争取每月读一本好书，小说也行，哲学，经济，历史都成。</li>
<li>锻炼，好吧，现在身体太差，不能这样懒了。跑步也行，游泳也成，总之要动起来了。</li>
</ul>
<h2 id="最后">最后</h2>
<p>2014年就快过去了，马上迎来2015，希望明年越来越好。</p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习zookeeper]]></title>
    <link href="http://siddontang.com/2014/12/03/learn-zookeeper/"/>
    <id>http://siddontang.com/2014/12/03/learn-zookeeper/</id>
    <published>2014-12-03T12:49:37.000Z</published>
    <updated>2014-12-03T12:51:23.000Z</updated>
    <content type="html"><![CDATA[<p>最近研究了一下zookeeper（后续以zk简称），对于一个自认为泡在服务器领域多年的老油条来说，现在才开始关注zk这个东西，其实有点晚了，但没办法，以前的工作经历让我压根用不到这个玩意。只是最近因为要考虑做ledisdb的cluster方案，以及重新考虑mixer的协调管理，才让我真正开始尝试去了解zk。</p>
<h2 id="什么是zookeeper">什么是zookeeper</h2>
<p>根据官网的介绍，zookeeper是一个分布式协调服务，主要用来处理分布式系统中各系统之间的协作问题的。</p>
<p>其实这么说有点抽象，初次接触zk，很多人真不知道用它来干啥，你可以将它想成一个总控节点（当然它能用多机实现自身的HA），能对所有服务进行操作。这样就能实现对整个分布式系统的统一管理。</p>
<p>譬如我现在有n台机器，需要动态更新某一个配置，一些做法可能是通过puppet或者salt将配置先分发到不同机器，然后运行指定的reload命令。zk的做法可能是所有服务都监听一个配置节点，直接更改这个节点的数据，然后各个服务就能收到更新消息，然后同步最新的配置，再自行reload了。</p>
<p>上面只是一个很简单的例子，其实通过它并不能过多的体现zk的优势（没准salt可能还更简单），但zk不光只能干这些，还能干更awesome的事情。网上有太多关于zk应用场景一览的文章了，这里就不详细说明，后续我只会说一下自己需要用zk解决的棘手问题。</p>
<h2 id="架构">架构</h2>
<p>zk使用类paxos算法来保证其HA，每次通过选举得到一个master用来处理client的请求，client可以挂载到任意一台zk server上面，因为paxos这种是强一致同步算法，所以zk能保证每一台server上面数据都是一致的。架构如下：</p>
<pre><code>                                                                     
                      +-------------------------------+                         
                      |                               |                         
              +----+--++          +----+---+        +-+--+---+                  
              | server |          | server |        | server |                  
              |        +----------+ master +--------+        |                  
              +--^--^--+          +----^---+        +----^---+                  
                 |  |                  |                 |                      
                 |  |                  |                 |                      
                 |  |                  |                 |                      
           +-----+  +-----+            +------+          +---------+            
           |              |                   |                    |            
           |              |                   |                    |            
      +----+---+        +-+------+         +--+-----+           +--+-----+      
      | client |        | client |         | client |           | client |      
      +--------+        +--------+         +--------+           +--------+
</code></pre><h2 id="Data_Model">Data Model</h2>
<p>zk内部是按照类似文件系统层级方式进行数据存储的，就像这样：</p>
<pre><code>                        +---+             
                        | / |             
                        +++-+             
                         ||               
                         ||               
          +-------+------++----+-------+  
          | /app1 |            | /app2 |  
          +-+--+--+            +---+---+  
            |  |                   |      
            |  |                   |      
            |  |                   |      
+----------++ ++---------+    +----+-----+
| /app1/p1 |  | /app1/p2 |    | /app2/p1 |
+----------+  +----------+    +----------+
</code></pre><p>对于任意一个节点，我们称之为znode，znode有很多属性，譬如<code>Zxid</code>（每次更新的事物ID）等，具体可以详见zk的文档。znode有ACL控制，我们可以很方便的设置其读写权限等，但个人感觉对于内网小集群来说意义不怎么大，所以也就没深入研究。</p>
<p>znode有一种Ephemeral Node，也就是临时节点，它是session有效的，当session结束之后，这个node自动删除，所以我们可以用这种node来实现对服务的监控。譬如一个服务启动之后就向zk挂载一个ephemeral node，如果这个服务崩溃了，那么连接断开，session无效了，这个node就删除了，我们也就知道该服务出了问题。</p>
<p>znode还有一种Sequence Node，用来实现序列化的唯一节点，我们可以通过这个功能来实现一个简单地leader服务选举，譬如每个服务启动的时候都向zk注册一个sequence node，谁最先注册，zk给的sequence最小，这个最小的就是leader了，如果leader当掉了，那么具有第二小sequence node的节点就成为新的leader。</p>
<h3 id="Znode_Watch">Znode Watch</h3>
<p>我们可以watch一个znode，用来监听对应的消息，zk会负责通知，但只会通知一次。所以需要我们再次重新watch这个znode。那么如果再次watch之前，znode又有更新了，client不是收不到了吗？这个就需要client不光要处理watch，同时也需要适当的主动get相关的数据，这样就能保证得到最新的消息了。也就是消息系统里面典型的推拉结合的方式。推只是为了提升性能，快速响应，而拉则为了更好的保证消息不丢失。</p>
<p>但是，我们需要注意一点，zk并不能保证client收到消息之后同时处理，譬如配置文件更新，zk可能通知了所有client，但client并不能全部在同一个时间同时reload，所以为了处理这样的问题，我们需要额外的机制来保证，这个后续说明。</p>
<p>watch只能应用于data（通过get，exists函数）以及children（通过getChildren函数）。也就是监控znode数据更新以及znode的子节点的改变。</p>
<h2 id="API">API</h2>
<p>zk的API时很简单的，如下：</p>
<ul>
<li>create</li>
<li>delete</li>
<li>exists</li>
<li>set data</li>
<li>get data</li>
<li>get chilren</li>
<li>sync</li>
</ul>
<p>就跟通常的文件系统操作差不多，就不过多说明了。</p>
<h2 id="Example">Example</h2>
<p>总的来说，如果我们不深入zk的内部实现，譬如paxos等，zk还是很好理解的，而且使用起来很简单。通常我们需要考虑的就是用zk来干啥，而不是为了想引入一个牛的新特性而用zk。</p>
<h3 id="Lock">Lock</h3>
<p>用zk可以很方便的实现一个分布式lock，记得最开始做企业群组盘的时候，我需要实现一个分布式lock，然后就用redis来弄了一个，其实当时就很担心redis单点当掉的问题，如果那时候我就引入了zk，可能就没这个担心了。</p>
<p>官方文档已经很详细的给出了lock的实现流程：</p>
<ol>
<li>create一个类似path/lock-n的临时序列节点</li>
<li>getChilren相应的path，注意这里千万不能watch，不然惊群很恐怖的</li>
<li>如果1中n是最小的，则获取lock</li>
<li>否则，调用exists watch到上一个比自己小的节点，譬如我现在n是5，我就可能watch node-4</li>
<li>如果exists失败，表明前一个节点没了，则进入步骤2，否则等待，直到watch触发重新进入步骤2</li>
</ol>
<h3 id="Codis">Codis</h3>
<p>最近在考虑ledisdb的cluster方案，本来也打算用proxy来解决的，然后就在想用zk来处理rebalance的问题，结果这时候codis横空出世，发现不用自己整了，于是就好好的研究了一下codis的数据迁移问题。其实也很简单：</p>
<ol>
<li>config发起pre migrate action</li>
<li>proxy接收到这个action之后，将对应的slot设置为pre migrate状态，同时等待config发起migrate action</li>
<li>config等待所有的proxy返回pre migrate之后，发起migrate action</li>
<li>proxy收到migrate action，将对应的slot设置为migrate状态</li>
</ol>
<p>上面这些，都是通过zk来完成的，这里需要关注一下为啥要有pre migrate这个状态，如果config直接发起migrate，那么zk并不能保证proxy同一时间全部更新成migrate状态，所以我们必须有一个中间状态，在这个中间状态里面，proxy对于特定的slot不会干任何事情，只能等待config将其设置为migrate。虽然proxy对于相应slot一段时间无法处理外部请求，但这个时间是很短的（不过此时config当掉了就惨了）。config知道所有proxy都变成pre migrate状态之后，就可以很放心的发送migrate action了。因为这时候，proxy只有两种可能，变成migrate状态，能正常工作，仍然还是pre migrate状态，不能工作，也自然不会对数据造成破坏。</p>
<p>其实上面也就是一个典型的2PC，虽然仍然可能有隐患，譬如config当掉，但并不会对实际数据造成破坏。而且config当掉了我们也能很快知晓并重新启动，所以问题不大。</p>
<h2 id="总结">总结</h2>
<p>总的来说，zk的使用还是挺简单的，只要我们知道它到底能用到什么地方，那zk就真的是分布式开发里面一把瑞士军刀了。不过我挺不喜欢装java那套东西，为了zk也没办法，虽然go现在也有etcd这些类zk的东西了，但毕竟还没经受过太多的考验，所以现在还是老老实实的zk吧。</p>
]]></content>
    
    
      <category term="zookeeper" scheme="http://siddontang.com/tags/zookeeper/"/>
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[深入浅出Web Service]]></title>
    <link href="http://siddontang.com/2014/10/16/head-first-web-service/"/>
    <id>http://siddontang.com/2014/10/16/head-first-web-service/</id>
    <published>2014-10-16T14:39:47.000Z</published>
    <updated>2014-10-16T14:45:13.000Z</updated>
    <content type="html"><![CDATA[<p>应<a href="http://my.oschina.net/javayou" target="_blank" rel="external">红薯</a>的邀约，决定给某大学的童鞋讲讲Web Service相关知识，鉴于是第一次在学校献丑，所以还是老老实实的准备，先把类似逐字稿的东西写出来，然后在准备PPT吧。</p>
<p>关于Web service，这个话题太广太泛，加之我也只熟悉一些特定的领域，所以准备从两方面入手，1，什么是Web service，就当是概念性的介绍，让大家有个相关认识。2，则是根据一个简单的例子，告诉大家如何构建一个Web service服务。</p>
<h2 id="什么是Web_service">什么是Web service</h2>
<p>首先根据Wiki的定义：<strong>A Web Service is a method of communication between two electronic devices over a network.</strong> </p>
<p>简单来说，Web Service就是基于网络不同设备之间互相通信的一种方式。Web Service是一个软件服务，它提供很多API，而客户端通过Web协议进行调用从而完成相关的功能。</p>
<p>Web service并不是一个新奇的概念，相反从很早的分布式计算，到网格计算，到现在的云，都或多或少的有着Web service的影子。只不过随着近几年一浪高过一浪的互联网热潮以及Google，Amazon等公司的大力推动，Web service变得愈发流行。</p>
<p>越来越多的公司开始提供Web service，而同时又有更多的公司基于这些Web service提供了更加上层的Web service。</p>
<p>Amazon的S3（Simple Storage Service）是一个文件存储服务，用户通过S3将文件存放到Amazon的服务器上面，Amazon负责保证该文件的安全（包括不被别人获取，不丢失等）。而Drew Houston则在S3的基础上，构造了一个令人惊奇的同步网盘：Dropbox，同时，Dropbox又将相关API提供出去，供其他的Application其同步服务。</p>
<p>可以看到，正是因为有了越来越多的Web services，才让我们现在的互联网生活变得越发精彩。</p>
<h2 id="实现一个简单的Web_service">实现一个简单的Web service</h2>
<p>好了，上面扯了这么多，是不是心痒痒想自己开发一个Web service？开发一个Web service并不是那么容易的事情，尤其是涉及到分布式之后。不过我觉得一个小例子没准就能说明很多东西。当然我自认并不是Web service的专家（这年头专家架构师太多，我只能算打酱油的），很多东西难免疏漏，并且一些设计也会带有很强烈的个人色彩，如果大家有啥更好的认识，欢迎跟我讨论（妹子优先！）。</p>
<p>一个简单的例子：KV Storage Service，后面就叫KV吧。类似于S3，只是我们不是存文件，而是元数据。后面我们就用KV来表明服务的名字吧。</p>
<p>对于KV来说，它只会涉及到三种操作，如果用代码表示如下：</p>
<pre><code>//根据指定的key获取对应的value
Get(key)

//设置key的值为value，如果key本来存在，则更新，否则新建
Put(key, value)

//删除key
Delete(key)
</code></pre><h3 id="交互协议">交互协议</h3>
<p>既然是Web service，自然选用HTTP来做交互，比起自己实现一套不通用的协议，或者使用Google Protocol Buffers这些的，HTTP具有太多的优势，虽然它的性能稍微有点差，数据量稍微有点臃肿，但几乎所有的浏览器以及数不清的库能直接支持，想想都有点小激动了。</p>
<p>所以我们唯一要做的，就是设计好我们的API，让外面更方便的使用Web service。</p>
<h3 id="API">API</h3>
<p>根据wiki的定义，Web service通常有两种架构方式，RESTful和Arbitrary（RPC，SOAP，etc）。</p>
<p>REST是Representational state transfer的缩写，而满足REST架构模型的我们通常称之为Restful：</p>
<ul>
<li>使用URI来表示资源，譬如<code>http://example.com/user/1</code> 代表ID为1的user。</li>
<li>使用标准HTTP方法GET，POST，PUT，DELETE等来操作资源，譬如<code>Get http://example.com/user/1</code> 来获取user 1的信息，而使用<code>Delete http://example.com/user/1</code> 来删除user 1。</li>
<li>支持资源的多种表现形式，譬如上例Get中设置Content-Type为json，让服务端返回json格式的user信息。</li>
</ul>
<p>相对于Restful，另一种则是Arbitrary的，我不熟悉SOAP，这里就以RPC为例。</p>
<p>RPC就是remote procedure call，它通过在HTTP请求中显示的制定需要调用的过程名字以及参数来与服务端进行交互。仍然是上面的例子，如果我们需要得到用户的信息，可能就是这样 <code>Get http://example.com/getuser?userid=1</code>, 如果要删除一个用户，没准是这样<code>Get http://example.com/delUser?userid=1</code>。</p>
<p>那选择何种架构呢？在这里，我倾向使用Restful架构模型，很大原因在于它理解起来很容易，而且实现简单，而现在越来越多的Web service提供的API采用的是Restful模式，从另一个方面也印证了它的流行。</p>
<p>所以这个Web service的接口就是这样了：</p>
<pre><code>GET http://kv.com/key
DELETE http://kv.com/key
POST http://kv.com/key -dvalue
PUT http://kv.com/key -dvalue
</code></pre><p>上面POST和PUT可以等价，如果key存在，则用value覆盖，不存在则新建。</p>
<h3 id="架构">架构</h3>
<p>好了，扯了这么多，我们也要开始搭建我们的Web service了。因为采用的是HTTP协议，所以我们可以直接使用现成的HTTP server来帮我们处理HTTP请求。譬如nginx，apache，不过用go或者python直接写一个也不是特别困难的事情。</p>
<p>我们还需要一个storage server用来存放key-value，mysql可以，redis也行，或者我的<a href="http://ledisdb.com" target="_blank" rel="external">ledisdb</a>，谁叫红薯说可以打广告的。</p>
<p>最开始，我们就只有一台机器，启动一个nginx用来处理HTTP请求，然后启动一个ledisdb用来存放数据。然后开始对外happy的提供服务了。</p>
<p>KV开始工作的很好，突然有一天，我们发现随着用户量的增大，一台机器处理不过来了。好吧，我们在加一台机器，将nginx和ledisdb放到不同的机器上面。</p>
<p>可是好景不长，用户量越来越多，压力越来越大，我们需要再加机器了，因为nginx是一个无状态的服务，所以我们很容易的将其扩展到多台机器上面去运行，最外层通过DNS或者LVS来做负载均衡。但是对于有状态的服务，譬如上面的ledisdb，可不能这么简单的处理了。好吧，我们终于要开始扯到分布式了。</p>
<h3 id="CAP">CAP</h3>
<p>在聊分布式之前，我们需要知道CAP定理，因为在设计分布式系统的时候，CAP都是必须得面对的。</p>
<ul>
<li>Consistency，一致性</li>
<li>Avaliability，可用性</li>
<li>Partition tolerance，分区容忍性</li>
</ul>
<p>CAP的核心就在于在分布式系统中，你不可能同时满足CAP，而只能满足其中两项，但在分布式中，P是铁定存在的，所以我们设计系统的时候就需要在C和A之间权衡。</p>
<p>譬如，对于MySQL,它最初设计的时候就没考虑分区P，所以很好的满足CA，所以做过MySQL proxy方面工作的童鞋应该都清楚，要MySQL支持分布式是多么的蛋疼。</p>
<p>而对于一般的NoSQL，则是倾向于采用AP，但并不是说不管C，只是允许短时间的数据不一致，但能达到最终一致。</p>
<p>而对于需要强一致的系统，则会考虑牺牲A来满足CP，譬如很多系统必须写多份才算成功，</p>
<h3 id="Replication">Replication</h3>
<p>对于前面提到的Ledisdb，因为涉及到数据存放，本着不要把鸡蛋放到一个篮子里面的原则，我们也不能将数据放到一台机器上面，不然当机了就happy了。而解决这个的办法就是replication。</p>
<p>熟悉MySQL或者Redis的童鞋对replication应该都不会陌生，它们的replication都采用的是异步的方式，也就是在一段时间内不满足数据一致性，但铁定能达到最终一致性。</p>
<p>但如果真想支持同步的replication，怎么办呢？谁叫我们容不得数据半点丢失。通常有几种做法：</p>
<ul>
<li>2PC，3PC</li>
<li>Paxos，Raft</li>
</ul>
<p>因为这方面的坑很深，就不在累述，不过我是很推崇Raft的，相比于2PC，3PC，以及Paxos，Raft足够简单，并且很好理解。有机会在说明吧。</p>
<h3 id="水平扩展">水平扩展</h3>
<p>好了，通过replication解决了ledisdb数据安全问题，但总有一天，一台机器顶不住了，我们要考虑将ledisdb的数据进行拆分到多台机器。通常做法如下：</p>
<ul>
<li>最简单的做法，hash(key) % num，num是机器的数量，但这种做法在添加或者删除机器的时候会造成rehash，导致大量的数据迁移。</li>
<li>一致性hash，它相对于传统的hash，在添加或者删除节点的时候，它能尽可能的少的进行数据迁移。不过终归还是有数据流动的。</li>
<li>路由映射表，不同于一致性hash，我们在外部自己负责维护一张路由表，这样添加删除节点的时候只需要更改路由表就可以了，相对于一致性hash，个人感觉更加可控。</li>
</ul>
<p>我个人比较喜欢预分配+路由表的方式来进行水平扩展，所谓预分配，就是首先我就将数据切分到n个（譬如1024）shard，开始这些shard可以在一个node里面，随着node的增加，我们只需要迁移相关的shard，同时更新路由表就可以了。这种方式个人感觉灵活性最好，但对程序员要求较高，需要写出能自动处理resharding的健壮代码。</p>
<p>好了，解决了replication，解决了水平扩展，很长一段时间我们都能happy，当然坑还是挺多的，遇到了慢慢再填吧。</p>
<h2 id="没有提到的关键点">没有提到的关键点</h2>
<ul>
<li>Cache，无论怎样，cache在服务器领域都是一个非常关键的东西，用好了cache，你的服务能处理更多的并发访问。facebook这篇paper <a href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf" target="_blank" rel="external">Scaling Memcache at Facebook</a>专门讲解了相关知识，那是绝对的干货。</li>
<li>消息队列，当并发量大了之后，光靠同步的API调用已经满足不了整个系统的性能需求，这时候就该MQ上场了，譬如RabbitMQ都是不错的选择。</li>
<li>很多很多其他的。。。。。。。这里就不列举了。</li>
</ul>
<h2 id="总结">总结</h2>
<p>上面只是我对于Web service一点浅显的见解，如果里面的知识稍微对你有用，那我已经感到非常高兴了。但就像实践才是检验真理的唯一标准一样，理论知道的再多，还不如先弄一个Web service来的实在，反正现在国内阿里云，腾讯云，百度云啥的都不缺，缺的只是跑在上面的好应用。</p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[LedisDB Replication设计]]></title>
    <link href="http://siddontang.com/2014/10/05/ledisdb-replication/"/>
    <id>http://siddontang.com/2014/10/05/ledisdb-replication/</id>
    <published>2014-10-05T13:58:59.000Z</published>
    <updated>2014-10-05T14:00:17.000Z</updated>
    <content type="html"><![CDATA[<p>对于使用SQL或者NoSQL的童鞋来说，replication都是一个避不开的话题，通过replication，能极大地保证你的数据安全性。毕竟谁都知道，不要把鸡蛋放在一个篮子里，同理，也不要把数据放到一台机器上面，不然机器当机了你就happy了。</p>
<p>在分布式环境下，对于任何数据存储系统，实现一套好的replication机制是很困难的，毕竟<a href="http://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="external">CAP</a>的限制摆在那里，我们不可能实现出一套完美的replication机制，只能根据自己系统的实际情况来设计和对CAP的取舍。</p>
<p>对于replication更详细的说明与解释，这里推荐<a href="http://book.mixu.net/distsys/index.html" target="_blank" rel="external">Distributed systems<br>for fun and profit</a>，后面，我会根据LedisDB的实际情况，详细的说明我在LedisDB里面使用的replication是如何实现的。</p>
<h2 id="BinLog">BinLog</h2>
<p>最开始的时候，Ledisdb采用的是类似MySQL通用binlog的replication机制，即通过binlog的filename + position来决定需要同步的数据。这套方式实现起来非常简单，但是仍然有一些不足，主要就在于hierarchical replication情况下如果master当掉，选择合适的slave提升为master是比较困难的。举个最简单的例子，假设A为master，B，C为slave，如果A当掉了，我们会在B，C里面选择同步数据最多的那个，但是是哪一个呢？这个问题，在MySQL的replication中也会碰到。</p>
<h2 id="MySQL_GTID">MySQL GTID</h2>
<p>在MySQL 5.6之后，引入了GTID（Global transaction ID）的概念来解决上述问题，它通过<code>Source:ID</code>的方式来在binlog里面表示一个唯一的transaction。Source为当前server的uuid，这个是全局唯一的，而ID则是该server内部的transaction ID（采用递增保证唯一）。具体到上面那个问题，采用GTID，如果A当掉了，我们只需要在B和C的binlog里面查找比较最后一个A这个uuid的transaction id的大小，譬如B的为uuid:10，而C的为uuid:30，那么铁定我们会选择C为新的master。</p>
<p>当然使用GTID也有相关的限制，譬如slave也必须写binlog等，但它仍然足够强大，解决了早期MySQL replication的时候一大摊子的棘手问题。但LedisDB并不准备使用，主要就在于应用场景没那么复杂的情况，我需要的是一个更加简单的解决方案。</p>
<h2 id="Google_Global_Transaction_ID">Google Global Transaction ID</h2>
<p>早在MySQL的GTID之前，google的一个MySQL版本就已经使用了<a href="https://code.google.com/p/google-mysql-tools/wiki/GlobalTransactionIds" target="_blank" rel="external">global transaction id</a>，在binlog里面，它对于任何的transaction，使用了group id来唯一标示。group id是一个全局的递增ID，由master负责维护生成。当master当掉之后，我们只需要看slave的binlog里面谁的group id最大，那么那一个就是能被选为master了。</p>
<p>可以看到，这套方案非常简单，但是限制更多，譬如slave端的binlog只能由replication thread写入，不支持Multi-Masters，不支持circular replication等。但我觉得它已经足够简单高效，所以LedisDB准备参考它来实现。</p>
<h2 id="Raft">Raft</h2>
<p>弄过分布式的童鞋应该都或多或少的接触过Paxos（至少我是没完全弄明白的），而<a href="http://raftconsensus.github.io/" target="_blank" rel="external">Raft</a>则号称是一个比Paxos简单得多的分布式一致性算法。</p>
<p>Raft通过replicated log来实现一致性，假设有A，B，C三台机器，A为Leader，B和C为follower，（其实也就是master和slave的概念）。A的任何更新，都必须首先写入Log（每个Log有一个LogID，唯一标示，全局递增），然后将其Log同步到至少Follower，然后才能在A上面提交更新。如果A当掉了，B和C重新选举，如果哪一台机器当前的LogID最大，则成为Leader。看到这里，是不是有了一种很熟悉的感觉？</p>
<p>LedisDB在支持consensus replication上面，参考了Raft的相关做法。</p>
<h2 id="名词解释">名词解释</h2>
<p>在详细说明LedisDB replication的实现前，有必要解释一些关键字段。</p>
<ul>
<li>LogID：log的唯一标示，由master负责生成维护，全局递增。</li>
<li>LastLogID：当前程序最新的logid，也就是记录着最后一次更新的log。</li>
<li>FirstLogID：当前程序最老的logid，之前的log已经被清除了。</li>
<li>CommitID：当前程序已经处理执行的log。譬如当前LastLogID为10，而CommitID为5，则还有6，7，8，9，10这几个log需要执行处理。如果CommitID = LastLogID，则证明程序已经处于最新状态，不再需要处理任何log了。</li>
</ul>
<h2 id="LedisDB_Replication">LedisDB Replication</h2>
<p>LedisDB的replication实现很简单，仍然是上面的例子，A，B，C三台机器，A为master，B和C为slave。</p>
<p>当master有任何更新，master会做如下事情：</p>
<ol>
<li>记录该更新到log，logid = LastLogID + 1，LastLogID = logid</li>
<li>同步该log到slaves，等待slaves的确认返回，或者超时</li>
<li>提交更新</li>
<li>更新CommitID = logid</li>
</ol>
<p>上面还需要考虑到错误处理的情况。</p>
<ul>
<li>如果1失败，记录错误日志，然后我们会认为该次更新操作失败，直接返回。</li>
<li>如果3失败，不更新CommitID返回，因为这时候CommitID小于LastLogID，master进入read only模式，replication thread尝试执行log，如果能执行成功，则更新CommitID，变成可写模式。</li>
<li>如果4失败，同上，因为LedisDB采用的是Row-Base Format的log格式，所以一次更新操作能够幂等多次执行。</li>
</ul>
<p>对于slave</p>
<p>如果是首次同步，则进入全同步模式：</p>
<ol>
<li>master生成一个snapshot，连同当前的LastLogID一起发送给slave。</li>
<li>slave收到该dump文件之后，load载入，同时更新CommitID为dump文件里面的LastLogID。</li>
</ol>
<p>然后进入增量同步模式，如果slave已经有相关log，则直接进入增量同步模式。</p>
<p>在增量模式下面，slave向master发送sync命令，sync的参数为下一个需要同步的log，如果slave当前没有binlog（譬如上面提到的全同步情况），则logid = CommitID + 1， 否则logid = LastLogID + 1。</p>
<p>master收到sync请求之后，有如下处理情况：</p>
<ul>
<li>sync的logid小于FirstLogID，master没有该log，slave收到该错误重新进入全同步模式。</li>
<li>master有该sync的log，于是将log发送给slave，slave收到之后保存，并再次发送sync获取下一个log，同时该次请求也作为ack告知master同步该log成功。</li>
<li>sync的log id已经大于LastLogID了，表明master和slave的状态已经到达一致，没有log可以同步了，slave将会等待新的log直到超时再次发送sync。</li>
</ul>
<p>在slave端，对于接受到的log，由replication thread负责执行，并更新CommitID。</p>
<p>如果master当机，我们只需要选择具有最大LastLogID的那个slave为新的master就可以了。</p>
<h2 id="Limitation">Limitation</h2>
<p>总的来说，这套replication机制很简单，易于实现，但是仍然有许多限制。</p>
<ul>
<li>不支持Multi-Master，因为同时只能有一个地方进行全局LogID的生成。不过我真的很少见到Multi-Master这样的架构模式，即使在MySQL里面。</li>
<li>不支持Circular-Replication，slave写入的log id不允许小于当前的LastLogID，这样才能保证只同步最新的log。</li>
<li>没有自动master选举机制，不过我觉得放到外部去实现更好。</li>
</ul>
<h2 id="Async/Sync_Replication">Async/Sync Replication</h2>
<p>LedisDB是支持强一致性的同步replication的，如果配置了该模式，那么master会等待slave同步完成log之后再提交更新，这样我们就能保证当master当机之后，一定有一台slave具有跟master一样的数据。但在实际中，可能因为网络环境等问题，master不可能一直等待slave同步完成log，所以通常都会有一个超时机制。所以从这点来看，我们仍然不能保证数据的强一致性。</p>
<p>使用同步replication机制会极大地降低master的写入性能，如果对数据一致性不敏感的业务，其实采用异步replication就可以了。</p>
<h2 id="Failover">Failover</h2>
<p>LedisDB现在没有自动的failover机制，master当机之后，我们仍然需要外部的干预来选择合适的slave（具有最大LastLogID那个），提升为master，并将其他slave重新指向该master。后续考虑使用外部的keeper程序来处理。而对于keeper的单点问题，则考虑使用raft或者zookeeper来处理。</p>
<h2 id="后记">后记</h2>
<p>虽然LedisDB现在已经支持replication，但仍然需要在生产环境中检验完善。</p>
<p>LedisDB是一个采用Go实现的高性能NoSQL，接口类似Redis，现在已经用于生产环境，欢迎大家使用。</p>
<p><a href="http://ledisdb.com" target="_blank" rel="external">Official Website</a></p>
<p><a href="http://github.com/siddontang/ledisdb" target="_blank" rel="external">Github</a></p>
]]></content>
    
    
      <category term="go" scheme="http://siddontang.com/tags/go/"/>
    
      <category term="ledisdb" scheme="http://siddontang.com/tags/ledisdb/"/>
    
      <category term="nosql" scheme="http://siddontang.com/tags/nosql/"/>
    
      <category term="go" scheme="http://siddontang.com/categories/go/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Reinventing the wheel?]]></title>
    <link href="http://siddontang.com/2014/08/12/reinventing-the-wheel/"/>
    <id>http://siddontang.com/2014/08/12/reinventing-the-wheel/</id>
    <published>2014-08-12T15:03:49.000Z</published>
    <updated>2014-08-12T15:04:31.000Z</updated>
    <content type="html"><![CDATA[<p><script async src="https://static.medium.com/embed.js"></script><a class="m-story" data-collapsed="true" href="https://medium.com/@siddontang/8241f1ba9068" target="_blank" rel="external">Reinventing the wheel?</a></p>
]]></content>
    
    
      <category term="program" scheme="http://siddontang.com/categories/program/"/>
    
  </entry>
  
</feed>
