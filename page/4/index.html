<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第 4 页 | Siddon&#39;s Blog</title>
  <meta name="author" content="SiddonTang">
  
  <meta name="description" content="My thought for program">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Siddon&#39;s Blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Siddon&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Siddon&#39;s Blog</a></h1>
  <h2><a href="/">My thought for program</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/Presentations">Presentations</a></li>
    
      <li><a href="/About">About</a></li>
    
      <li><a href="/atom.xml">RSS</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-03-05T06:26:04.000Z"><a href="/2014/03/05/libcoro/">03-05-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/03/05/libcoro/">简易的C++协程库</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="起因">起因</h1>
<p>在第一个版本的<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>开发完成之后，我一直在思考如何让异步方式的网络编程更加简单。</p>
<p>虽然<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>通过c++ shared_ptr以及function等技术很大程度上面解决了异步代码编写的一些问题，但是仍然会出现代码逻辑被强制拆分的情况。而这个则是项目中童鞋无法很好的使用其进行开发的原因。</p>
<p>所以我考虑让<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>支持coroutine。</p>
<h1 id="Coroutine">Coroutine</h1>
<p>第一次接触coroutine的概念是在lua里面，记得当时想了很久才算弄明白了coroutine的使用以及原理。在lua中，coroutine的使用如下：</p>
<pre><code>co = coroutine.create(function ()
        print(&quot;begin yield&quot;)
        coroutine.yield()
        print(&quot;after yield&quot;)
    end)

coroutine.resume(co)
print(&quot;after resume&quot;)

coroutine.resume(co)
</code></pre><p>我们可以通过resume执行一个新创建或者已经被挂起的coroutine，通过yield挂起当前的coroutine，这样就可以实现类似多线程方式下面的多任务调度。</p>
<p>至于coroutine的原理，很多地方都有说明，主要就在于每个coroutine都有自己的堆栈，这样当coroutine挂起的时候，它的当前执行状态会被完整保留，下次resume的时候就可以接着执行了。</p>
<p>而使用coroutine的好处，我觉得最大的一点在于它将拆分的异步逻辑同步化了，更利于代码编写。</p>
<p>在使用python tornado的时候，我们开始阶段写了太多的callback回调，以至于代码的维护非常困难，而这个则在引入<a href="https://github.com/python-greenlet/greenlet" target="_blank" rel="external">greenlet</a>后有了明显好转。</p>
<p>而后续在使用go语言中，因为它原生的支持coroutine（其实在go里面更准确的说法应该是goroutine），写代码非常的方便，所以现在go已经成为了我服务器的首选开发语言，我也用它开发了多个项目（如<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">mixer</a>，一个mysql proxy），并且已经在公司项目中实施。</p>
<p>当然，使用coroutine并不是毫无缺点的：</p>
<ul>
<li>每个coroutine都需要维护自己的堆栈，当我们需要创建数以百万计的coroutine的时候，内存的开销就需要考虑了。</li>
<li>coroutine的切换，都需要保留当前的上下文环境，以便于下次resume的时候接着执行，如果coroutine切换频繁，开销也不小。</li>
</ul>
<h1 id="libcoro">libcoro</h1>
<p>很早之前使用luajit的时候，我就知道可以在c++中实现coroutine的功能，在linux中，这通过makecontext，swapcontext等相关函数实现。虽然也可以通过setjmp/longjmp这两个古老的函数实现，但看了luajit的coco就知道，即使在linux下面，它也需要写很多define宏去适配。</p>
<p>所以，我只考虑使用makecontext这套函数族来实现coroutine。虽然swapcontext会有性能问题，详见<a href="http://rethinkdb.com/blog/making-coroutines-fast/" target="_blank" rel="external">这里</a>，但早期我还不打算对其进行性能优化。</p>
<p><a href="https://github.com/siddontang/libcoro" target="_blank" rel="external">libcoro</a>是一个简单的c++ coroutine库，只支持linux（因为我们的服务器只有linux的）。</p>
<p>在接口上面，<a href="https://github.com/siddontang/libcoro" target="_blank" rel="external">libcoro</a>参考的是lua的coroutine的接口设计，使用非常简单:</p>
<pre><code>void func1()
{
    coroutine.yield();
}

void func2(Coro_t co1)
{
    coroutine.resume(co1);    
    coroutine.yield();
}

void func()
{
    Coro_t co1 = coroutine.create(std::bind(&amp;func1));    
    coroutine.resume(co1);    
    Coro_t co2 = coroutine.create(std::bind(&amp;func2, co1));
    coroutine.resume(co2);
    coroutine.resume(co2);
}

int main()
{    
    Coro_t co = coroutine.create(std::bind(&amp;func));
    coroutine.resume(co);
    return 0;
}
</code></pre><ul>
<li>coroutine.create创建一个coroutine，参数为一个std::function，这样我们就可以通过std::bind非常方便的实现函数闭包了。</li>
<li>coroutine.resume唤醒一个挂起或者新建的coroutine。</li>
<li>coroutine.yield挂起当前coroutine。</li>
<li>coroutine.running获取当前运行的coroutine，如果是主线程调用，则返回0。</li>
<li>coroutine.status获取coroutine的状态。</li>
</ul>
<p>后续我考虑将<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>支持coroutine，不过这可能会成为一个新的网络库了。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-02-24T05:41:38.000Z"><a href="/2014/02/24/go-mysql-introduction/">02-24-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/02/24/go-mysql-introduction/">Go MySQL接口开发</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="介绍">介绍</h1>
<p><a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>是一个用go写的mysql driver，使用接口类似于go自身的database sql，但是稍微有一点不同，现阶段还不支持集成进go database/sql中，但实现难度并不大，后续可能会接入。</p>
<p><a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>最先开始于<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">mixer</a>（一个用go实现的mysql proxy）中，随着mixer的演化，我觉得有必要将其mysql模块独立出来使用。对于<a href="https://github.com/siddontang/mixer" target="_blank" rel="external">mixer</a>，后续我会详细介绍。</p>
<p>为什么要自己实现一套新的接口，而不是go自身的sql接口呢？最主要的原因在于我很不习惯使用Query的查询方式。go自身的query例子：</p>
<pre><code>age := 27
rows, err := db.Query(&quot;SELECT name FROM users WHERE age=?&quot;, age)
if err != nil {
    log.Fatal(err)
}
for rows.Next() {
    var name string
    if err := rows.Scan(&amp;name); err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&quot;%s is %d\n&quot;, name, age)
}
if err := rows.Err(); err != nil {
    log.Fatal(err)
}
</code></pre><p>可以看到，使用起来非常的繁琐复杂，如果代码里面select语句很多（恰恰我们代码里面n多select），那么如果每次select都要靠这种方式得到我们需要的结果，那我可能写代码会写崩溃的。所以势必我们需要提供一套封装用来简化select的结果集获取。</p>
<h1 id="Resultset">Resultset</h1>
<p><a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>跟go自身的sql接口最大的不一样在于Query的时候直接返回了一个resultset，而这个resultset定义如下：</p>
<pre><code>type Field struct {
    Name []byte
    Type uint8
    Flag uint16
}

type Resultset struct {
    Status uint16 //server status for this query resultset

    Fields     []Field
    FieldNames map[string]int

    Data [][]interface{}
}
</code></pre><p>Field用来表示查询的时候返回的数据每列的名字，数据类型以及一些特定的Flag。而resultset中的Data则是存放了query结果中对于的实际数据。因为对于mysql select来说，它返回的是一个<strong>“m x n”</strong>的结果集，我们直接使用[][]interface{}在go中表示。</p>
<p>Resultset提供了非常方便的接口用于select数据的获取：</p>
<pre><code>//指定某一行，某一列获取数据，结果为string
func (r *Resultset) GetString(row, column int) (string, error)
//执行某一行，某一列的名字获取数据，结果为string
func (r *Resultset) GetStringByName(row int, columnName int) (string, error)
</code></pre><h1 id="接口">接口</h1>
<p><a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>除了query之外，几乎提供了与go database/sql一样的接口使用方式：</p>
<pre><code>//创建一个db，最大允许保活16个空闲连接
//dsn格式为：&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;
db := NewDB(&quot;qing:admin@127.0.0.1:3306/mixer&quot;, 16)

//ping一下，看mysql server是不是还是活的
db.Ping()

//执行exec，包括insert,update,delete,replace
r, err := db.Exec(&quot;insert into mixer_conn (id, str) values (1, `abc`)&quot;)
println(r.LastInsertId(), r.RowsAffected())

//执行exec，语句中包含 ？占位符，需要传递相应的参数
r, err := db.Exec(&quot;insert into mixer_conn (id, str) values (?, ?)&quot;, 2, &quot;efg&quot;)
println(r.LastInsertId(), r.RowsAffected())

//执行select，得到结果集，并获取相关数据
r, err := db.Query(&quot;select str from mixer_conn where id     = 1&quot;)
str, _ = r.GetString(0, 0)
str, _ = r.GetStringByName(0, &quot;str&quot;)

//开始一个事物
tx, err = db.Begin()

//在事物里面执行语句
tx.Exec(&quot;insert into mixer_conn (id, str) values (3, `abc`)&quot;)

//提交事物
tx.Commit()

//创建一个prepare statement
s, err := db.Prepare(&quot;insert into mixer_conn (id, str) values(?, ?)&quot;)

//执行 prepare statement
s.Exec(5, &quot;abc&quot;)

//关闭 prepare statement
s.Close()
</code></pre><p>不光如此，<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>还提供单独获取一个conn，用于给外部额外使用的功能，譬如:</p>
<pre><code>conn, err := db.GetConn()

//我需要设置conn的字符集为gb2312
conn.SetCharset(&quot;gb2312&quot;)    

conn.Close()
</code></pre><p>可以看到，<a href="https://github.com/siddontang/go-mysql" target="_blank" rel="external">go-mysql</a>的使用非常简单，现阶段正逐渐在我们项目中使用，也非常期待您的反馈。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-02-21T01:51:40.000Z"><a href="/2014/02/21/golang-rpc-frame/">02-21-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/02/21/golang-rpc-frame/">使用go reflect实现一套简易的rpc框架</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="go_jsonrpc">go jsonrpc</h2>
<p>在实际项目中，我们经常会碰到服务之间交互的情况，如何方便的与远端服务进行交互，就是一个需要我们考虑的问题。</p>
<p>通常，我们可以采用restful的编程方式，各个服务提供相应的web接口，相互之间通过http方式进行调用。或者采用rpc方式，约定json格式进行数据交互。</p>
<p>在我们的项目中，服务端对用户客户端提供的是restful的接口方式，而在服务器内部，我们则采用rpc方式进行服务之间的交互。</p>
<p>go语言本来就提供了jsonrpc的支持，所以自然开始我们就直接使用jsonrpc。jsonrpc的使用非常简单，对于调用端来说，就如同一个函数调用，如下：</p>
<pre><code>args := &amp;Args{7, 8}
reply := new(Reply)
err := client.Call(&quot;Arith.Add&quot;, args, reply)
</code></pre><p>上面是go jsonrpc自带的一个例子，可以看到，虽然我们通过call(rpcName, inParams, outParams)这样的形式可以很方便的进行rpc的调用，但是跟go实际的函数调用还是稍微有一点区别，对我来说，这么使用总觉得很别扭。</p>
<h2 id="我觉得方便的rpc使用方式">我觉得方便的rpc使用方式</h2>
<p>对于go jsonrpc来说，它的调用格式是这样的</p>
<pre><code>err := call(name, in, out)
</code></pre><p>但是对我来说，我希望采用这样的调用方式：</p>
<pre><code>out, err := RpcFunc(in)
</code></pre><p>假设server端有如下的一个RPC函数，注册的rpc name为testrpc。</p>
<pre><code>func Test(id int) (int, error)
</code></pre><p>对于client端来说，我希望的使用方式是这样:</p>
<pre><code>var rpcTest func(id int) (int, error)
MakeRpc(&quot;testrpc&quot;, &amp;rpcTest)

id, err := rpcTest(10)
</code></pre><p>在client端，我们首先声明了跟server Test类型完全一致的一个函数变量。然后通过MakeRpc接口将其命名为testrpc，并且将rpcTest绑定到实际的rpc函数上面，最后rpcTest就跟普通函数的调用一样。</p>
<p>可以看到，这种使用方式跟jsonrpc最大的不同在于rpc的返回值直接就是函数自身的返回值。而这个可能更符合我使用go函数的习惯。</p>
<h2 id="实现">实现</h2>
<p>实现一套rpc框架需要考虑server，client以及包协议的问题。</p>
<h3 id="包协议">包协议</h3>
<p>我使用了最简单的包头 + 实际数据的做法，包头使用一个4字节的int表示后续数据的长度。而对于实际的rpc数据，我采用的是gob进行打包解包。</p>
<p>为什么选用gob而不是json？主要在于我不想自己做数据类型的转换，在json中，int类型的encode，decode会变成float类型的，如果函数需要的参数是int，json decode之后还需要我们自己根据参数实际的类型进行转换。增加了复杂度。而gob则在encode时候会加上实际的数据类型，这样decode之后我就能直接使用。</p>
<p>而且gob还支持注册自定义的类型，但是为了简单，建议只支持基本的数据类型，因为对于rpc来说，传递复杂的数据类型进行函数调用，我总觉得有点复杂，这在设计上面已经有问题了。</p>
<h3 id="server">server</h3>
<p>在server需要解决的问题就是rpc函数注册并通过名字能进行该rpc函数调用。而这个通过reflect就能非常方便的实现，一个通过函数名字进行函数调用的例子：</p>
<pre><code>func Test(id int) (string, error) {
    return &quot;abc&quot;, nil
}

funcmap  = map[string]reflect.Value{}

v := reflect.ValueOf(Test)

funcmap[&quot;test_rpc&quot;] = v

args := []reflect.Value{reflect.ValueOf(10)}

funcmap[&quot;test_rpc&quot;](args)
</code></pre><h3 id="client">client</h3>
<p>在client层，我们需要关注在声明一个rpc原型的函数变量之后，如何将其替换成另一个函数进行rpc调用。我们可以通过reflect的MakeFunc函数方便的做到，go自身的例子：</p>
<pre><code>swap := func(in []reflect.Value) []reflect.Value {
    return []reflect.Value{in[1], in[0]}
}

 makeSwap := func(fptr interface{}) {
    fn := reflect.ValueOf(fptr).Elem()
    v := reflect.MakeFunc(fn.Type(), swap)
    fn.Set(v)
}

var intSwap func(int, int) (int, int)
makeSwap(&amp;intSwap)
fmt.Println(intSwap(0, 1))
</code></pre><p>MakeFunc的原理在于，根据传入的函数变量的类型，创建一个新的函数，该函数调用的是我们指定的另一个函数。</p>
<p>同时，我们得到传入变量的指针，并用新的函数重新给该变量赋值。</p>
<h3 id="error处理">error处理</h3>
<p>因为rpc调用可能会出现其他错误，譬如网络断线，gob encode错误等，client在调用的时候必须得处理这些错误，暴力的作法就是如果是这种内部错误，我们直接panic，但是我觉得太不友好，所以我们约定，所有的rpc函数在最后一个返回值必须是error。这样就是是rpc内部的错误，我们也能够通过error返回。</p>
<p>在注册rpc的时候，我们可以通过判断最后一个返回值是否是interface，同时是否具有Error函数来强制要求必须为error。如下</p>
<pre><code>v := reflect.ValueOf(rpcFunc)

nOut := v.Type().NumOut()

if nOut == 0 || v.Type().Out(nOut-1).Kind() != reflect.Interface {
    err = fmt.Errorf(&quot;%s return final output param must be error interface&quot;, name)
    return
}

_, b := v.Type().Out(nOut - 1).MethodByName(&quot;Error&quot;)
if !b {
    err = fmt.Errorf(&quot;%s return final output param must be error interface&quot;, name)
    return
}
</code></pre><p>但是，如果在MakeFunc里面直接返回error，会出现“reflect: function created by MakeFunc using closure returned wrong type: have *errors.errorString for error”这样的问题，主要在于reflect.Value需要知道我们error的接口类型，参考<a href="http://grokbase.com/t/gg/golang-nuts/139qpwv48h/go-nuts-returning-interface-value-from-reflect-makefunc" target="_blank" rel="external">这里</a>。</p>
<p>所以，我们通过如下方式对error进行处理，转成相应的reflect.Value</p>
<pre><code>v := reflect.ValueOf(&amp;e).Elem()
</code></pre><h3 id="nil处理">nil处理</h3>
<p>在实际rpc中，我们可能还会面临参数为nil的问题，如果直接对nil进行reflect.ValueOf，是得不到我们期望的类型的，这时候的Kind是0，reflect压根不能将其正确的转换成函数实际的类型。</p>
<p>当碰到nil的情况，我们只需要根据当前函数参数实际的类型，生成一个Zero Value，就可以很方便的解决这个问题：</p>
<p>假设函数第一个返回值为nil，那么我们这样</p>
<pre><code>v := reflect.Zero(fn.Type().Out(0))
</code></pre><h2 id="代码">代码</h2>
<p>最开始写了一个代码片段验证自己的想法，在<a href="https://gist.github.com/siddontang/9088489" target="_blank" rel="external">这里</a>。</p>
<p>一个rpc frame的完整<a href="https://github.com/siddontang/golib/tree/master/rpc" target="_blank" rel="external">实现</a>。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-16T08:22:05.000Z"><a href="/2014/01/16/golang-mylib-timingwheel/">01-16-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/16/golang-mylib-timingwheel/">Go实现简易的TimingWheel</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Ticker">Ticker</h1>
<p>最近的项目用go实现的服务器需要挂载大量的socket连接。如何判断连接是否还存活就是我们需要考虑的一个问题了。</p>
<p>通常情况下面，socket如果被客户端正常close，服务器是能检测到的，但是如果客户端突然拔掉网线，或者是断电，那么socket的状态在服务器看来可能仍然是established。而实际上该socket已经不可用了。</p>
<p>为了判断连接是否可用，通常我们会用timer机制来定时检测，在go里面，这非常容易实现，如下：</p>
<pre><code>ticker := time.NewTicker(60 * time.Second)

for {
    select {
        case &lt;-ticker.C:
            if err := ping(); err != nil {
                close()
            }
    }
}
</code></pre><p>上面我们使用一个60s的ticker，定时去ping，如果ping失败了，证明连接已经断开了，这时候就需要close了。</p>
<p>这套机制比较简单，也运行的很好，直到我们的服务器连上了10w+的连接。因为每一个连接都有一个ticker，所以同时会有大量的ticker运行，cpu一直在30%左右徘徊，性能不能让人接受。</p>
<p>其实，我们只需要的是一套高效的超时通知机制。</p>
<h1 id="Close_channel_to_broadcast">Close channel to broadcast</h1>
<p>在go里面，channel是一个很不错的东西，我们可以通过close channel来进行broadcast。如下：</p>
<pre><code>ch := make(bool)

for i := 0; i &lt; 10; i++ {
    go func() {
        println(&quot;begin&quot;)
        &lt;-ch
        println(&quot;end&quot;)
    }
}

time.Sleep(10 * time.Second)

close(ch)
</code></pre><p>上面，我们启动了10个goroutine，它们都会因为等待ch的数据而block，10s之后close这个channel，那么所有等待该channel的goroutine就会继续往下执行。</p>
<h1 id="TimingWheel">TimingWheel</h1>
<p>通过channel这种close broadcast机制，我们可以非常方便的实现一个timer，timer有一个channel ch，所有需要在某一个时间 “T” 收到通知的goroutine都可以尝试读该ch，当T到达时候，close该ch，那么所有的goroutine都能收到该事件了。</p>
<p>timingwheel的使用很简单，首先我们创建一个wheel</p>
<pre><code>//这里我们创建了一个timingwheel，精度是1s，最大的超时等待时间为3600s
w := timingwheel.NewTimingWheel(1 * time.Second, 3600)

//等待10s
&lt;-w.After(10 * time.Second)
</code></pre><p>因为timingwheel只有一个1s的ticker，并且只创建了3600个channel，系统开销很小。当我们程序换上timingwheel之后，10w+连接cpu开销在10%以下，达到了优化效果。</p>
<p>timingwheel的代码在<a href="https://github.com/siddontang/golib/tree/master/timingwheel" target="_blank" rel="external">这里</a>。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-16T07:33:02.000Z"><a href="/2014/01/16/lua-function-register/">01-16-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/16/lua-function-register/">Lua与C接口层开发</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="lua与c的交互">lua与c的交互</h1>
<p>关于lua和c的交互，主要有两个方面，一是lua调用c的函数，而另一个则是c调用lua函数。而这些都是通过lua stack来进行的。</p>
<h2 id="c调用lua">c调用lua</h2>
<p>在c里面使用lua，主要是通过lua_call这类函数，下面来自lua manual的例子：</p>
<pre><code>lua_getglobal(L, &quot;f&quot;);                  /* function to be called */
lua_pushstring(L, &quot;how&quot;);                        /* 1st argument */
lua_getglobal(L, &quot;t&quot;);                    /* table to be indexed */
lua_getfield(L, -1, &quot;x&quot;);        /* push result of t.x (2nd arg) */
lua_remove(L, -2);                  /* remove &#39;t&#39; from the stack */
lua_pushinteger(L, 14);                          /* 3rd argument */
lua_call(L, 3, 1);     /* call &#39;f&#39; with 3 arguments and 1 result */
lua_setglobal(L, &quot;a&quot;);                         /* set global &#39;a&#39; */
</code></pre><p>该例子等同于直接在lua里面调用 a = f(“how”, t.x, 14)。<br>通过上面的例子可以看到，在c中使用lua是一件很容易的事情，首先获取需要调用的lua函数，然后将其需要的参数依次压入stack，然后通过lua_call调用，该函数调用的返回值也压入stack，供c去获取。</p>
<h2 id="lua调用c">lua调用c</h2>
<p>对于lua调用c，我们首先需要将c函数注册给lua，而注册给lua的函数，需要满足 int (*lua_CFunction)(lua_State* pState) 这种类型。如下例子:</p>
<pre><code>int multi(lua_State* pState)
{
    int data1 = int(lua_tonumber(pState, 1));
    int data2 = int(lua_tonumber(pState, 2));
    lua_pushnumber(pState, data1 * data2);
    return 1;
}

lua_register(pState, multi, &quot;multi&quot;);

#for use in lua
#a = multi(10, 20)
</code></pre><p>我们通过lua_register将mutli函数注册给lua，该函数接受两个参数，并且有一个返回值。当lua调用multi的时候，会将参数压入stack，所以我们可以通过lua_tonumber(pState, 1)和lua_tonumber(pState, 2)来获取，其中1为第一个参数10，2为第二个参数20。当运行完成之后，multi函数通过lua_pushnumber将结果压入lua堆栈，并通过return 1告知lua有一个返回值，为200。</p>
<p>可以看到，在lua中使用c也是一件很简单的事情。</p>
<h1 id="lua_mainly_or_c_mainly">lua mainly or c mainly</h1>
<p>通过上面的例子可以看出，lua与c是很方便的交互的，但是在实际的游戏项目中，我们首先必须确定的一个问题就是，代码逻辑是以lua为主还是以c为主。</p>
<ul>
<li>lua为主的游戏就是逻辑主要由lua负责，核心的对性能要求较高的逻辑则由c负责，游戏中的数据大多由lua负责。</li>
<li>c为主的游戏则是逻辑主要由c负责，lua只是负责简单的配置。</li>
</ul>
<p>这两种方式都有优劣，对于实际游戏项目来说，个人认为，应该采用lua为主，c作为高性能核心的方式。之所以这样选择，是因为游戏的逻辑变动很大，我们需要快速的进行代码迭代，这个对于lua来说非常方便。而对于核心引擎，因为变化不大，同时对性能要求较高，所以采用c是一个很好的选择。</p>
<h1 id="reg_helper">reg helper</h1>
<p>在实际的游戏项目中，我们会遇到这样一个问题，假设c提供的函数为 int func(int a, int b)，如果这个函数要提供给lua使用，我们需要写一个对应的注册函数，如下:</p>
<pre><code>int func_wrapper(lua_State* pState)
{
    int a = int(lua_tonumber(pState, 1));
    int b = int(lua_tonumber(pState, 2));
    int ret = func(a, b);
    lua_pushnumber(pState, ret);
    return 1;
}

lua_register(pState, func_wrapper, &quot;func&quot;);
</code></pre><p>对于任意的c函数，我们需要写一个对应的wrapper用来注册给lua。如果项目中只有几个c函数，那么无所谓，但是如果需要注册给lua的c函数很多，那么对于每一个c函数写一个wrapper，是一件很不现实的事情。并且如果c函数的参数或者返回值有变化，我们同时需要修改对应的wrapper函数。基于上述原因，我们需要一套自动机制，能够将任意的c函数注册给lua使用。实际来说，我们需要提供一个函数，对于任意的c函数func，我们只需要调用register(func, “func”)，那么就能直接注册给lua使用。</p>
<h2 id="traits">traits</h2>
<p>首先，我们必须面对的问题就是，不同的c函数，参数和返回值是不一样的，譬如对于int类型的参数，我们需要通过lua_tonumber获取数据，而对于const char* 类型的参数，我们需要通过lua_tostring来获取，同理对于返回值也一样。所以我们需要一套机制，根据c函数不同的参数和返回值类型来调用lua对应的stack操纵函数。我们可以通过c++ traits来实现。</p>
<p>我们提供如下一套函数:</p>
<pre><code>template&lt;typename T&gt;
struct TypeHelper{};
bool getValue(TypeHelper&lt;bool&gt;, lua_State* pState, int index)
{
    return lua_toboolean(pState, index) == 1;
}
char getValue(TypeHelper&lt;char&gt;, lua_State* pState, int index)
{
    return static_cast&lt;char&gt;(lua_tonumber(pState, index));
}
int getValue(TypeHelper&lt;int&gt;, lua_State* pState, int index)
{
    return static_cast&lt;int&gt;(lua_tonumber(pState, index));
}
void pushValue(lua_State* pState, bool value)
{
    lua_pushboolean(pState, int(value));
}

void pushValue(lua_State* pState, char value)
{
    lua_pushnumber(pState, value);
}

void pushValue(lua_State* pState, int value)
{
    lua_pushnumber(pState, value);
}
</code></pre><p>通过traits技术，对于不同的参数类型，我们可以调用对应的getValue函数，来从lua获取实际的数据，而对于返回值，通过c++自动的参数匹配，就能调用对应的pushValue函数。</p>
<h2 id="CCallHelper">CCallHelper</h2>
<p>上面解决了类型匹配的问题，下面就需要提供wrapper函数，用以封装c函数。这里我们提供call helper类来实现。</p>
<pre><code>template&lt;typename Ret&gt;
class CCallHelper
{
public:
    static int call(Ret (*func)(), lua_State* pState)
    {
        Ret ret = (*func)();
        pushValue(pState, ret);
        return 1;
    }

    template&lt;typename P1&gt;
    static int call(Ret (*func)(P1), lua_State* pState)
    {
        P1 p1 = getValue(TypeHelper&lt;P1&gt;(), pState, 1);
        Ret ret = (*func)(p1);
        pushValue(pState, ret);
        return 1;
    }
};
</code></pre><p>CCallHelper提供了静态的call函数，第一个参数就是实际c函数，通过函数模板可以进行任意c函数的匹配，这里只提供了匹配无参数和一个参数类型的c函数模板，我们可以扩展到支持任意参数个数，但也别太多了。因为对于任意c函数来说，可能有一个返回值，也可能没有返回值，那么我们如何匹配没有返回值的c函数呢？这里就是为什么我们需要CCallHelper的原因。在c++中，是不支持函数级别的模板特化的，但是类却可以，所以我们通过特化CCallHelper来匹配无返回值的c函数。如下：</p>
<pre><code>template&lt;&gt;
class CCallHelper&lt;void&gt;
{
public:
    static int call(void (*func)(), lua_State* pState)
    {
        (*func)();
        return 0;
    } 

    template&lt;typename P1&gt;
    static int call(void (*func)(P1), lua_State* pState)
    {
        P1 p1 = getValue(TypeHelper&lt;P1&gt;(), pState, 1);
        (*func)(p1);
        return 0;
    }
};
</code></pre><h2 id="CCallDispatcher">CCallDispatcher</h2>
<p>通过CCallHelper::call(func, pState)，我们就可以与lua进行交互，那么又如何调用到相应的CCallHelper呢？这里我们通过CCallDispatcher来进行，如下：</p>
<pre><code>template&lt;typename Func&gt;
class CCallDispatcher
{
public:
    template&lt;typename Ret&gt;
    static int dispatch(Ret (*func)(), lua_State* pState)
    {
        return CCallHelper&lt;Ret&gt;::call(func, pState);
    }

    template&lt;typename Ret, typename P1&gt;
    static int dispatch(Ret (*func)(P1), lua_State* pState)
    {
        return CCallHelper&lt;Ret&gt;::call(func, pState);
    }
};
</code></pre><p>通过CCallDispatcher，我们就可以将不同的c函数dispatch到不同的CCallHelper上面。</p>
<h2 id="CCallRegister_and_regFunction">CCallRegister and regFunction</h2>
<p>解决了c函数派发调用的问题，最后我们就需要处理如何将任意的c函数注册给lua，代码如下：</p>
<pre><code>template&lt;typename Func&gt;
class CCallRegister
{
public:
    static int call(lua_State* pState)
    {
        Func* func = static_cast&lt;Func*&gt;(lua_touserdata(pState, lua_upvalueindex(1));
        return CCallDispatcher&lt;Func&gt;::dispatch(*func, pState);
    }
};

template&lt;typename Func&gt;
void regFunction(lua_State* pState, Func func, const char* funcName)
{
    int funcSize = sizeof(Func);
    void* data = lua_newuserdata(pState, funcSize);
    memcpy(data, &amp;func, funcSize);

    lua_pushcclosure(pState, CCallRegister&lt;Func&gt;::call, 1);
    lua_setglobal(pState, funcName);
}
</code></pre><p>首先，我们提供CCallRegister类，里面提供了一个static的call函数，该函数满足lua注册格式，所以实际我们是将该函数注册给lua，在call函数里面，我们通过lua_touserdata(pState, lua_upvalueindex(1))来获取实际的func，然后传递给CCallDispatcher进行派发。而将call注册则是通过regFunction，该函数将实际的c函数func存储在一个userdata中，然后将该userdata绑定到对应的CCallRgister call上面，作为一个upvalue，这样当在lua里面调用call函数的时候，通过lua_upvalueindex获取对应的upvalue，则可以取到实际的c函数。</p>
<h1 id="一些设计上面的考虑">一些设计上面的考虑</h1>
<p>上述reghelper的实现，我已经放到github <a href="https://github.com/siddontang/luahelper" target="_blank" rel="external">luahelper</a>上面，并且在max os，gcc 4.2，lua5.2环境下面测试通过。</p>
<p>这里谈一些设计上面的问题，首先，我说的任意c函数，参数和返回值只能是基本数值类型，如bool，char，short，int，long，float，double以及字符串类型char*等，这里我并没有提供复杂类型譬如class，struct的支持。之所以这样考虑，是因为我想保证lua与<br>c交互的简单，云风曾经说过<a href="http://blog.codingnow.com/2008/08/lua_is_not_c_plus_plus.html" target="_blank" rel="external">lua不是c++</a>，我本人当年也曾经用了2年的时间做了同样的事情。但是实现了这套东西，功能是狠强大了，以至于可以把lua当成c++来用了，但是这真的是使用lua正确的方式吗？我现在觉得，引入复杂的结合层，反而在某些时候会带来更大的复杂性，导致语言侧重点的混淆。所以有时候，我反而觉得，对于这种语言的交互，可能使用其他方式，譬如json，反而来的更容易。</p>
<h1 id="写在后面的话">写在后面的话</h1>
<p>对于lua和游戏开发的一些东西，其实一直想写，但是以前因为很多方面的原因而中途放弃。现在重新开始，有几个方面的原因，一个在于仍然对于游戏开发的热爱，做了4年的游戏开发，虽然现在从事云存储方面的研究，但是对游戏热情依旧。另一个方面在于lua5.2的发布，觉得是应该对以前游戏人生做一个总结了。</p>
<p>如果需要更强大的lua与c的交互，我觉得<a href="http://www.swig.org/" target="_blank" rel="external">swig</a>可能更合适。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T06:04:17.000Z"><a href="/2014/01/01/libtnet-comettest/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/libtnet-comettest/">Libtnet 百万连接测试</a></h1>
  

    </header>
    <div class="entry">
      
        <p>最近在用go语言做一个挂载大量长连接的推送服务器，虽然已经完成，但是内存占用情况让我不怎么满意，于是考虑使用libtnet来重新实现一个。后续我会使用comet来表明推送服务器。</p>
<p>对于comet来说，单机能支撑大量的并发连接，是最优先考虑的事项。虽然现在业界已经有了很多数据，说单机支撑200w，300w，但我还是先把目标定在100w上面，主要的原因在于实际运行中，comet还会有少量逻辑功能，我得保证在单机挂载100w的基础上，完全能无压力的处理这些逻辑。</p>
<h1 id="CometServer_Test">CometServer Test</h1>
<p>首先我用libtnet简单写了一个comet server。它接受http请求，并将其挂起，过一段随机时间之后在返回200。</p>
<pre><code>void onTimeout(const TimingWheelPtr_t&amp; wheel, const WeakHttpConnectionPtr_t&amp; conn)
{
    HttpConnectionPtr_t c = conn.lock();
    if(c)
    {
        c-&gt;send(200);
    } 
}

void onHandler(const HttpConnectionPtr_t&amp; conn, const HttpRequest&amp; request)
{
    int timeout = random() % 60 + 30;
    comet.wheel-&gt;add(std::bind(&amp;onTimeout, _1, WeakHttpConnectionPtr_t(conn)), timeout * 1000);
}

int main()
{
    TcpServer s;        
    s.setRunCallback(std::bind(&amp;onServerRun, _1));
    HttpServer httpd(&amp;s);
    httpd.setHttpCallback(&quot;/&quot;, std::bind(&amp;onHandler, _1, _2));
    httpd.listen(Address(11181));
    s.start(8);
    return 0; 
}
</code></pre><p>可以看到comet server只是负责了挂载长连接的事情，而没有消息的推送。在实际项目中，我已经将挂载连接和推送消息分开到两个服务去完成。所以这里comet仅仅是挂载连接测试。</p>
<h1 id="测试机器准备">测试机器准备</h1>
<p>因为linux系统上面一个网卡tcp连接端口数量是有限制的，我们调整ip_local_port_range使其能支撑60000个tcp连接：</p>
<pre><code>net.ipv4.ip_local_port_range = 1024 65535
</code></pre><p>对于100w连接来说，我们至少需要16台机器，但实际我只有可怜的3台4G内存的虚拟机。所以就要运维的童鞋在每台机器上面装了6块网卡。这样我就能建立100w的连接了。</p>
<p>测试客户端也非常简单，每秒向服务器请求1000个连接，但是需要注意的是，因为一台机器上面有多块网卡，所以在创建socket之后，我们需要将socket绑定到某一块网卡上面。</p>
<p>实际测试中，因为内存问题，每台机器顶多能支撑34w左右的tcp连接，对我来说已经足够，所以也懒得去调优了。</p>
<h1 id="CometServer_Linux调优">CometServer Linux调优</h1>
<p>首先我们需要调整最大打开文件数，在我的机器上面，nr_open最大的值为1048576，对我来说已经足够，所以我将最大文件描述符数量调整为1040000。</p>
<pre><code>fs.file-max = 1040000
</code></pre><p>然后就是对tcp一些系统参数的调优：</p>
<pre><code>net.core.somaxconn = 60000
net.core.rmem_default = 4096
net.core.wmem_default = 4096
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 4096 16777216
net.ipv4.tcp_wmem = 4096 4096 16777216
net.ipv4.tcp_mem = 786432 2097152 3145728
net.core.netdev_max_backlog = 60000
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_max_syn_backlog = 60000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_max_orphans = 131072
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_max_tw_buckets = 60000
net.netfilter.nf_conntrack_max = 1000000
net.netfilter.nf_conntrack_tcp_timeout_established = 1200
</code></pre><p>对于如何调节这些值，网上都是各有各的说法，建议直接man 7 tcp。我在实践中也会通过查看dmesg输出的tcp错误来动态调节。这里单独需要说明的是tcp buffer的设置，我最小和默认都是4k，这主要是考虑到推送服务器不需要太多太频繁的数据交互，所以需要尽可能的减少tcp的内存消耗。</p>
<h1 id="测试结果">测试结果</h1>
<p>实际的测试比较让我满意。</p>
<p>cometserver test8个进程cpu消耗都比较低，因为有轮训timing wheel然后再发送200的逻辑，所以铁定有cpu消耗，如果只是挂载，cpu应该会更低。</p>
<pre><code>PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                       
 4685 root      20   0  187m 171m  652 S 19.9  1.1   2:19.20 cometserver_test                                                                                                
 4691 root      20   0  191m 175m  656 S 16.6  1.1   2:17.80 cometserver_test                                                                                                
 4686 root      20   0  170m 155m  652 S 16.3  1.0   2:09.54 cometserver_test                                                                                                
 4690 root      20   0  183m 167m  652 S 16.3  1.1   2:11.44 cometserver_test                                                                                                
 4692 root      20   0  167m 152m  652 S 16.3  1.0   2:11.29 cometserver_test                                                                                                
 4689 root      20   0  167m 152m  652 S 15.3  1.0   2:03.08 cometserver_test                                                                                                
 4687 root      20   0  173m 158m  652 S 14.3  1.0   2:07.34 cometserver_test                                                                                                
 4688 root      20   0  129m 114m  652 S 12.3  0.7   1:35.77 cometserver_test
</code></pre><p>socket的统计情况：</p>
<pre><code>[root@localhost ~]# cat /proc/net/sockstat
sockets: used 1017305
TCP: inuse 1017147 orphan 0 tw 0 alloc 1017167 mem 404824
UDP: inuse 0 mem 0
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0
</code></pre><p>可以看到，总共有1017147个tcp链接，同时占用了将近4G（1017167是页数，需要乘以4096）的内存。</p>
<p>系统内存的情况：</p>
<pre><code>[root@localhost ~]# free
             total       used       free     shared    buffers     cached
Mem:      16334412   11210224    5124188          0     179424    1609300
-/+ buffers/cache:    9421500    6912912
Swap:      4194296          0    4194296
</code></pre><p>系统有16G内存，还有5G可用，所以不出意外单机应该还能承载更多的tcp连接。</p>
<h1 id="总结">总结</h1>
<p>使用libtnet开发的一个简单的comet server支撑了百万级的连接，加深了我对其应用的信心。</p>
<p>libnet地址<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">https://github.com/siddontang/libtnet</a>，欢迎围观。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T05:03:54.000Z"><a href="/2014/01/01/my-2013/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/my-2013/">我的2013</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="前言">前言</h1>
<p>每年到这个时候，总需要回顾过去，展望未来。2013这一年学到了很多东西，收货了很多，也成长了很多。主要在技术和生活上面，让自己有了记录一下的冲动。</p>
<h1 id="技术">技术</h1>
<p>在技术上面，这一年接触了很多新的东西，让自己眼界开阔不少，同时也开始自我提升，疯狂的在github上面玩<a href="https://github.com/siddontang" target="_blank" rel="external">开源</a>，只是很多都惨不忍睹。</p>
<h2 id="Openresty">Openresty</h2>
<p>今年最开始接触的新东西就是<a href="http://openresty.org/" target="_blank" rel="external">openresty</a>，一个集成nginx的web应用开发框架。</p>
<p>最开始，我们项目的架构采用的是前端nginx做proxy，然后将请求反向代理到后端python tornado的方式实现的。但是随着压力的增加以及一些新功能的上线，这套架构开始显现其局限性，首当其冲的在于慢，虽然可以通过增加tornado进程的方式来进行负载均衡处理，但我总觉得不是长久之计。</p>
<p>同时，随着业务逻辑的复杂，一些操作需要多个service协同完成，然后在返回给用户，而在什么地方组织多个service的数据以及逻辑也成了我们的一个难题。</p>
<p>鉴于上述几个原因，我开始研究openresty，因为之前有3年多lua开发的经验，所以非常容易上手。同时，通过研读openresty的一整套源码，真正的了解了nginx以及之上的openresty的运行机制。可以这么说，这段时间我从一个连nginx配置都不会写的小白程序员一跃成为了名义上精通nginx开发的屌丝程序员。</p>
<p>自然，我开始在项目中推广openresty，这也得到了大家的支持，现在虽然我们很多代码仍然是使用python在编写，但是对于很多高性能模块我们已经逐步转向了openresty。</p>
<p>在使用openresty的时候，还提交了几个bug，这点颇为自豪，同时也写了一些东西，譬如 <a href="http://siddontang.github.io/introduction-to-nginx/" target="_blank" rel="external">Introduction To Nginx</a></p>
<h2 id="Go">Go</h2>
<p>接触go纯属偶然，在上半年终于完结了一个持续时间特长的项目之后，整个组的童鞋都陷于一个无事可干的真空期，也就是在这段时间，第一次学习了go，立刻就被它的简单强大所吸引，尤其是在服务器并发编程方面，那可是非常的强悍。</p>
<p>于是，我带着两个完全不懂go的童鞋开始了我们推送服务器的编程之旅。最开始的时候，因为两位童鞋只会python，为了尽快的出功能，一些后台的服务采用tornado搭建，而我用go写了挂载大量长连接的comet服务。</p>
<p>这里不得不说go开发服务的快捷，在goroutine以及channel的机制下面，没有了层层的callback，没有了死锁，我只用了3天就弄出了comet，而且能持续稳定运行。</p>
<p>鉴于用go成功开发了comet，我让另外两个童鞋也开始用go重构先前写的python逻辑，进展也很顺利。</p>
<p>不过对于我来说，go现在最大的一个问题在于内存占用，go现在默认的stack大小为8k，对于需要挂载百万连接的comet来说，内存开销实在太大，虽然现在机器的配置完全不需要我担心，但总觉得有点不爽。不过如果优化，也是后续的事情了。</p>
<p>今年，对于我来说，竟然吃了两次螃蟹，第一个就是openresty，而第二个就是go，而且很幸运的是都能在项目中实施。</p>
<h2 id="Libtnet">Libtnet</h2>
<p>今年，我真正的开始了一个算是比较大的开源项目：<a href="https://github.com/siddontang/libtnet" target="_blank" rel="external">libtnet</a>，它是一个参考tornado的c++高性能网络库。之所以写libtnet，主要是为了后续能用到comet上面，同时也让我自己对多年的网络编程做一个总结。</p>
<p>以前总说自己精通网络编程啥啥的，其实心里面也知道是用来忽悠的，毕竟精通这个词没多少年的沉淀是不可能的。但是通过写libtnet，不说精通，至少让我又对很多网络编程的东西了解了。</p>
<p>不过libtnet的问题在于使用c++进行开发，同时大量采用function + bind的开发模式，对于组内的童鞋来说理解上面还比较困难，如果在项目中实施很有可能面临只有我一个人维护的窘境。</p>
<h2 id="移动开发">移动开发</h2>
<p>今年没事的时候也涉猎了一些移动开发的方面，包括android以及ios。在android上面开发了一些小应用，只是都是自娱自乐。在ios上面使用cocos2d-x开发了一个小游戏demon，也当是消遣了。</p>
<p>不过在明年准备好好的尝试一下该领域的开发。尤其是ios上面，毕竟老婆都有了土豪金了，为了展示老公的程序员风采，再怎么也得弄一个出来。</p>
<h1 id="工作">工作</h1>
<p>今年在公司，我开始尝试站着上班，不得不说这对我工作效率的提升有很大的帮助，站着上班，不光减肥，还能让我专心工作，因为任何的聊天浏览网页都是一件很耗费力气的事情。这里也不得不佩服自己的毅力，每天竟然都能坚持站7，8个小时。</p>
<h1 id="生活">生活</h1>
<p>生活上面今年最主要就是几件事情：</p>
<ul>
<li>举办了婚礼</li>
<li>老婆怀了孩子</li>
<li>拿到驾照</li>
<li>买了小车</li>
</ul>
<p>可能对于我来说，明年在生活上面最大的事情就是要照顾孩子了。</p>
<h1 id="总结">总结</h1>
<p>总之，2013过的很快，但也过的很充实，希望自己在2014里面越来越好，能有更大的突破。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2014-01-01T01:32:56.000Z"><a href="/2014/01/01/learn-tornado-base/">01-01-2014</a></time>
      
      
  
    <h1 class="title"><a href="/2014/01/01/learn-tornado-base/">学习Tornado：基本</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="前言">前言</h1>
<p>在python里面，有许多<a href="http://wiki.python.org/moin/WebFrameworks" target="_blank" rel="external">web framework</a>。对于我来说，因为很长一段时间都在使用tornado，所以有了一些心得体会。</p>
<p>在这里，要说明一下，tornado采用的是<strong>2.4</strong>版本。</p>
<h1 id="架构">架构</h1>
<p>tornado是一个典型的prefork + io event loop的web server架构，<img src="https://raw.github.com/siddontang/blog/master/asserts/tornado-architecture.png" alt="Alt text" title="architecture"></p>
<p>从图上可以看出，tornado的架构是很简单清晰的。</p>
<ul>
<li>ioloop是tornado的核心，它就是一个io event loop，底层封装了select，epoll和kqueue，并根据不同的平台选择不同的实现。</li>
<li>iostream封装了non-blocking socket，用它来进行实际socket的数据读写。</li>
<li>TCPServer则是通过封装ioloop实现了一个简易的server，同时我们也在这里进行prefork的处理</li>
<li>HTTPServer则是继承TCPServer实现了一个能够处理http协议的server。</li>
<li>Application则是实际处理http请求的模块，HTTPServer收到http请求并解析之后会通过Application进行处理。</li>
<li>RequestHandler和WebSocketHandler则是注册给Application用来处理对应url的。</li>
<li>WSGIApplication则是tornado用于支持WSGI标准的接口，通过WSGIContainer包装共HTTPServer使用。</li>
</ul>
<h1 id="例子">例子</h1>
<p>通过上面的分析，直到tornado的架构是很简单明了的，所以自然我们也能够通过简短的一些代码就能搭建起自己的http server。以一个hello world开始：</p>
<pre><code>import tornado.web 
import tornado.httpserver 
import tornado.ioloop 

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write(&#39;Hello World&#39;)

application = tornado.web.Application([
    (r&quot;/&quot;, MainHandler),
])
http_server = tornado.httpserver.HTTPServer(application)
http_server.listen(8080)
tornado.ioloop.IOLoop.instance().start()
</code></pre><p>流程很简单，如下：</p>
<ul>
<li>定义了一个MainHandler，该handler用来处理对应url</li>
<li>生成一个Application实例，并设置url dispatch规则，(r”/“, MainHandler)就是一个规则，第一个pattern用来表明需要处理的url，内部会使用正则匹配，第二个就是对应url处理的handler</li>
<li>生成一个HTTPServer实例，使用Application进行构造，这样HTTPServer处理的http请求就会转给application处理。</li>
<li>HTTPServer监听一个端口8080，该listen socket会加入ioloop中，用于监听连接的建立。</li>
<li>ioloop启动，程序进入io event loop模式。</li>
</ul>
<p>当ioloop start之后，服务器就启动了，后续就是一个http server最基本的流程处理了。</p>
<h1 id="ReuqestHandler">ReuqestHandler</h1>
<h2 id="pattern_and_handler">pattern and handler</h2>
<p>从上面例子可以看出，搭建一个http server很简单，所以我们重点只需要考虑的是如何处理不同的url http请求，这也就是RequestHandler需要做的事情。</p>
<p>我们在创建Application的时候，会指定不同的url pattern需要处理的handler。如下：</p>
<pre><code>import tornado.web 
import tornado.httpserver 
import tornado.ioloop 

class Index1Handler(tornado.web.RequestHandler):
    def get(self):
        self.write(&#39;Index1&#39;)

class Index2Handler(tornado.web.RequestHandler):
    def get(self, data):
        self.write(&#39;Index2&#39;)
        self.write(data)

application = tornado.web.Application([
    (r&quot;/index1&quot;, Index1Handler),
    (r&quot;/index2/(\w+)&quot;, Index2Handler),
])

http_server = tornado.httpserver.HTTPServer(application)
http_server.listen(8080)
tornado.ioloop.IOLoop.instance().start()
</code></pre><p>在上面的例子中，我们有两个handler，分别处理url path为index1和index2的情况，对于index2来说，我们看到，它后面还需要匹配一个单词。我们通过curl访问如下：</p>
<pre><code>$ curl http://127.0.0.1:8080/index1
index1

$ curl http://127.0.0.1:8080/index2/abc
index2abc
</code></pre><h2 id="http_method">http method</h2>
<p>RequestHandler支持任何http mthod，包括get，post，head和delete，也就是说，tornado天生支持restful编程模型。</p>
<pre><code>class MainHandler(tornado.web.RequestHandler):
    def get(self):
        pass

    def post(self):
        pass

    def head(self):
        pass

    def delete(self):
        pass
</code></pre><p>从上面可以看到，我们只需要在handler里面实现自己的get，post，head和delete函数就可以了，这点再次说明tornado的简洁与强大。</p>
<h1 id="后续next">后续next</h1>
<p>这里，只是简单了介绍了一下tornado，后续将会从template，asynchronous，security等分别介绍一下。希望通过这个能让自己对tornado的理解更加深刻，同时也为后续使用其他python web framework做参考。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-29T08:27:12.000Z"><a href="/2013/12/29/libtnet-http/">12-29-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/29/libtnet-http/">Libtnet HTTP实现</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="HTTP">HTTP</h1>
<p>libtnet提供了简单的http支持，使用也很简单。</p>
<p>一个简单的http server：</p>
<pre><code>void onHandler(const HttpConnectionPtr_t&amp; conn, const HttpRequest&amp; request)
{
    HttpResponse resp;
    resp.statusCode = 200;
    resp.setContentType(&quot;text/html&quot;);
    resp.body.append(&quot;Hello World&quot;);    
    conn-&gt;send(resp);
}

TcpServer s;
HttpServer httpd(&amp;s);
httpd.setHttpCallback(&quot;/test&quot;, std::bind(&amp;onHandler, _1, _2));
httpd.listen(Address(80));
s.start(4);
</code></pre><p>我们对http server的<strong>“/test”</strong>注册了一个handler，当用户访问该url的时候，就会显示”Hello World”。</p>
<p>同样，http client的使用也很简单：</p>
<pre><code>void onResponse(IOLoop* loop, const HttpResponse&amp; resp)
{
    cout &lt;&lt; resp.body &lt;&lt; endl;
    loop-&gt;stop();
}

IOLoop loop;
HttpClientPtr_t client = std::make_shared&lt;HttpClient&gt;(&amp;loop);
client-&gt;request(&quot;http://127.0.0.1:80/test&quot;, std::bind(&amp;onResponse, &amp;loop, _1));
loop.start(); 
</code></pre><p>这里，我们使用了一个http client，向server请求”/test”的内容，当服务器有响应之后，会调用响应的回调函数。</p>
<h1 id="HTTP_Parser">HTTP Parser</h1>
<p>对于http的解析，我采用的是<a href="https://github.com/joyent/http-parser" target="_blank" rel="external">http parser</a>，因为它采用的是流式解析，同时非常容易集成进libtnet。</p>
<p>使用http parser只需要设置相应的回调函数即可。http parser有如下几种回调：</p>
<ul>
<li>message begin，解析开始的时候调用</li>
<li>url，解析url的时候调用</li>
<li>status complete，http response解析status的时候调用</li>
<li>header field，解析http header的field调用</li>
<li>header value，解析http header的value调用</li>
<li>headers complete，解析完成http header调用</li>
<li>body，解析http body调用</li>
<li>message complete，解析完成调用</li>
</ul>
<p>这里特别需要注意的是http header的解析，因为http parser将其拆分成了两种回调，所以我们在处理的时候需要记录上一次header callback是field的还是value的。在解析field的时候，如果上一次是value callback，那我们就需要将上一次解析的field和value保存下来，而该次的解析则是一个新的field了。</p>
<p>另外，http parser还提供了upgrade的支持，所以我们很方便的就能区分该次请求是否为websocket。</p>
<h1 id="Websocket">Websocket</h1>
<p>libtnet也提供了websocket的支持，现阶段，只支持<a href="http://tools.ietf.org/html/rfc6455" target="_blank" rel="external">RFC6455</a>。</p>
<p>当libtnet通过http parser发现该次请求为websocket的时候，就进入了websocket的流程。websocket的使用也很简单，当握手成功之后，后续的所有通讯就是纯粹的tcp通信了。</p>
<p>一个简单的websocket server：</p>
<pre><code>void onWsCallback(const WsConnectionPtr_t&amp; conn, WsEvent event, const void* context)
{
    switch(event)
    {
        case Ws_CloseEvent:
            break;
        case Ws_MessageEvent:
            {
                const string&amp; str = *(const string*)context;
                conn-&gt;send(str);
            }
            break;
        case Ws_PongEvent:
            break;
        default:
            break;
    }
}

TcpServer s;
HttpServer httpd(&amp;s);
httpd.setWsCallback(&quot;/push/ws&quot;, std::bind(&amp;onWsCallback, _1, _2, _3));    
httpd.listen(Address(80));
s.start();
</code></pre><p>可以看到，websocket的callback机制也类似于libtnet connection的callback机制，用户需通过event + context的方式来处理该次回调的数据。</p>
<p>libtnet对websocket的frame的处理参照的是tornado的websocket模块。也能够组合多frame的数据，外部只需要关注Ws_MessageEvent即可。</p>
<p>websocket client的实现也很简单：</p>
<pre><code>void onWsConnEvent(const WsConnectionPtr_t&amp; conn, WsEvent event, const void* context)
{
    switch(event)
    {
        case Ws_OpenEvent:
            conn-&gt;send(&quot;Hello world&quot;);
            break;    
        case Ws_MessageEvent:
            {
                const string&amp; msg = *(const string*)context;

                LOG_INFO(&quot;message %s&quot;, msg.c_str());
                conn-&gt;close();                        
            }
            break;
        default:
            break;
    }    
}

IOLoop loop;
WsClientPtr_t client = std::make_shared&lt;WsClient&gt;(&amp;loop);    
client-&gt;connect(&quot;ws://127.0.0.1:80/push/ws&quot;, std::bind(&amp;onWsConnEvent, _1, _2, _3));
loop.start();
</code></pre>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-12-28T10:28:49.000Z"><a href="/2013/12/28/libtnet-connection/">12-28-2013</a></time>
      
      
  
    <h1 class="title"><a href="/2013/12/28/libtnet-connection/">Libtnet Connection实现</a></h1>
  

    </header>
    <div class="entry">
      
        <p>libtnet只支持IPv4 TCP Connection，之所以这么做都是为了使得实现尽可能的简单。我们主要在Connection类中封装了对tcp连接的操作。</p>
<p>Connection继承自std::enable_shared_from_this，也就意味着外部我们会操作其shared_ptr<connection>，libtnet几乎所有的对象都采用智能指针的方式来进行内存管理。</connection></p>
<p>当Connection创建成功之后，会通过IOLoop的addHandler接口，将其绑定到ioloop上面：</p>
<pre><code>ConnectionPtr_t conn = shared_from_this();
m_loop-&gt;addHandler(m_fd, TNET_READ, std::bind(&amp;Connection::onHandler, conn, _1, _2));
</code></pre><p>因为我们直接在std::bind里面使用了shared_ptr，所以ioloop自然引用了该Connection，外部不需要在存储Connection，以防内存泄露。</p>
<p>对于一个connection而言，它只可能有几种状态，</p>
<ul>
<li>Connecting，表明正在尝试连接，发生在connect返回EINPROGRESS。</li>
<li>Connected，连接已经建立成功，发生在connect成功或者accept成功。</li>
<li>Disconnecting，表明连接正在断开，发生在用户主动调用shutDown之后。</li>
<li>Disconnected，连接已经断开，这时候对应的socket也会被close掉。</li>
</ul>
<h1 id="Event_Callback">Event Callback</h1>
<p>在Connection中，我们使用一个event callback来绑定相应事件的回调。主要有如下connection event：</p>
<ul>
<li>Conn_EstablishedEvent, 当server accept成功，创建了Connection对象之后，触发。</li>
<li>Conn_ConnectEvent，当client connect成功，触发。</li>
<li>Conn_ConnectingEvent，当client connect返回EINPROGRESS，触发。</li>
<li>Conn_ReadEvent，当连接可读，触发。</li>
<li>Conn_WriteCompleteEvent，当发送的数据都发送完毕，触发。</li>
<li>Conn_ErrorEvent，当连接有错误发生，触发。</li>
<li>Conn_CloseEvent，当连接主动或者被动关闭，触发。</li>
</ul>
<p>event callback原型如下：</p>
<pre><code>typedef shared_ptr&lt;Connection&gt; ConnectionPtr_t;
typedef std::function&lt;void (const ConnectionPtr_t&amp;, ConnEvent, const void* context)&gt; ConnEventCallback_t;
</code></pre><p>对应不同的事件，触发的时候context的内容不同。现阶段，只有ReadEvent的时候context为StackBuffer，原型如下：</p>
<pre><code>class StackBuffer
{
public:
    StackBuffer(const char* buf, size_t c) : buffer(buf), count(c) {}

    const char* buffer;
    size_t count;    
};
</code></pre><p>当连接可读的时候，Connection会将数据读取到栈上面，并用StackBuffer来指代，这样当外部处理ReadEvent的时候就能通过将context转换成StackBuffer获取到读取的数据。</p>
<p>下面简单说明一下一些设计上面的取舍：</p>
<ul>
<li><p>为什么只提供一个event callback，而不提供read callback，write complete callback，close callback多个回调接口？</p>
<p>  libtnet的所有callback都采用的是std::function实现，而该对象占用32字节，如果每个event都提供一个对应的callback，那么内存的开销会有点大，同时大部分时候很多callback我们是不感兴趣的。</p>
<p>  还有一个重要的原因在于只提供一个event callback，外部的一些对象就可以通过该callback跟Connection绑定，也就是将其自身的生命周期与Connection绑定在了一起，当Connection删除的时候该对象也自行删除。libtnet中，HttpConnection，WsConnection都是采用该方法，因为对于一个Http连接来说，如果底层的Tcp连接都断开无效了，基于Tcp的Http连接自然就无效了。</p>
</li>
<li><p>Connection为什么不缓存读取的数据，而是交由外部callback去处理？</p>
<p>  Connection作为一个底层的类，对于读取的数据，并不知道具体需要如何处理，所以还不如将数据直接发到外层，供上层实际的应用逻辑处理。但是如果后续Connection考虑支持ssl，那么就需要进行缓存数据了。</p>
</li>
</ul>
<h1 id="Write">Write</h1>
<p>Connection建立之后，默认只会在ioloop中设置TNET_READ事件，因为epoll采用的水平触发模式，如果直接设置TNET_WRITE事件，那么epoll会一直通知socket可写，但实际上并没有可以发送的数据。</p>
<p>所以，libtnet采用如下的方式进行数据发送：</p>
<ul>
<li>直接调用writev函数进行数据发送</li>
<li>如果数据未发送完毕，则向ioloop注册TNET_WRITE事件，下次触发可写的时候继续发送，直至发送成功，清除TNET_WRITE事件。</li>
</ul>
<p>另外，在发送的时候，我们还需要考虑signal pipe的情况，所以需要忽略该singal。使用如下方式：</p>
<pre><code>class IgnoreSigPipe
{
public:
    IgnoreSigPipe()
    {
        signal(SIGPIPE, SIG_IGN);    
    }    
};

static IgnoreSigPipe initObj;
</code></pre><p>当libtnet启动的时候，就忽略了signal pipe信号。虽然这样做稍微有一点副作用，但大部分时候我们并不需要关注SIGPIPE信号。</p>
<h1 id="Kick_Off_Connection">Kick Off Connection</h1>
<p>通常，为了处理不活跃连接，程序都会将每个connection设置一个timer，如果timer到了该连接仍然没有交互，则会删除该连接，否则则继续更新timer。另一种做法就是提供一个time wheel，将connection放置在该wheel中，如果有交互，则在wheel中移动。</p>
<p>libtnet采用了一种更简单，但是精度比较差的做法。</p>
<p>当server成功创建一个connection之后，将会添加到一个ConnChecker中，checker保存的是该connection的weak_ptr。每隔一段时间，checker检查一批connection：</p>
<ul>
<li>如果connection weak_ptr无法lock提升至shared_ptr，证明该连接已经删除，checker直接移除。</li>
<li>如果connection处于connecting状态，并且超过了设置的最大连接超时时间，shutDown该connection。</li>
<li>如果connection处于connected状态，并且在一段时间内没有任何交互，shutDown。</li>
</ul>
<p>ConnChecker的检查间隔以及每次检查步数都可以通过外部设置。使用ConnChecker虽然简单，但是在连接数过大的情况下面，一些过期的connection不能立刻被清理掉。对于这个问题，我觉得可以接受，一个连接一秒之后被关闭还是两秒之后被关闭，差别真的不大。如果我们真的需要对一些connection做精确的时间控制，那直接可以对其使用timer。</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/page/3/" class="alignleft prev">上一页</a>
  
  
    <a href="/page/5/" class="alignright next">下一页</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:siddontang.com">
  </form>
</div>

  <div class="widget tag">
  <h3 class="title">Github</h3>
  <ul class="entry">
  <li><a href="https://github.com/siddontang/ledisdb" target="_blank">LedisDB</a><small>Fast NoSQL</small></li>
  <li><a href="https://github.com/siddontang/mixer" target="_blank">Mixer</a><small>MySQL Proxy with Go</small></li>
  <li><a href="https://github.com/siddontang/libtnet" target="_blank">Libtnet</a><small>High Performance EventLoop</small></li>
  <li><a href="https://github.com/siddontang/moonmq" target="_blank">MoonMQ</a><small>Fast Message Queue</small></li>
  <li><a href="https://github.com/siddontang" target="_blank">More...</a></li>
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/c++/">c++</a><small>7</small></li>
  
    <li><a href="/categories/go/">go</a><small>26</small></li>
  
    <li><a href="/categories/leetcode/">leetcode</a><small>1</small></li>
  
    <li><a href="/categories/lua/">lua</a><small>2</small></li>
  
    <li><a href="/categories/mysql/">mysql</a><small>1</small></li>
  
    <li><a href="/categories/nginx/">nginx</a><small>2</small></li>
  
    <li><a href="/categories/program/">program</a><small>10</small></li>
  
    <li><a href="/categories/python/">python</a><small>6</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">标签云</h3>
  <div class="entry">
    <a href="/tags/algorithm/" style="font-size: 10.00px;">algorithm</a><a href="/tags/c++/" style="font-size: 18.57px;">c++</a><a href="/tags/celery/" style="font-size: 10.00px;">celery</a><a href="/tags/go/" style="font-size: 20.00px;">go</a><a href="/tags/gopkg/" style="font-size: 10.00px;">gopkg</a><a href="/tags/goroutine/" style="font-size: 10.00px;">goroutine</a><a href="/tags/json/" style="font-size: 10.00px;">json</a><a href="/tags/ledisdb/" style="font-size: 14.29px;">ledisdb</a><a href="/tags/leetcode/" style="font-size: 10.00px;">leetcode</a><a href="/tags/leveldb/" style="font-size: 10.00px;">leveldb</a><a href="/tags/libtnet/" style="font-size: 15.71px;">libtnet</a><a href="/tags/log/" style="font-size: 10.00px;">log</a><a href="/tags/lua/" style="font-size: 14.29px;">lua</a><a href="/tags/message-queue/" style="font-size: 10.00px;">message queue</a><a href="/tags/mixer/" style="font-size: 12.86px;">mixer</a><a href="/tags/moonmq/" style="font-size: 11.43px;">moonmq</a><a href="/tags/mysql/" style="font-size: 15.71px;">mysql</a><a href="/tags/nginx/" style="font-size: 11.43px;">nginx</a><a href="/tags/nosql/" style="font-size: 14.29px;">nosql</a><a href="/tags/polaris/" style="font-size: 11.43px;">polaris</a><a href="/tags/pprof/" style="font-size: 10.00px;">pprof</a><a href="/tags/proxy/" style="font-size: 10.00px;">proxy</a><a href="/tags/python/" style="font-size: 17.14px;">python</a><a href="/tags/reflect/" style="font-size: 10.00px;">reflect</a><a href="/tags/restful/" style="font-size: 10.00px;">restful</a><a href="/tags/rpc/" style="font-size: 10.00px;">rpc</a><a href="/tags/timer/" style="font-size: 11.43px;">timer</a><a href="/tags/timingwheel/" style="font-size: 10.00px;">timingwheel</a><a href="/tags/tornado/" style="font-size: 12.86px;">tornado</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2014 SiddonTang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'siddontang';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>